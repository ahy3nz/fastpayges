{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Reddit, part 1\n",
    "\n",
    "- toc: false\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: false\n",
    "- categories: [data science, personal]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In light of recent internet trends about retail investors, I'm sure many of us have questions about the kinds of content that gets posted on reddit, and if there are home-grown, analytical ways of addressing these questions.\n",
    "I'll be showing two ways of parsing submissions and comments to Reddit, this one focusing on using [pushshift API endpoints](http://pushshift.io/) using the `requests` library, some custom classes for processing these responses, and `asyncio` to handle asynchronous threading for multiple requests to pushshift.\n",
    "\n",
    "These codes ran quickly on my chromebook (dual-core, dual-thread, 1.90 Ghz, 4 Gb memory), but querying lots of data from pushshift makes some of the final cells take ~10 minutes.\n",
    "\n",
    "Note: at the time of putting this together, parts of pushshift appear to be down for repair/upgrade, but at least the [github repo](https://github.com/pushshift/api) is still online\n",
    "\n",
    "Raw notebook [here](https://github.com/ahy3nz/ahy3nz.github.io/tree/master/files/notebooks), but I didn't bother adding an environment -- most of these packages are in the python standard library or easily available on conda or pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import datetime as dt\n",
    "import asyncio\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At its core, we are submitting queries to a URL and getting responses to these queries. \n",
    "Technically speaking, this means we are submitting get requests to pushshift endpoints.\n",
    "\n",
    "The endpoint generally takes the form of something like \"https://api.pushshift.io/reddit/search/submission\", with the \"payload\" or `params` kwarg to our request being some set of search parameters (like a keyword, subreddit, or timestamp info), [pushshift API parameters here](https://pushshift.io/api-parameters/). \n",
    "With this endpoint, we're searching the Reddit submissions (not the comments)\n",
    "\n",
    "One of the simpler payloads could be searching a subreddit within a particular time window. \n",
    "This requires before and after timestamps, which can easily be handled with python's `datetime `library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = dt.datetime.today().replace(hour=8, minute=0, second=0, microsecond=0).timestamp()\n",
    "today_minus_seven = (dt.datetime.today().replace(hour=8, minute=0, second=0, microsecond=0) - \n",
    "                     dt.timedelta(days=7)).timestamp()\n",
    "today_minus_eight = (dt.datetime.today().replace(hour=8, minute=0, second=0, microsecond=0) - \n",
    "                     dt.timedelta(days=8)).timestamp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This the the actual get request, observe the URL as the main arg, and the various search parameters in the `params` kwarg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_response = requests.get(\"https://api.pushshift.io/reddit/search/submission\",\n",
    "                              params={'subreddit': 'stocks',\n",
    "                                      'before': int(today_minus_seven), \n",
    "                                      'after': int(today_minus_eight)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a variety of ways to parse [request responses](https://requests.readthedocs.io/en/master/), but here's one way to parse the title and text from the response to a Reddit submission get request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Would it be wise to increase the geographical diversity of my portfolio?',\n",
       " 'Hello everyone, \\n\\nMy portfolio of 16 companies consists of 13 US stocks because they all seem to have some of the highest potential returns but in the midst of the pandemic I feel I should reallocate some resources towards European and UK stocks. Is anyone watching any interesting non-US stocks at the moment?')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_response.json()['data'][0]['title'],reddit_response.json()['data'][0]['selftext'],"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a little bit of dressing on top, we can grab a list of stock tickers. \n",
    "There are a lot of sources to pull tickers from (`yfinance` is a popular one), but we can also pull a list of tickers from the SEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_response = requests.get(\"https://www.sec.gov/include/ticker.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = pd.read_csv(\n",
    "    io.StringIO(ticker_response.text), \n",
    "    delimiter='\\t', \n",
    "    header=None, \n",
    "    usecols=[0],\n",
    ")[0].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aapl', 'msft', 'amzn', 'goog', 'tcehy']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickers[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string                                                                         \n",
    "from typing import List, Union, Dict, Optional, Any \n",
    "from collections import Counter\n",
    "from requests import Response\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have all the raw information contained within the request response object, but for data processing purposes, we can define a class and some functions to simplify the work. \n",
    "\n",
    "Key characteristics:\n",
    "\n",
    "- A corresponding python object property for each relevant property of a typical reddit submission.\n",
    "    - Unfortuantely the `score` property from pushshift isn't the most reliable because it's only a snapshot from when the data were indexed\n",
    "- `summarize()` that uses `collections.Counter` to tally up how frequently a stock ticker appears\n",
    "- `to_dict()` for serialization and conversion for pandas\n",
    "- `from_response()` to quickly instantiate a `List[RedditSubmission]` from a single response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RedditSubmission:\n",
    "    title: str \n",
    "    body: str \n",
    "    permalink: str \n",
    "    author: str \n",
    "    score: float\n",
    "    timestamp: dt.datetime\n",
    "\n",
    "    def summarize(self, \n",
    "        tickers: List[str], \n",
    "        weighted: bool = True\n",
    "    ) -> Dict[str, Union[float, int]]:\n",
    "        \"\"\" Process RedditSubmission for tickers \n",
    "        \n",
    "        Use a Counter to count the number of times a ticker occurs.\n",
    "        Include some corrections for punctuation\n",
    "        \"\"\"\n",
    "        if self.title is not None:\n",
    "            title_no_punctuation = self.title.translate(\n",
    "                str.maketrans('', '', string.punctuation)\n",
    "            )\n",
    "            tickers_title = Counter(\n",
    "                filter(lambda x: x in tickers, title_no_punctuation.split())\n",
    "            )\n",
    "        else:\n",
    "            tickers_title = Counter()\n",
    "        if self.body is not None:\n",
    "            body_no_punctuation = self.body.translate(\n",
    "                str.maketrans('', '', string.punctuation)\n",
    "            )\n",
    "\n",
    "            tickers_body = Counter(\n",
    "                filter(lambda x: x in tickers, body_no_punctuation.split())\n",
    "            )\n",
    "        else:\n",
    "            tickers_body = Counter()\n",
    "        total_tickers = tickers_title + tickers_body\n",
    "        \n",
    "        return total_tickers\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'title': self.title,\n",
    "            'body': self.body,\n",
    "            'permalink': self.permalink,\n",
    "            'author': self.author,\n",
    "            'score': self.score,\n",
    "            'timestamp': self.timestamp\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_response(\n",
    "        cls, \n",
    "        resp_object: Response\n",
    "    ) -> Optional[List[Any]]:\n",
    "        \"\"\" Create a list of RedditSubmission objects from response\"\"\"\n",
    "        if resp_object.status_code == 200:\n",
    "            processed_response = [\n",
    "                cls(\n",
    "                    msg.get(\"title\", None),\n",
    "                    msg.get(\"body\", None),\n",
    "                    msg.get(\"permalink\", None),\n",
    "                    msg.get(\"author\", None),\n",
    "                    msg.get(\"score\", None),\n",
    "                    (\n",
    "                        dt.datetime.fromtimestamp(msg['created_utc']) \n",
    "                        if msg['created_utc'] is not None else None\n",
    "                    )\n",
    "                ) for msg in resp_object.json()['data']\n",
    "            ]\n",
    "            return processed_response\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reality, there's a decently-long wait time after we make the initial get request. \n",
    "The time to make and process the request is actually fairly quick, so this is a good opportunity to use python's [asyncio](https://docs.python.org/3/library/asyncio.html) library.\n",
    "\n",
    "Asyncio allows for concurrency in a different manner than multiprocessing or multithreading. \n",
    "You can have many tasks running, but only one is \"controlling\" the CPU, and gives up control when it's not actively doing any work (like waiting for a response from the pushshift server).\n",
    "\n",
    "The overall syntax is very similar to writing any other python function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def submission_request_coroutine(**kwargs):\n",
    "    await asyncio.sleep(5)\n",
    "    reddit_response = requests.get(\"https://api.pushshift.io/reddit/search/submission\",\n",
    "                              params=kwargs)\n",
    "    return reddit_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a range of timestamps, initialize an async coroutine for each timestamp, then use asyncio to submit each request and gather them back together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshots = pd.date_range(\n",
    "    start=dt.datetime.now(tz=dt.timezone.utc) - dt.timedelta(days=7),\n",
    "    end=dt.datetime.now(tz=dt.timezone.utc) - dt.timedelta(days=1),\n",
    "    freq='10min'\n",
    ")\n",
    "\n",
    "tasks = [\n",
    "    submission_request_coroutine(subreddit='stocks', \n",
    "                 after=int(snapshot.timestamp()),\n",
    "                 before=int(snapshots[i+1].timestamp()),\n",
    "                 size=10\n",
    "                ) \n",
    "    for i, snapshot in enumerate(snapshots[:-1])\n",
    "]\n",
    "all_submission_responses = await asyncio.gather(\n",
    "    *tasks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is a `List[Response]` objects, which we can conver to a `List[List[RedditSubmission]]`, then flatten as a `List[RedditSubmission]` with itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "\n",
    "reddit_submissions = [*it.chain.from_iterable(\n",
    "    RedditSubmission.from_response(resp) for resp in all_submission_responses\n",
    "    if resp.status_code == 200\n",
    ")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a ticker counter for each `RedditSubmission`, but we'd like to quickly aggregate them all into a single, summary ticker counter over all the reddit submission in our time window. \n",
    "This can be easily achieved with `functools.reduce`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from collections import Counter\n",
    "\n",
    "def aggregate_dictionaries(d1, d2):\n",
    "    \"\"\" Given two dictionaries, aggregate key-value pairs \"\"\"\n",
    "    if len(d1) == 0:\n",
    "        return dict(Counter(**d2).most_common())\n",
    "    my_counter = Counter(**d1)\n",
    "    my_counter.update(d2)\n",
    "    return dict(my_counter.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions_breakdown = reduce(\n",
    "    aggregate_dictionaries, \n",
    "    (submission.summarize(tickers) for submission in reddit_submissions)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the list of tickers from the SEC was pretty generous (\\$A appears to be a ticker), but we can subselect for some of the recent trending tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 11)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submissions_breakdown['gme'], submissions_breakdown['amc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 322,\n",
       " 'on': 234,\n",
       " 'for': 181,\n",
       " 'it': 105,\n",
       " 'or': 76,\n",
       " 'be': 76,\n",
       " 'next': 71,\n",
       " 'are': 62,\n",
       " 'new': 56,\n",
       " 'good': 54,\n",
       " 'now': 53,\n",
       " 'can': 52,\n",
       " 'all': 49,\n",
       " 'at': 45,\n",
       " 'out': 40,\n",
       " 'amp': 34,\n",
       " 'an': 33,\n",
       " 'by': 31,\n",
       " 'go': 30,\n",
       " 'has': 26,\n",
       " 'am': 24,\n",
       " 'any': 22,\n",
       " 'when': 21,\n",
       " 'best': 20,\n",
       " 'vs': 20,\n",
       " 'one': 19,\n",
       " 'so': 18,\n",
       " 'gme': 18,\n",
       " 'big': 17,\n",
       " 'free': 15,\n",
       " 'play': 13,\n",
       " 'apps': 13,\n",
       " 'amc': 11,\n",
       " 'cash': 10,\n",
       " 'see': 10,\n",
       " 'find': 9,\n",
       " 'run': 8,\n",
       " 'rise': 7,\n",
       " 'else': 7,\n",
       " 'ever': 7,\n",
       " 'work': 6,\n",
       " 'real': 6,\n",
       " 'open': 6,\n",
       " 'wall': 5,\n",
       " 'fund': 5,\n",
       " 'post': 5,\n",
       " 'love': 5,\n",
       " 'well': 5,\n",
       " 'very': 5,\n",
       " 'ago': 5,\n",
       " 'info': 5,\n",
       " 'plan': 5,\n",
       " 'pay': 5,\n",
       " 'bit': 5,\n",
       " 'ride': 4,\n",
       " 'life': 4,\n",
       " 'huge': 4,\n",
       " 'low': 4,\n",
       " 'nok': 4,\n",
       " 'grow': 4,\n",
       " 'cap': 4,\n",
       " 'link': 3,\n",
       " 'safe': 3,\n",
       " 'plus': 3,\n",
       " 'fast': 3,\n",
       " 'stay': 3,\n",
       " 'tech': 3,\n",
       " 'fun': 3,\n",
       " 'he': 3,\n",
       " 'step': 3,\n",
       " 'turn': 3,\n",
       " 'live': 3,\n",
       " 'site': 3,\n",
       " 'ways': 3,\n",
       " 'hear': 2,\n",
       " 'teva': 2,\n",
       " 'bb': 2,\n",
       " 'co': 2,\n",
       " 'boom': 2,\n",
       " 'nice': 2,\n",
       " 'mass': 2,\n",
       " 'peak': 2,\n",
       " 'max': 2,\n",
       " 'wash': 2,\n",
       " 'pump': 2,\n",
       " 'tell': 2,\n",
       " 'fly': 2,\n",
       " 'pros': 2,\n",
       " 'rock': 1,\n",
       " 'both': 1,\n",
       " 'gt': 1,\n",
       " 'loan': 1,\n",
       " 'nga': 1,\n",
       " 'invu': 1,\n",
       " 'most': 1,\n",
       " 'ofc': 1,\n",
       " 'nio': 1,\n",
       " 'spot': 1,\n",
       " 'min': 1,\n",
       " 'onto': 1,\n",
       " 'evfm': 1,\n",
       " 'blue': 1,\n",
       " 'nat': 1,\n",
       " 'pure': 1,\n",
       " 'sign': 1,\n",
       " 'man': 1,\n",
       " 'st': 1,\n",
       " 'de': 1,\n",
       " 'w': 1,\n",
       " 'trtc': 1,\n",
       " 'form': 1,\n",
       " 'hi': 1,\n",
       " 'joe': 1,\n",
       " 'true': 1,\n",
       " 'home': 1,\n",
       " 'vrs': 1,\n",
       " 'med': 1,\n",
       " 'sqz': 1,\n",
       " 'five': 1,\n",
       " 'ship': 1,\n",
       " 'trxc': 1,\n",
       " 'wish': 1,\n",
       " 're': 1,\n",
       " 'car': 1,\n",
       " 'nakd': 1,\n",
       " 'rkt': 1,\n",
       " 'flex': 1,\n",
       " 'pm': 1,\n",
       " 'ppl': 1,\n",
       " 'earn': 1,\n",
       " 'flow': 1,\n",
       " 'lscc': 1,\n",
       " 'peg': 1,\n",
       " 'two': 1,\n",
       " 'gain': 1,\n",
       " 'wow': 1,\n",
       " 'pro': 1,\n",
       " 'team': 1,\n",
       " 'fix': 1,\n",
       " 'fnko': 1,\n",
       " 'et': 1,\n",
       " 'al': 1,\n",
       " 'muh': 1,\n",
       " 'save': 1,\n",
       " 'gold': 1,\n",
       " 'beat': 1,\n",
       " 'vive': 1,\n",
       " 'u': 1,\n",
       " 'rh': 1,\n",
       " 'x': 1,\n",
       " 'vxrt': 1,\n",
       " 'mind': 1,\n",
       " 'ehth': 1,\n",
       " 'job': 1,\n",
       " 'road': 1,\n",
       " 'box': 1}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submissions_breakdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, if we're not interested in the tickers that occur, we can still boil all the data into a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(a.to_dict() for a in reddit_submissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>permalink</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KSTR ETF \"The nasdaq of china\"</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/stocks/comments/l664ce/kstr_etf_the_nasdaq_...</td>\n",
       "      <td>GioDesa</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-27 09:56:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Opinions/Projections on AMC?</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/stocks/comments/l665a0/opinionsprojections_...</td>\n",
       "      <td>Double_jn_it</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-27 09:58:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GE, SPCE, &amp;amp; PLUG</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/stocks/comments/l6668r/ge_spce_plug/</td>\n",
       "      <td>_MeatLoafLover</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-27 09:59:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Reddit is under DDOS attack. Certain gaming re...</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/stocks/comments/l66692/reddit_is_under_ddos...</td>\n",
       "      <td>theBacillus</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-27 09:59:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#GainStock</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/stocks/comments/l66777/gainstock/</td>\n",
       "      <td>lxPHENOMENONxl</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-27 10:00:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2338</th>\n",
       "      <td>AN OPEN LETTER TO GAMESTOP CEO</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/stocks/comments/l98k85/an_open_letter_to_ga...</td>\n",
       "      <td>Artuhan</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-31 03:55:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2339</th>\n",
       "      <td>AN OPEN LETTER TO GAMESTOP CEO</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/stocks/comments/l98lai/an_open_letter_to_ga...</td>\n",
       "      <td>Artuhan</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-31 03:58:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2340</th>\n",
       "      <td>Thoughts on YOLO (AdvisorShares Pure Cannabis ...</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/stocks/comments/l98nly/thoughts_on_yolo_adv...</td>\n",
       "      <td>ConfidentProgrammer1</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-31 04:02:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2341</th>\n",
       "      <td>Daily advice</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/stocks/comments/l98pic/daily_advice/</td>\n",
       "      <td>Bukprotingas</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-31 04:06:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2342</th>\n",
       "      <td>AMC- Next stop?</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/stocks/comments/l98pif/amc_next_stop/</td>\n",
       "      <td>Hj-Fish</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-31 04:06:24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2343 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  body  \\\n",
       "0                        KSTR ETF \"The nasdaq of china\"  None   \n",
       "1                          Opinions/Projections on AMC?  None   \n",
       "2                                  GE, SPCE, &amp; PLUG  None   \n",
       "3     Reddit is under DDOS attack. Certain gaming re...  None   \n",
       "4                                            #GainStock  None   \n",
       "...                                                 ...   ...   \n",
       "2338                     AN OPEN LETTER TO GAMESTOP CEO  None   \n",
       "2339                     AN OPEN LETTER TO GAMESTOP CEO  None   \n",
       "2340  Thoughts on YOLO (AdvisorShares Pure Cannabis ...  None   \n",
       "2341                                       Daily advice  None   \n",
       "2342                                    AMC- Next stop?  None   \n",
       "\n",
       "                                              permalink                author  \\\n",
       "0     /r/stocks/comments/l664ce/kstr_etf_the_nasdaq_...               GioDesa   \n",
       "1     /r/stocks/comments/l665a0/opinionsprojections_...          Double_jn_it   \n",
       "2               /r/stocks/comments/l6668r/ge_spce_plug/        _MeatLoafLover   \n",
       "3     /r/stocks/comments/l66692/reddit_is_under_ddos...           theBacillus   \n",
       "4                  /r/stocks/comments/l66777/gainstock/        lxPHENOMENONxl   \n",
       "...                                                 ...                   ...   \n",
       "2338  /r/stocks/comments/l98k85/an_open_letter_to_ga...               Artuhan   \n",
       "2339  /r/stocks/comments/l98lai/an_open_letter_to_ga...               Artuhan   \n",
       "2340  /r/stocks/comments/l98nly/thoughts_on_yolo_adv...  ConfidentProgrammer1   \n",
       "2341            /r/stocks/comments/l98pic/daily_advice/          Bukprotingas   \n",
       "2342           /r/stocks/comments/l98pif/amc_next_stop/               Hj-Fish   \n",
       "\n",
       "      score           timestamp  \n",
       "0         1 2021-01-27 09:56:46  \n",
       "1         1 2021-01-27 09:58:03  \n",
       "2         1 2021-01-27 09:59:21  \n",
       "3         1 2021-01-27 09:59:22  \n",
       "4         1 2021-01-27 10:00:19  \n",
       "...     ...                 ...  \n",
       "2338      1 2021-01-31 03:55:51  \n",
       "2339      1 2021-01-31 03:58:05  \n",
       "2340      1 2021-01-31 04:02:29  \n",
       "2341      1 2021-01-31 04:06:24  \n",
       "2342      1 2021-01-31 04:06:24  \n",
       "\n",
       "[2343 rows x 6 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next up\n",
    "\n",
    "While we just built our own Reddit API from some fundamental python libraries, there are more sophisticated API out there that do a better job of querying Reddit, like [praw](https://praw.readthedocs.io/en/latest/), and then we could try some other things like sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
