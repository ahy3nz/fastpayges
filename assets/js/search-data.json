{
  
    
        "post0": {
            "title": "My first post with fastpages",
            "content": "print(&quot;Hello world&quot;) . Hello world . import pandas as pd import numpy as np pd.DataFrame({ &#39;a&#39;: np.random.sample(10), &#39;b&#39;: np.random.sample(10)/2 }) . a b . 0 0.929214 | 0.464544 | . 1 0.521751 | 0.051030 | . 2 0.777646 | 0.028839 | . 3 0.270206 | 0.020832 | . 4 0.061954 | 0.208365 | . 5 0.146290 | 0.074646 | . 6 0.994034 | 0.262676 | . 7 0.527353 | 0.027162 | . 8 0.392875 | 0.451405 | . 9 0.386635 | 0.337624 | .",
            "url": "https://ahy3nz.github.io/fastpayges/data%20science/2021/04/10/first.html",
            "relUrl": "/data%20science/2021/04/10/first.html",
            "date": " ‚Ä¢ Apr 10, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Scraping Reddit, part 2",
            "content": "The last post dealt with using pushshift and handling requests to access posts and comments from Reddit. This post deals with using the Python Reddit API wrapper to accces posts and comments from Reddit and then using some NLP tools for some basic sentiment analysis. . There is some work to set up an application to use praw with oauth, but straightforward enough for anyone who&#39;s just using this as a script. . After setting up the praw application, we can build up a small pipeline: . Use praw to download posts and comments from r/nba | Format them into a dataframe | Use huggingface and spacy for sentiment analysis | from dataclasses import dataclass import itertools as it from functools import reduce, partial import datetime as dt import pandas as pd pd.set_option(&#39;display.max_colwidth&#39;, 150) import praw from praw.models import MoreComments import matplotlib.pyplot as plt import hfapi import spacy from spacytextblob.spacytextblob import SpacyTextBlob nlp = spacy.load(&quot;en_core_web_sm&quot;) spacy_text_blob = SpacyTextBlob() nlp.add_pipe(spacy_text_blob) client = hfapi.Client() . reddit = praw.Reddit(&quot;bot1&quot;) # Pulls from praw.ini file rnba = reddit.subreddit(&#39;nba&#39;) . Compiling praw objects into a dataframe . @dataclass class RedditSubmission: title: str body: str permalink: str author: str score: float timestamp: dt.datetime def to_dict(self): return { &#39;title&#39;: self.title, &#39;body&#39;: self.body, &#39;permalink&#39;: self.permalink, &#39;author&#39;: self.author, &#39;score&#39;: self.score, &#39;timestamp&#39;: self.timestamp } @classmethod def from_praw_submission( cls, praw_submission: praw.models.Submission ): return cls( praw_submission.title, praw_submission.selftext, praw_submission.permalink, praw_submission.author, praw_submission.score, dt.datetime.fromtimestamp(praw_submission.created_utc) ) @dataclass class RedditComment: body: str permalink: str author: str score: float timestamp: dt.datetime def to_dict(self): return { &#39;body&#39;: self.body, &#39;permalink&#39;: self.permalink, &#39;author&#39;: self.author, &#39;score&#39;: self.score, &#39;timestamp&#39;: self.timestamp } @classmethod def from_praw_comment( cls, praw_comment: praw.models.Comment ): return cls( praw_comment.body, praw_comment.permalink, praw_comment.author, praw_comment.score, dt.datetime.fromtimestamp(praw_comment.created_utc) ) def process_submission_from_praw(praw_submission_generator): for praw_submission in praw_submission_generator: yield RedditSubmission.from_praw_submission(praw_submission) def process_comment_from_praw_submission(praw_submission_generator): for praw_submission in praw_submission_generator: for praw_comment in praw_submission.comments: if isinstance(praw_comment, MoreComments): continue else: yield RedditComment.from_praw_comment(praw_comment) . praw_submission_generator1 = rnba.hot(limit=100) praw_submission_generator2 = rnba.hot(limit=100) submissions = process_submission_from_praw(praw_submission_generator1) comments = process_comment_from_praw_submission(praw_submission_generator2) . submission_df = pd.DataFrame(a.to_dict() for a in submissions) comment_df = pd.DataFrame(a.to_dict() for a in comments) . Using huggingface for sentiment analysis . Specifically, using huggingface api . def classification_single_body(client, sentence): classification = client.text_classification(sentence) if &#39;error&#39; in classification: return None, None neg_sentiment, pos_sentiment = classification[0] return neg_sentiment[&#39;score&#39;], pos_sentiment[&#39;score&#39;] def classification_multiple_body(client, bunch_of_sentences, colnames=None): if colnames is None: colnames = [&#39;negative_score&#39;, &#39;positive_score&#39;] df = pd.DataFrame( map(lambda x: classification_single_body(client, x), bunch_of_sentences), columns=colnames ) return df client = hfapi.Client() classification_multiple_bodies_partial = partial(classification_multiple_body, client) . submission_df = pd.concat([ submission_df, classification_multiple_bodies_partial(submission_df[&#39;title&#39;].to_list()) ], axis=1) . Scoring the submissions, here&#39;s a title with an appropriately positive score &quot;Nikola Jokic leads the league in offensive win shares at 8.9. This is also more than any player&#39;s OVERALL win shares for the current season.&quot; . Here&#39;s a title that is scored as incredibly negative, but in reality is pretty positive &quot;Kyrie Irving needs one more 3 point make to enter the 50-40-90 club for the 2020-2021 season&quot; -- being even close to the 50-40-90 club is incredible . submission_df.sort_values(&quot;negative_score&quot;)[[&#39;title&#39;, &#39;score&#39;, &#39;negative_score&#39;, &#39;positive_score&#39;]] . title score negative_score positive_score . 19 [Orsborn]: Mike Malone on Pop still going strong at 72: &quot;For him to be as engaged and as locked in and as committed as he is at this juncture of h... | 241 | 0.000185 | 0.999816 | . 8 Kevin Durant: ‚ÄúStephen Curry and Klay Thompson are the best shooters I‚Äôve played with.‚Äù | 1610 | 0.000185 | 0.999815 | . 12 [Thinking Basketball] The 10 Best NBA peaks since 1977 | 1346 | 0.000283 | 0.999717 | . 25 [Highlight] Russell banks in the 3 to tie it at 124 | 92 | 0.000615 | 0.999385 | . 23 Nikola Jokic leads the league in offensive win shares at 8.9. This is also more than any player&#39;s OVERALL win shares for the current season. | 406 | 0.000845 | 0.999155 | . ... ... | ... | ... | ... | . 15 Charles Barkley: &quot;I&#39;ve been poor, I&#39;ve been rich, I&#39;ve been fat, I&#39;ve been in the Hall of Fame, and one thing I can tell you is that the Clippers ... | 23341 | 0.999229 | 0.000771 | . 38 Kyrie Irving needs one more 3 point make to enter the 50-40-90 club for the 2020-2021 season | 443 | 0.999282 | 0.000718 | . 75 [Stein] The Bucks&#39; too-long-to-list-it-all injury report tonight against Charlotte includes no Giannis Antetokounmpo (left knee soreness) or Jrue ... | 43 | 0.999286 | 0.000714 | . 40 Bucks missing all five starters against Hornets | 79 | 0.999449 | 0.000551 | . 93 China‚Äôs Forced-Labor Backlash Threatens to Put N.B.A. in Unwanted Spotlight | 174 | 0.999517 | 0.000483 | . 100 rows √ó 4 columns . I think we were querying the API too quickly, so these responses started timing out, but you get the idea here . comment_df = pd.concat([ comment_df, classification_multiple_bodies_partial(comment_df[&#39;body&#39;].to_list()) ], axis=1) . Using spacy for sentiment analysis . submission_df[&#39;title_sentiment&#39;] = [*map(lambda x: x._.sentiment.polarity, nlp.pipe(submission_df[&#39;title&#39;]))] submission_df[&#39;body_sentiment&#39;] = [*map(lambda x: x._.sentiment.polarity, nlp.pipe(submission_df[&#39;body&#39;]))] comment_df[&#39;body_sentiment&#39;] = [*map(lambda x: x._.sentiment.polarity, nlp.pipe(comment_df[&#39;body&#39;]))] . Here&#39;s a simple title to score &quot;Kevin Durant: ‚ÄúStephen Curry and Klay Thompson are the best shooters I‚Äôve played with.‚Äù&quot; . submission_df[[&#39;title&#39;, &#39;score&#39;, &#39;title_sentiment&#39;]].sort_values(&quot;title_sentiment&quot;) . title score title_sentiment . 99 The Mavs will play 3 back-to-backs over a 7 game span to start April. Over April and May, 62% of their games will be part of a b2b | 15 | -0.400000 | . 83 [Post Game Thread] The Los Angeles Clippers (35-18) defeat the Phoenix Suns (36-15), 113 - 103 | 727 | -0.400000 | . 43 [Post Game Thread] The Boston Celtics (27-26) defeat the Minnesota Timberwolves (13-40) in OT, 145 - 136 | 49 | -0.400000 | . 91 [Post Game Thread] The Dallas Mavericks (29-22) defeat the Milwaukee Bucks (32-19), 116 - 101 | 754 | -0.400000 | . 37 The Denver Nuggets came onto the floor for their game against the Spurs with &quot;X Gon&#39; Give it to Ya&quot; playing in the background | 88 | -0.400000 | . ... ... | ... | ... | . 19 [Orsborn]: Mike Malone on Pop still going strong at 72: &quot;For him to be as engaged and as locked in and as committed as he is at this juncture of h... | 241 | 0.505556 | . 18 Steve Kerr on leaving the Warriors: ‚ÄúI have a great job right now. I love coaching the Warriors, so I&#39;m not going anywhere.‚Äù | 465 | 0.528571 | . 84 [Highlight] Cody Zeller perfectly blocks Sam Merrill&#39;s layup off the backboard | 15 | 1.000000 | . 8 Kevin Durant: ‚ÄúStephen Curry and Klay Thompson are the best shooters I‚Äôve played with.‚Äù | 1610 | 1.000000 | . 12 [Thinking Basketball] The 10 Best NBA peaks since 1977 | 1346 | 1.000000 | . 100 rows √ó 3 columns . I want to point out one comment &quot;Goes off üòéüòé in OT ‚åõ‚åõ against the worst team in the league üê∫üê∫&quot;, which has a negative sentiment, probably because of the words &quot;off&quot; and &quot;words&quot;, but the sentence itself is more positive because it&#39;s about a player performing very well . comment_df[[&#39;body&#39;, &#39;score&#39;, &#39;body_sentiment&#39;]].sort_values(&quot;body_sentiment&quot;) . body score body_sentiment . 2480 he has some of the worst luck with injuries. | 591 | -1.0 | . 118 I tea bagged your fucking drum set!!! | 3 | -1.0 | . 2081 RIP to the insane plus/minus of the Spurs bench | 71 | -1.0 | . 1379 Goes off üòéüòé in OT ‚åõ‚åõ against the worst team in the league üê∫üê∫ | 1 | -1.0 | . 1287 fucking disgusting | 1 | -1.0 | . ... ... | ... | ... | . 2270 Perfect.... boost his confidence, while we continue to tank | 5 | 1.0 | . 273 It‚Äôs almost like he‚Äôs one of the best point guards of all time! | 2 | 1.0 | . 31 Best scorer on the Bulls since MJ | 120 | 1.0 | . 1632 Remember when DSJ was like the mavs best player? What a time | 1 | 1.0 | . 436 I will zag and point out another thing here. KD doesn&#39;t want to outright say Steph is the greatest shooter ever. He needs to add Klay to this stat... | -1 | 1.0 | . 3200 rows √ó 3 columns . Closing remarks . Thanks to praw, it was really easy to pull and gather raw data. On top of that, the plethora of NLP software development has made it really easy to apply these models to whatever context you want. . To really take this further, an important middle step would need data cleaning (modifying for typos, slang, abbreviations), maybe filters/named entity resolution to look for specific players. Maybe you want to find some way to add weights to highly up-voted submissions/comments, or maybe you want some way to combine the sentiments from both submissions and comments. Lastly, the big caveat in NLP for reddit is using a language model sophisticated enough to capture the sarcasm, nuance, and toxicity that is the reddit community (and specifically within r/nba). .",
            "url": "https://ahy3nz.github.io/fastpayges/data%20science/basketball/2021/04/09/reddit2.html",
            "relUrl": "/data%20science/basketball/2021/04/09/reddit2.html",
            "date": " ‚Ä¢ Apr 9, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Scraping Reddit, part 1",
            "content": "In light of recent internet trends about retail investors, I&#39;m sure many of us have questions about the kinds of content that gets posted on reddit, and if there are home-grown, analytical ways of addressing these questions. I&#39;ll be showing two ways of parsing submissions and comments to Reddit, this one focusing on using pushshift API endpoints using the requests library, some custom classes for processing these responses, and asyncio to handle asynchronous threading for multiple requests to pushshift. . These codes ran quickly on my chromebook (dual-core, dual-thread, 1.90 Ghz, 4 Gb memory), but querying lots of data from pushshift makes some of the final cells take ~10 minutes. . Note: at the time of putting this together, parts of pushshift appear to be down for repair/upgrade, but at least the github repo is still online . Raw notebook here, but I didn&#39;t bother adding an environment -- most of these packages are in the python standard library or easily available on conda or pip . import pandas as pd import requests import datetime as dt import asyncio import io . At its core, we are submitting queries to a URL and getting responses to these queries. Technically speaking, this means we are submitting get requests to pushshift endpoints. . The endpoint generally takes the form of something like &quot;https://api.pushshift.io/reddit/search/submission&quot;, with the &quot;payload&quot; or params kwarg to our request being some set of search parameters (like a keyword, subreddit, or timestamp info), pushshift API parameters here. With this endpoint, we&#39;re searching the Reddit submissions (not the comments) . One of the simpler payloads could be searching a subreddit within a particular time window. This requires before and after timestamps, which can easily be handled with python&#39;s datetimelibrary . today = dt.datetime.today().replace(hour=8, minute=0, second=0, microsecond=0).timestamp() today_minus_seven = (dt.datetime.today().replace(hour=8, minute=0, second=0, microsecond=0) - dt.timedelta(days=7)).timestamp() today_minus_eight = (dt.datetime.today().replace(hour=8, minute=0, second=0, microsecond=0) - dt.timedelta(days=8)).timestamp() . This the the actual get request, observe the URL as the main arg, and the various search parameters in the params kwarg . reddit_response = requests.get(&quot;https://api.pushshift.io/reddit/search/submission&quot;, params={&#39;subreddit&#39;: &#39;stocks&#39;, &#39;before&#39;: int(today_minus_seven), &#39;after&#39;: int(today_minus_eight)}) . reddit_response.status_code . 200 . There are a variety of ways to parse request responses, but here&#39;s one way to parse the title and text from the response to a Reddit submission get request . reddit_response.json()[&#39;data&#39;][0][&#39;title&#39;],reddit_response.json()[&#39;data&#39;][0][&#39;selftext&#39;], . (&#39;Would it be wise to increase the geographical diversity of my portfolio?&#39;, &#39;Hello everyone, n nMy portfolio of 16 companies consists of 13 US stocks because they all seem to have some of the highest potential returns but in the midst of the pandemic I feel I should reallocate some resources towards European and UK stocks. Is anyone watching any interesting non-US stocks at the moment?&#39;) . As a little bit of dressing on top, we can grab a list of stock tickers. There are a lot of sources to pull tickers from (yfinance is a popular one), but we can also pull a list of tickers from the SEC . ticker_response = requests.get(&quot;https://www.sec.gov/include/ticker.txt&quot;) . tickers = pd.read_csv( io.StringIO(ticker_response.text), delimiter=&#39; t&#39;, header=None, usecols=[0], )[0].to_list() . tickers[:5] . [&#39;aapl&#39;, &#39;msft&#39;, &#39;amzn&#39;, &#39;goog&#39;, &#39;tcehy&#39;] . import string from typing import List, Union, Dict, Optional, Any from collections import Counter from requests import Response from dataclasses import dataclass . We have all the raw information contained within the request response object, but for data processing purposes, we can define a class and some functions to simplify the work. . Key characteristics: . A corresponding python object property for each relevant property of a typical reddit submission. Unfortuantely the score property from pushshift isn&#39;t the most reliable because it&#39;s only a snapshot from when the data were indexed | . | summarize() that uses collections.Counter to tally up how frequently a stock ticker appears | to_dict() for serialization and conversion for pandas | from_response() to quickly instantiate a List[RedditSubmission] from a single response | . @dataclass class RedditSubmission: title: str body: str permalink: str author: str score: float timestamp: dt.datetime def summarize(self, tickers: List[str], weighted: bool = True ) -&gt; Dict[str, Union[float, int]]: &quot;&quot;&quot; Process RedditSubmission for tickers Use a Counter to count the number of times a ticker occurs. Include some corrections for punctuation &quot;&quot;&quot; if self.title is not None: title_no_punctuation = self.title.translate( str.maketrans(&#39;&#39;, &#39;&#39;, string.punctuation) ) tickers_title = Counter( filter(lambda x: x in tickers, title_no_punctuation.split()) ) else: tickers_title = Counter() if self.body is not None: body_no_punctuation = self.body.translate( str.maketrans(&#39;&#39;, &#39;&#39;, string.punctuation) ) tickers_body = Counter( filter(lambda x: x in tickers, body_no_punctuation.split()) ) else: tickers_body = Counter() total_tickers = tickers_title + tickers_body return total_tickers def to_dict(self): return { &#39;title&#39;: self.title, &#39;body&#39;: self.body, &#39;permalink&#39;: self.permalink, &#39;author&#39;: self.author, &#39;score&#39;: self.score, &#39;timestamp&#39;: self.timestamp } @classmethod def from_response( cls, resp_object: Response ) -&gt; Optional[List[Any]]: &quot;&quot;&quot; Create a list of RedditSubmission objects from response&quot;&quot;&quot; if resp_object.status_code == 200: processed_response = [ cls( msg.get(&quot;title&quot;, None), msg.get(&quot;body&quot;, None), msg.get(&quot;permalink&quot;, None), msg.get(&quot;author&quot;, None), msg.get(&quot;score&quot;, None), ( dt.datetime.fromtimestamp(msg[&#39;created_utc&#39;]) if msg[&#39;created_utc&#39;] is not None else None ) ) for msg in resp_object.json()[&#39;data&#39;] ] return processed_response else: return None . In reality, there&#39;s a decently-long wait time after we make the initial get request. The time to make and process the request is actually fairly quick, so this is a good opportunity to use python&#39;s asyncio library. . Asyncio allows for concurrency in a different manner than multiprocessing or multithreading. You can have many tasks running, but only one is &quot;controlling&quot; the CPU, and gives up control when it&#39;s not actively doing any work (like waiting for a response from the pushshift server). . The overall syntax is very similar to writing any other python function . async def submission_request_coroutine(**kwargs): await asyncio.sleep(5) reddit_response = requests.get(&quot;https://api.pushshift.io/reddit/search/submission&quot;, params=kwargs) return reddit_response . Define a range of timestamps, initialize an async coroutine for each timestamp, then use asyncio to submit each request and gather them back together . snapshots = pd.date_range( start=dt.datetime.now(tz=dt.timezone.utc) - dt.timedelta(days=7), end=dt.datetime.now(tz=dt.timezone.utc) - dt.timedelta(days=1), freq=&#39;10min&#39; ) tasks = [ submission_request_coroutine(subreddit=&#39;stocks&#39;, after=int(snapshot.timestamp()), before=int(snapshots[i+1].timestamp()), size=10 ) for i, snapshot in enumerate(snapshots[:-1]) ] all_submission_responses = await asyncio.gather( *tasks ) . The data is a List[Response] objects, which we can conver to a List[List[RedditSubmission]], then flatten as a List[RedditSubmission] with itertools . import itertools as it reddit_submissions = [*it.chain.from_iterable( RedditSubmission.from_response(resp) for resp in all_submission_responses if resp.status_code == 200 )] . We can get a ticker counter for each RedditSubmission, but we&#39;d like to quickly aggregate them all into a single, summary ticker counter over all the reddit submission in our time window. This can be easily achieved with functools.reduce . from functools import reduce from collections import Counter def aggregate_dictionaries(d1, d2): &quot;&quot;&quot; Given two dictionaries, aggregate key-value pairs &quot;&quot;&quot; if len(d1) == 0: return dict(Counter(**d2).most_common()) my_counter = Counter(**d1) my_counter.update(d2) return dict(my_counter.most_common()) . submissions_breakdown = reduce( aggregate_dictionaries, (submission.summarize(tickers) for submission in reddit_submissions) ) . It seems the list of tickers from the SEC was pretty generous ( $A appears to be a ticker), but we can subselect for some of the recent trending tickers . submissions_breakdown[&#39;gme&#39;], submissions_breakdown[&#39;amc&#39;] . (18, 11) . submissions_breakdown . {&#39;a&#39;: 322, &#39;on&#39;: 234, &#39;for&#39;: 181, &#39;it&#39;: 105, &#39;or&#39;: 76, &#39;be&#39;: 76, &#39;next&#39;: 71, &#39;are&#39;: 62, &#39;new&#39;: 56, &#39;good&#39;: 54, &#39;now&#39;: 53, &#39;can&#39;: 52, &#39;all&#39;: 49, &#39;at&#39;: 45, &#39;out&#39;: 40, &#39;amp&#39;: 34, &#39;an&#39;: 33, &#39;by&#39;: 31, &#39;go&#39;: 30, &#39;has&#39;: 26, &#39;am&#39;: 24, &#39;any&#39;: 22, &#39;when&#39;: 21, &#39;best&#39;: 20, &#39;vs&#39;: 20, &#39;one&#39;: 19, &#39;so&#39;: 18, &#39;gme&#39;: 18, &#39;big&#39;: 17, &#39;free&#39;: 15, &#39;play&#39;: 13, &#39;apps&#39;: 13, &#39;amc&#39;: 11, &#39;cash&#39;: 10, &#39;see&#39;: 10, &#39;find&#39;: 9, &#39;run&#39;: 8, &#39;rise&#39;: 7, &#39;else&#39;: 7, &#39;ever&#39;: 7, &#39;work&#39;: 6, &#39;real&#39;: 6, &#39;open&#39;: 6, &#39;wall&#39;: 5, &#39;fund&#39;: 5, &#39;post&#39;: 5, &#39;love&#39;: 5, &#39;well&#39;: 5, &#39;very&#39;: 5, &#39;ago&#39;: 5, &#39;info&#39;: 5, &#39;plan&#39;: 5, &#39;pay&#39;: 5, &#39;bit&#39;: 5, &#39;ride&#39;: 4, &#39;life&#39;: 4, &#39;huge&#39;: 4, &#39;low&#39;: 4, &#39;nok&#39;: 4, &#39;grow&#39;: 4, &#39;cap&#39;: 4, &#39;link&#39;: 3, &#39;safe&#39;: 3, &#39;plus&#39;: 3, &#39;fast&#39;: 3, &#39;stay&#39;: 3, &#39;tech&#39;: 3, &#39;fun&#39;: 3, &#39;he&#39;: 3, &#39;step&#39;: 3, &#39;turn&#39;: 3, &#39;live&#39;: 3, &#39;site&#39;: 3, &#39;ways&#39;: 3, &#39;hear&#39;: 2, &#39;teva&#39;: 2, &#39;bb&#39;: 2, &#39;co&#39;: 2, &#39;boom&#39;: 2, &#39;nice&#39;: 2, &#39;mass&#39;: 2, &#39;peak&#39;: 2, &#39;max&#39;: 2, &#39;wash&#39;: 2, &#39;pump&#39;: 2, &#39;tell&#39;: 2, &#39;fly&#39;: 2, &#39;pros&#39;: 2, &#39;rock&#39;: 1, &#39;both&#39;: 1, &#39;gt&#39;: 1, &#39;loan&#39;: 1, &#39;nga&#39;: 1, &#39;invu&#39;: 1, &#39;most&#39;: 1, &#39;ofc&#39;: 1, &#39;nio&#39;: 1, &#39;spot&#39;: 1, &#39;min&#39;: 1, &#39;onto&#39;: 1, &#39;evfm&#39;: 1, &#39;blue&#39;: 1, &#39;nat&#39;: 1, &#39;pure&#39;: 1, &#39;sign&#39;: 1, &#39;man&#39;: 1, &#39;st&#39;: 1, &#39;de&#39;: 1, &#39;w&#39;: 1, &#39;trtc&#39;: 1, &#39;form&#39;: 1, &#39;hi&#39;: 1, &#39;joe&#39;: 1, &#39;true&#39;: 1, &#39;home&#39;: 1, &#39;vrs&#39;: 1, &#39;med&#39;: 1, &#39;sqz&#39;: 1, &#39;five&#39;: 1, &#39;ship&#39;: 1, &#39;trxc&#39;: 1, &#39;wish&#39;: 1, &#39;re&#39;: 1, &#39;car&#39;: 1, &#39;nakd&#39;: 1, &#39;rkt&#39;: 1, &#39;flex&#39;: 1, &#39;pm&#39;: 1, &#39;ppl&#39;: 1, &#39;earn&#39;: 1, &#39;flow&#39;: 1, &#39;lscc&#39;: 1, &#39;peg&#39;: 1, &#39;two&#39;: 1, &#39;gain&#39;: 1, &#39;wow&#39;: 1, &#39;pro&#39;: 1, &#39;team&#39;: 1, &#39;fix&#39;: 1, &#39;fnko&#39;: 1, &#39;et&#39;: 1, &#39;al&#39;: 1, &#39;muh&#39;: 1, &#39;save&#39;: 1, &#39;gold&#39;: 1, &#39;beat&#39;: 1, &#39;vive&#39;: 1, &#39;u&#39;: 1, &#39;rh&#39;: 1, &#39;x&#39;: 1, &#39;vxrt&#39;: 1, &#39;mind&#39;: 1, &#39;ehth&#39;: 1, &#39;job&#39;: 1, &#39;road&#39;: 1, &#39;box&#39;: 1} . Lastly, if we&#39;re not interested in the tickers that occur, we can still boil all the data into a single dataframe . df = pd.DataFrame(a.to_dict() for a in reddit_submissions) . df . title body permalink author score timestamp . 0 KSTR ETF &quot;The nasdaq of china&quot; | None | /r/stocks/comments/l664ce/kstr_etf_the_nasdaq_... | GioDesa | 1 | 2021-01-27 09:56:46 | . 1 Opinions/Projections on AMC? | None | /r/stocks/comments/l665a0/opinionsprojections_... | Double_jn_it | 1 | 2021-01-27 09:58:03 | . 2 GE, SPCE, &amp;amp; PLUG | None | /r/stocks/comments/l6668r/ge_spce_plug/ | _MeatLoafLover | 1 | 2021-01-27 09:59:21 | . 3 Reddit is under DDOS attack. Certain gaming re... | None | /r/stocks/comments/l66692/reddit_is_under_ddos... | theBacillus | 1 | 2021-01-27 09:59:22 | . 4 #GainStock | None | /r/stocks/comments/l66777/gainstock/ | lxPHENOMENONxl | 1 | 2021-01-27 10:00:19 | . ... ... | ... | ... | ... | ... | ... | . 2338 AN OPEN LETTER TO GAMESTOP CEO | None | /r/stocks/comments/l98k85/an_open_letter_to_ga... | Artuhan | 1 | 2021-01-31 03:55:51 | . 2339 AN OPEN LETTER TO GAMESTOP CEO | None | /r/stocks/comments/l98lai/an_open_letter_to_ga... | Artuhan | 1 | 2021-01-31 03:58:05 | . 2340 Thoughts on YOLO (AdvisorShares Pure Cannabis ... | None | /r/stocks/comments/l98nly/thoughts_on_yolo_adv... | ConfidentProgrammer1 | 1 | 2021-01-31 04:02:29 | . 2341 Daily advice | None | /r/stocks/comments/l98pic/daily_advice/ | Bukprotingas | 1 | 2021-01-31 04:06:24 | . 2342 AMC- Next stop? | None | /r/stocks/comments/l98pif/amc_next_stop/ | Hj-Fish | 1 | 2021-01-31 04:06:24 | . 2343 rows √ó 6 columns . Next up . While we just built our own Reddit API from some fundamental python libraries, there are more sophisticated API out there that do a better job of querying Reddit, like praw, and then we could try some other things like sentiment analysis .",
            "url": "https://ahy3nz.github.io/fastpayges/data%20science/2021/02/01/reddit1.html",
            "relUrl": "/data%20science/2021/02/01/reddit1.html",
            "date": " ‚Ä¢ Feb 1, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Accessing Folding@Home data on AWS",
            "content": "Some F@H data is freely accessible on AWS. This will be a relatively short post on accessing and navigating the data on AWS. . If you regularly use AWS, this will be nothing new. If you&#39;re a grad student who has only ever navigated local file directories or used scp/rsync/ssh to interact with remote clusters, this might be your first time interacting with files on AWS S3. . The python environment is fairly straightforward analytical environment, but with s3fs, boto3, and botocore to interact with files on S3 . conda create -n fahaws python=3.7 pandas s3fs jupyter ipykernel -c conda-forge -yq . (Active environment) . python -m pip install boto3 botocore . The AWS CLI . The tools to navigate files within AWS directories follow that of unix-like systems. AWS CLI installation. . aws s3 ls s3://fah-public-data-covid19-absolute-free-energy/ --no-sign-request to list files within this particular S3 bucket. The no sign request flag at the end helps us bypass the need for any credentials. . You can read from stdout or pipe the output to a textfile, but this will be your bread and butter for wading through terabytes and terabytes of F@H data. . As of this post (Dec 2020), looks like the files in free_energy_data/ have been last updated end of Sept 2020 . Summary of free energy results data . Fortunately, loading remote files via pandas is a common task, so there are convenient functions. Loading a dataframe over S3 is just like loading a dataframe locally (note the S3 string syntax) . The column febkT looks like the binding free energies in units of $k_B T$ (multiply by Boltzmann&#39;s constant and temperature to get energies in kJ or kcal). It&#39;s worth mentioning that the value of the binding free energy is not as helpful as the relative binding free energy to find the best binder of the bunch (how do these free energies compare against each other?) . import pandas as pd . df = pd.read_pickle(&quot;s3://fah-public-data-covid19-absolute-free-energy/free_energy_data/results.pkl&quot;) . df.head() . dataset fah identity receptor score febkT error ns_RL ns_L wl_RL L_error RL_error . 1155 MS0323_v3 | PROJ14822/RUN258 | DAR-DIA-43a-5 | protein-0387.pdb | -5.201610 | -25.546943 | 3.773523 | [131, 89, 74, 113, 80] | [450, 490, 540, 410, 620] | [0.18446, 0.14757, 0.18446, 0.18446, 0.18446] | 0.116912 | 3.280887 | . 609 MS0326_v3 | PROJ14823/RUN1202 | MUS-SCH-c2f-13 | Mpro-x0107-protein.pdb | -9.550890 | -25.259420 | 22.776358 | [121, 138, 96, 16, 5] | [200, 200, 200, 200, 200] | [0.18446, 0.18446, 0.23058, 0.23058, 0.23058] | 16.216396 | 0.109175 | . 759 MS0331_v3 | PROJ14825/RUN685 | MAK-UNK-129-18 | Mpro-x0107_0.pdb | -8.425830 | -24.789359 | 18.021078 | [58, 68, 5, 7] | [200] | [0.37782, 0.30226, 0.9224, 0.59034] | 0.000000 | 9.238496 | . 615 MS0326_v3 | PROJ14823/RUN2911 | ‚àö√ÖLV-UNI-7ff-30 | Mpro-x0540-protein.pdb | -2.774634 | -24.447756 | 6.605737 | [174, 124, 70] | [200, 200, 200, 200, 200] | [0.14757, 0.14757, 0.18446] | 0.042010 | 5.184169 | . 1086 MS0326_v3 | PROJ14823/RUN2580 | SEL-UNI-842-3 | Mpro-x0397-protein.pdb | -4.474095 | -23.705301 | 1.248983 | [166, 134, 45] | [200, 200, 200, 200, 200] | [0.18015, 0.22519, 0.35183] | 0.212546 | 2.529874 | . Some code to iterate through these buckets . Pythonically, we can build some S3 code to list each object in this S3 bucket. . import boto3 from botocore import UNSIGNED from botocore.client import Config s3 = boto3.resource(&#39;s3&#39;, config=Config(signature_version=UNSIGNED)) s3_client = boto3.client(&#39;s3&#39;, config=Config(signature_version=UNSIGNED)) bucket_name = &quot;fah-public-data-covid19-absolute-free-energy&quot; bucket = s3.Bucket(bucket_name) . This S3 bucket is very large -- all the simulation inputs, trajectories, and outputs are in here, so it will take a while to enumerate every object. Instead, we&#39;ll just make a generator and pull out a single item for proof-of-concept. . paginator = s3_client.get_paginator(&#39;list_objects_v2&#39;) pages = paginator.paginate(Bucket=bucket_name) . def page_iterator(pages): for page in pages: for item in page[&#39;Contents&#39;]: yield item[&#39;Key&#39;] . all_objects = page_iterator(pages) . next(all_objects) . &#39;PROJ14377/RUN0/CLONE0/frame0.tpr&#39; . And if you wanted to, you could layer a filter over the generator to impose some logic like filtering for the top-level directories . first_level_dirs = filter(lambda x: x.count(&#39;/&#39;)==1, all_objects) . Unix-like python filesytem libraries . S3FS, built on botocore and fsspec, has a very unix-like syntax to navigate and open files . import s3fs fs = s3fs.S3FileSystem(anon=True) . fs.ls(bucket_name) . [&#39;fah-public-data-covid19-absolute-free-energy/PROJ14377&#39;, &#39;fah-public-data-covid19-absolute-free-energy/PROJ14378&#39;, &#39;fah-public-data-covid19-absolute-free-energy/PROJ14379&#39;, &#39;fah-public-data-covid19-absolute-free-energy/PROJ14380&#39;, &#39;fah-public-data-covid19-absolute-free-energy/PROJ14383&#39;, &#39;fah-public-data-covid19-absolute-free-energy/PROJ14384&#39;, &#39;fah-public-data-covid19-absolute-free-energy/PROJ14630&#39;, &#39;fah-public-data-covid19-absolute-free-energy/PROJ14631&#39;, &#39;fah-public-data-covid19-absolute-free-energy/PROJ14650&#39;, &#39;fah-public-data-covid19-absolute-free-energy/PROJ14651&#39;, &#39;fah-public-data-covid19-absolute-free-energy/PROJ14652&#39;, &#39;fah-public-data-covid19-absolute-free-energy/PROJ14653&#39;, &#39;fah-public-data-covid19-absolute-free-energy/PROJ14654&#39;, &#39;fah-public-data-covid19-absolute-free-energy/PROJ14655&#39;, &#39;fah-public-data-covid19-absolute-free-energy/PROJ14656&#39;, &#39;fah-public-data-covid19-absolute-free-energy/PROJ14665&#39;, &#39;fah-public-data-covid19-absolute-free-energy/PROJ14666&#39;, &#39;fah-public-data-covid19-absolute-free-energy/PROJ14667&#39;, &#39;fah-public-data-covid19-absolute-free-energy/PROJ14668&#39;, &#39;fah-public-data-covid19-absolute-free-energy/PROJ14669&#39;, &#39;fah-public-data-covid19-absolute-free-energy/PROJ14670&#39;, &#39;fah-public-data-covid19-absolute-free-energy/PROJ14671&#39;, &#39;fah-public-data-covid19-absolute-free-energy/PROJ14702&#39;, &#39;fah-public-data-covid19-absolute-free-energy/PROJ14703&#39;, &#39;fah-public-data-covid19-absolute-free-energy/PROJ14704&#39;, &#39;fah-public-data-covid19-absolute-free-energy/PROJ14705&#39;, &#39;fah-public-data-covid19-absolute-free-energy/PROJ14723&#39;, &#39;fah-public-data-covid19-absolute-free-energy/PROJ14724&#39;, &#39;fah-public-data-covid19-absolute-free-energy/PROJ14726&#39;, &#39;fah-public-data-covid19-absolute-free-energy/PROJ14802&#39;, &#39;fah-public-data-covid19-absolute-free-energy/PROJ14803&#39;, &#39;fah-public-data-covid19-absolute-free-energy/PROJ14804&#39;, &#39;fah-public-data-covid19-absolute-free-energy/PROJ14805&#39;, &#39;fah-public-data-covid19-absolute-free-energy/PROJ14806&#39;, &#39;fah-public-data-covid19-absolute-free-energy/PROJ14807&#39;, &#39;fah-public-data-covid19-absolute-free-energy/PROJ14808&#39;, &#39;fah-public-data-covid19-absolute-free-energy/PROJ14809&#39;, &#39;fah-public-data-covid19-absolute-free-energy/PROJ14810&#39;, &#39;fah-public-data-covid19-absolute-free-energy/PROJ14811&#39;, &#39;fah-public-data-covid19-absolute-free-energy/PROJ14812&#39;, &#39;fah-public-data-covid19-absolute-free-energy/PROJ14813&#39;, &#39;fah-public-data-covid19-absolute-free-energy/PROJ14823&#39;, &#39;fah-public-data-covid19-absolute-free-energy/PROJ14824&#39;, &#39;fah-public-data-covid19-absolute-free-energy/PROJ14826&#39;, &#39;fah-public-data-covid19-absolute-free-energy/PROJ14833&#39;, &#39;fah-public-data-covid19-absolute-free-energy/SVR51748107&#39;, &#39;fah-public-data-covid19-absolute-free-energy/free_energy_data&#39;, &#39;fah-public-data-covid19-absolute-free-energy/receptor_structures.tar.gz&#39;, &#39;fah-public-data-covid19-absolute-free-energy/setup_files&#39;] . fs.ls(bucket_name + &quot;/free_energy_data&quot;) . [&#39;fah-public-data-covid19-absolute-free-energy/free_energy_data/&#39;, &#39;fah-public-data-covid19-absolute-free-energy/free_energy_data/BRO_L_14382.pkl&#39;, &#39;fah-public-data-covid19-absolute-free-energy/free_energy_data/BRO_RL_14717.pkl&#39;, &#39;fah-public-data-covid19-absolute-free-energy/free_energy_data/BRO_RL_14718.pkl&#39;, &#39;fah-public-data-covid19-absolute-free-energy/free_energy_data/BRO_RL_14719.pkl&#39;, &#39;fah-public-data-covid19-absolute-free-energy/free_energy_data/BRO_RL_14720.pkl&#39;, &#39;fah-public-data-covid19-absolute-free-energy/free_energy_data/BRO_RL_14817.pkl&#39;, &#39;fah-public-data-covid19-absolute-free-energy/free_energy_data/BRO_RL_14818.pkl&#39;, &#39;fah-public-data-covid19-absolute-free-energy/free_energy_data/BRO_RL_14819.pkl&#39;, &#39;fah-public-data-covid19-absolute-free-energy/free_energy_data/BRO_RL_14820.pkl&#39;, &#39;fah-public-data-covid19-absolute-free-energy/free_energy_data/HITS_L_14676.pkl&#39;, &#39;fah-public-data-covid19-absolute-free-energy/free_energy_data/HITS_RL_14730.pkl&#39;, &#39;fah-public-data-covid19-absolute-free-energy/free_energy_data/HITS_RL_14830.pkl&#39;, &#39;fah-public-data-covid19-absolute-free-energy/free_energy_data/MLTN_L_14374.pkl&#39;, &#39;fah-public-data-covid19-absolute-free-energy/free_energy_data/MLTN_RL_14721.pkl&#39;, &#39;fah-public-data-covid19-absolute-free-energy/free_energy_data/MLTN_RL_14821.pkl&#39;, &#39;fah-public-data-covid19-absolute-free-energy/free_energy_data/MS0323_L_14364.pkl&#39;, &#39;fah-public-data-covid19-absolute-free-energy/free_energy_data/MS0323_RL_14722.pkl&#39;, &#39;fah-public-data-covid19-absolute-free-energy/free_energy_data/MS0323_RL_14822.pkl&#39;, &#39;fah-public-data-covid19-absolute-free-energy/free_energy_data/MS0326_L_14369_14372_14370_14371.pkl&#39;, &#39;fah-public-data-covid19-absolute-free-energy/free_energy_data/MS0326_RL_14723.pkl&#39;, &#39;fah-public-data-covid19-absolute-free-energy/free_energy_data/MS0326_RL_14724.pkl&#39;, &#39;fah-public-data-covid19-absolute-free-energy/free_energy_data/MS0326_RL_14823.pkl&#39;, &#39;fah-public-data-covid19-absolute-free-energy/free_energy_data/MS0326_RL_14824.pkl&#39;, &#39;fah-public-data-covid19-absolute-free-energy/free_energy_data/MS0331_L_14376.pkl&#39;, &#39;fah-public-data-covid19-absolute-free-energy/free_energy_data/MS0331_RL_14725.pkl&#39;, &#39;fah-public-data-covid19-absolute-free-energy/free_energy_data/MS0331_RL_14825.pkl&#39;, &#39;fah-public-data-covid19-absolute-free-energy/free_energy_data/MS0406-2_L_14380.pkl&#39;, &#39;fah-public-data-covid19-absolute-free-energy/free_energy_data/MS0406-2_RL_14727.pkl&#39;, &#39;fah-public-data-covid19-absolute-free-energy/free_energy_data/MS0406-2_RL_14728.pkl&#39;, &#39;fah-public-data-covid19-absolute-free-energy/free_energy_data/MS0406-2_RL_14827.pkl&#39;, &#39;fah-public-data-covid19-absolute-free-energy/free_energy_data/MS0406-2_RL_14828.pkl&#39;, &#39;fah-public-data-covid19-absolute-free-energy/free_energy_data/MS0406_L_14378.pkl&#39;, &#39;fah-public-data-covid19-absolute-free-energy/free_energy_data/MS0406_RL_14752.pkl&#39;, &#39;fah-public-data-covid19-absolute-free-energy/free_energy_data/MS0406_RL_14852.pkl&#39;, &#39;fah-public-data-covid19-absolute-free-energy/free_energy_data/hello.txt&#39;, &#39;fah-public-data-covid19-absolute-free-energy/free_energy_data/organization.pkl&#39;, &#39;fah-public-data-covid19-absolute-free-energy/free_energy_data/results.pkl&#39;] . with fs.open(&#39;fah-public-data-covid19-absolute-free-energy/free_energy_data/hello.txt&#39;, &#39;r&#39;) as f: print(f.read()) . hello aws! . with fs.open(&quot;fah-public-data-covid19-absolute-free-energy/free_energy_data/organization.pkl&quot;, &#39;rb&#39;) as f: organization_df = pd.read_pickle(f) . organization_df.head() . dataset identity receptor score v1_project v1_run v2_project v2_run v3_project v3_run project run . 0 72_RL | CCNCC(COC)Oc1ccccc1 | receptor-270-343.pdb | 0.999790 | 14600 | 0 | 14700 | 0 | 14800 | 0 | NaN | NaN | . 1 72_RL | O=C(Cc1cccnc1)c1ccccc1 | receptor-343.pdb | 0.999652 | 14600 | 1 | 14700 | 1 | 14800 | 1 | NaN | NaN | . 2 72_RL | CCCCC(N)c1cc(C)ccn1 | receptor-343.pdb | 0.999256 | 14600 | 2 | 14700 | 2 | 14800 | 2 | NaN | NaN | . 3 72_RL | COCC(C)Nc1ccncn1 | receptor-343.pdb | 0.999096 | 14600 | 3 | 14700 | 3 | 14800 | 3 | NaN | NaN | . 4 72_RL | CCN(CC)CCNc1ccc(C#N)cn1 | receptor-270-343.pdb | 0.998980 | 14600 | 4 | 14700 | 4 | 14800 | 4 | NaN | NaN | .",
            "url": "https://ahy3nz.github.io/fastpayges/data%20science/molecular%20modeling/2020/12/29/fahonaws.html",
            "relUrl": "/data%20science/molecular%20modeling/2020/12/29/fahonaws.html",
            "date": " ‚Ä¢ Dec 29, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Poetry and Docker",
            "content": "What is poetry and where does this fit in the python software/DS ecosystem? And some beginner forays into docker. . To skip the reading and jump to the code, go to this repo . Personal opinions motivating this work . The data science world is large. Data science is kind of like an intersection of statistics/math, subject matter, software engineering, algorithms, and all the collaboration/teamwork that comes with a job. Starting out, you definitely cannot be expected to have a mastery over everything, but at least some minimal competencies and capacity to learn (this basically applies to all jobs). . I‚Äôm about 11 months into technically being labeled a ‚Äúdata scientist‚Äù and I‚Äôve observed that each data scientist ends up cultivating their own sets of skills they find valuable and/or interesting ‚Äì generalist/specialist is basically skilling up however you want. Seeing the work of other data scientists has built up a long laundry list of things I‚Äôd like to learn but don‚Äôt have the business-hours to devote because of more-pressing project demands. Among these is this concept of packaging and building ‚Äúapplications‚Äù. For a graduate student, your ‚Äúapplication‚Äù might be a codebase and set of functions that others can reliably and consistently use in their own hacky codes. For a software engineer, your ‚Äúapplication‚Äù might be deployed onto some cloud server, where the code needs to be self-sufficient and robust, listening for input, processing this input, and pumping out some output without hands on a keyboard. For a data scientist, you may eventually need to think about how an application gets deployed, from consistency of functions and numerical accuracy to considering the entire technical stack involved. Day-to-day, I think consistency of functions and numerical accuracy are generally kept front-of-mind with unit tests or and mainly because you‚Äôre always thinking of the mathematical model. . If you‚Äôre a little more software-savvy, you‚Äôll think about your python environment, using conda or something to control your python software dependencies, your software build, and any compilation that has to happen. Since I‚Äôm on socially-distanced, self-quarantined holiday, this is a great time to do some learning . Poetry . ‚ÄúDependency‚Äù hell, an introduction . Most software depends on other software, and if the dependencies change some core functionality, then your own software may no longer function as intended. To resolve this, you venture through ‚Äúdependency hell‚Äù to figure out whose code broke your code, and how to fix this. . Data scientists like to use python virtual environments to ensure dependencies are compatible and runnable. Some like to use pip and venv, which is fine for installing packages, but only recently will pip attempt to address dependency resolution. Conda is also very popular for managing software packages, compiling software, and resolving software dependencies. . What does poetry do? . A new contestant, poetry finds itself in some python packaging and dependency conversations like ‚Äúoh I‚Äôve heard of poetry but never really tried it‚Äù. Poetry helps manage the python package dependencies for a given software, with a simple CLI to add and update new package dependencies. Poetry generally involves the binary (available on conda and pip), but interacts with your package via two files, the poetry.lock and pyproject.toml. If someone gives you those files, you should be able to build your own compatible python environment. In tandem, the two specify the necessary dependncies for your project, with the former pinning dependencies and the latter floating dependencies. Poetry also has some convenient functions for compiling source distributions and wheels so you can distribute this code on somewhere like pypi (but it doesn‚Äôt look like there‚Äôs any mention of conda recipes). . What about docker? . Docker provides a lot of virtualization and environment control so you can put together an entire tech stack just for your application to run on a bare-bones, nothing-installed server somewhere. This comes in the form of a dockerfile, which like a set of instructions on how to build your container. For an early career data scientist, that‚Äôs probably all you need to know. Software engineers deal with this all the time, and data scientists eventually dip their toes here as a model/project comes to maturity. . You can learn a lot about dockerfiles by reading them and writing your own, so take a look at the repo linked at the beginning of this post. In general, it kind of resembles a lot of shell commands. Getting conda to work with docker comes with some sticking points: . conda commands within each layer won‚Äôt work unless you run the shell script that comes with conda, so you have to remember to run that script throughout the dockerfile | Note the use of the entrypoint.sh file, which becomes the final script that is executed when you call docker run. Observe the necessary chmod to make it executable, and note the conda.sh command even inside the entrypoint.sh file if you want the container to run some code within a conda environment. | docker run -it poetry /bin/bash if you want to open an interactive shell session to the container, running commands/codes inside the docker container like you would an SSH session. | Technically, since you have absolute control over the image, you might not need the virtual environment for small python packages. As the packages get more complex and package builds become more complicated, it becomes easier to let conda handle the package management rather than try to correctly install everything in a dockerfile | . If you envision running lots of python code or calculations on cloud servers, docker containers and python environments are the sorts of tech that make it happen (and if you and your proejct are up for it, container-orchestration and workflow tools) . Bare bones example . I‚Äôve documented my experiences in this sandbox for using docker and poetry. There are a lot of tutorials on the internet, so I won‚Äôt bother here. But, for a data scientist versed in python environments, this repo showcases how to build your docker images for conda/poetry/python. For a ‚Äúreal‚Äù industrial application, things will likely get messier as the environments and software stack get more complex, but this is a decent start for an amateur. .",
            "url": "https://ahy3nz.github.io/fastpayges/data%20science/2020/12/23/poetrypackaging.html",
            "relUrl": "/data%20science/2020/12/23/poetrypackaging.html",
            "date": " ‚Ä¢ Dec 23, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Exploring PyTorch + ANI + MD",
            "content": "PyTorch provides nice utilities for differentiation. ANI provides some interatomic potentials trained on some neural networks. Molecular Dynamics might be an interesting combination . Some basic pytorch functionality, a 1-D spring . Pytorch replicates a lot of numpy functionality, and we can build python functions that take pytorch tensors as input . import torch import matplotlib.pyplot as plt x = torch.ones((2,2), requires_grad=True) . A simple quadratic function . def sq_function(x): return x**2 . Since we have an array of 1s, the square won&#39;t look very interesting... . foo = sq_function(x) . foo . tensor([[1., 1.], [1., 1.]], grad_fn=&lt;PowBackward0&gt;) . More interstingly, we can compute the gradient of this function. . To compute the gradient, the value/function needs to be a scalar, but this scalar could be computed from a bunch of other functions stemming from some independent variables (our tensor x). In this case, our final scalar looks like this, $ Y = x_0^2 + x_1^2 + x_2^2 + x_3^2 $. Taking the gradient means taking 4 partial derivatives for each input. Fortunately, the equation is simple to compute each partial derivative, $ frac{ partial Y}{ partial x_i} = 2*x_i $, where $i = [0,4)$. Since this is an array of 1s, each partial derivative evaluates to 2 . torch.autograd.grad(foo.sum(), x) . (tensor([[2., 2.], [2., 2.]]),) . We&#39;ve evaluated the function and its gradient at just one point, but we can use some numpy-esque functions to evaluate the square-function and its gradient at a range of points. . Yup, looks right to me . some_xvals = torch.arange(-12., 12., step=0.5, requires_grad=True) some_yvals = sq_function(some_xvals) fig, ax = plt.subplots(1,1) ax.plot(some_xvals.detach().numpy(), some_yvals.detach().numpy()) ax.plot(some_xvals.detach().numpy(), torch.autograd.grad(some_yvals.sum(), some_xvals)[0]) . [&lt;matplotlib.lines.Line2D at 0x7f9c907aa910&gt;] . Slightly more book-keeping, 3x 1-D harmonic springs . Define an energy function as the sum of 3 harmonic springs . $ V(x, y, z) = V_x + V_y + V_z = (x-x_0)^2 + (y-y_0)^2 + (z-z_0)^2 $ . The gradient, the 3 partial derivatives, are computed as such (being verbose with the chain rule) . $ frac{ partial V}{ partial X} = 2 *(x-x_0) * 1 $ . $ frac{ partial V}{ partial Y} = 2 *(y-y_0) * 1$ . $ frac{ partial V}{ partial Z} = 2 *(z-z_0) * 1$ . def harmonic_spring_3d(coord, origin=torch.tensor([0,0,0])): V_x = (coord[0]-origin[0])**2 V_y = (coord[1]-origin[1])**2 V_z = (coord[2]-origin[2])**2 return V_x + V_y + V_z . We can evaluate the potential energy at 1 point, which involves computing the energy in 3 dimensions. . Our &quot;anchor&quot; will be the origin, and our endpoint will be (1,2,3) . $ 1^2 + 2^2 + 3^2 = 14 $ . my_coords = torch.tensor([1.,2.,3.], requires_grad=True) total_energy = harmonic_spring_3d(my_coords) total_energy . tensor(14., grad_fn=&lt;AddBackward0&gt;) . Computing the gradient, partial derivatives in each direction, which is simply 2 times the distance in each dimension . $ nabla hat V = &lt; 2*1, 2*2, 2*3 &gt; = &lt;2,4,6&gt; $ . torch.autograd.grad(total_energy, my_coords) . (tensor([2., 4., 6.]),) . More involved: Lennard Jones . The Lennard-Jones potential describes the potential energy between two particles. Not the most accurate potential, but has been decent for a long time now. Some background information on the Lenanrd-Jones potential. For simplicity, assume $ epsilon =1$ and $ sigma=1$ in unitless quantities: . $ V_{LJ} = 4 * ( frac{1}{r}^{12} - frac{1}{r}^6) $ . $ - frac{ partial V}{ partial r} = -4 * (-12 * r^{-13} + 6 * r^{-7}) $ . def lj(val): return 4 * ((1/val)**12 - (1/val)**6) . r_values = torch.arange(0.1, 12., step=0.001, requires_grad=True) energy = lj(r_values) forces = -torch.autograd.grad(energy.sum(), r_values)[0] . For sanity check, we can confirm that energy reaches a critical point (local minimum) when the force is 0. . Also, this definitely looks like a LJ potential to me . import matplotlib.pyplot as plt fig, ax = plt.subplots(1,1, dpi=100) ax.plot(r_values.detach().numpy(), energy.detach().numpy(), label=&#39;energy&#39;) ax.plot(r_values.detach().numpy(), forces.detach().numpy(), label=&#39;force&#39;) ax.set_ylim([-2,1]) ax.legend() ax.set_xlim([0,2]) ax.axhline(y=0, color=&#39;r&#39;, linestyle=&#39;--&#39;) . &lt;matplotlib.lines.Line2D at 0x7f9d2859b2d0&gt; . Moving to torchani . ANI is an interatomic potential built upon neural networks. Rather than write our own function to evaluate the energy between atoms, maybe we can just use ANI. Since this is pytorch-based, this is still available for autodifferentiation to get the forces . https://github.com/aiqm/torchani . To begin, we have to define our elements (a tensor of atomic numbers). For the molecular mechanics people, each atom is identifiable by its element, and not one of many atom-types. . We have to define the positions (units of Angstrom), which is also a multi-dimensional tensor. . Load the model, specifying to convert the atomic numbers to indices suitable for ANI. . We can compute the energies and forces from the model. The energy comes from the model, but the force is obtained via an autograd call, observing that we are differentiating the sum of the forces, evaluating at the positions . import torchani elements = torch.tensor([[6, 6]]) positions = torch.tensor([[[3.0, 3.0, 3.0], [3.5, 3.5, 3.5]]], requires_grad=True) model = torchani.models.ANI2x(periodic_table_index=True) energy = model((elements, positions)).energies forces = -1.0 * torch.autograd.grad(energy.sum(), positions)[0] . /home/ayang41/miniconda3/envs/torch37/lib/python3.7/site-packages/torchani/aev.py:195: UserWarning: This overload of nonzero is deprecated: nonzero() Consider using one of the following signatures instead: nonzero(*, bool as_tuple) (Triggered internally at /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.) in_cutoff = (distances &lt;= cutoff).nonzero() . energy . tensor([-75.7952], dtype=torch.float64, grad_fn=&lt;AddBackward0&gt;) . forces . tensor([[[-0.4016, -0.4016, -0.4016], [ 0.4016, 0.4016, 0.4016]]]) . Going a step further, we can try to visualize the interaction potential by evaluating the energy at a variety of distances. We can also do some autodifferentiation to compute the forces. . In this example, we have 2 atoms that share X and Y coordinates, but pull them apart in the Z direction . all_z = torch.arange(3.0, 12.0, step=0.1) all_energy = [] all_forces = [] for z in all_z: # Generate a new set of positions positions = torch.tensor([[[3.0, 3.0, 3.0], [3.0, 3.0, z]]], requires_grad=True ) # Compute energy energy = model((elements, positions)).energies # Compute force forces = -1.0 * torch.autograd.grad(energy.sum(), positions)[0] # Get the force vector on the first atom one_atom_forces = forces[0,0] # Compute the magnitude of this force vector force_magnitude = torch.sqrt(torch.dot(one_atom_forces, one_atom_forces)) # Calculate the unit vector for this force vector, # although it&#39;s a little unnecessary because the only distance is in the # z direction unit_vector_force = one_atom_forces/force_magnitude # Get z-component of force vector force_vector_z = unit_vector_force[2]*force_magnitude # Some nans will form if the force magnitude is zero, but this # is really just a 0 force vector if torch.isnan(force_vector_z).any(): force_vector_z = 0.0 else: force_vector_z = float(force_vector_z.detach().numpy()) # Accumulate all_energy.append(float(energy.detach().numpy())) all_forces.append(force_vector_z) . Hmmm... this does not resemble the Lennard-Jones potential (or basic chemistry for that matter) . fig, ax = plt.subplots(1,1, dpi=100) ax.plot(all_z-3, all_energy) ax.set_xlabel(r&quot;Distance ($ AA$)&quot;) ax.set_ylabel(&quot;Energy (Hartree)&quot;) . Text(0, 0.5, &#39;Energy (Hartree)&#39;) . fig, ax = plt.subplots(1,1, dpi=100) ax.plot(all_z-3, all_forces) ax.set_xlabel(r&quot;Distance ($ AA$)&quot;) ax.set_ylabel(&quot;Force (Hartree / $ AA$)&quot;) . Text(0, 0.5, &#39;Force (Hartree / $ AA$)&#39;) . from mbuild.lib.recipes import Alkane # The mBuild alkane recipe is mainly used to generate # some particles and positions cmpd = Alkane(n=5) # Convert to mdtraj trajectory out of convenience for atomic numbers traj = cmpd.to_trajectory() # Periodic cell, from nm to angstrom cell = torch.tensor(traj.unitcell_vectors[0]*10) # We just need atomic numbers species = torch.tensor([[ a.element.atomic_number for a in traj.top.atoms ]]) # Make tensor for coordinates # Since we are differentiating WRT coordinates, we need the # requires_grad=True coordinates = torch.tensor(traj.xyz*10, requires_grad=True) # PBC flag necessary for computing energies with periodic boundaries pbc = torch.tensor([True, True, True], dtype=torch.bool) energies = model((species, coordinates), cell=cell, pbc=pbc).energies forces = -1.0 * ( torch.autograd.grad(energies.sum(), coordinates)[0] ) . /home/ayang41/miniconda3/envs/torch37/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above. and should_run_async(code) #################################################################### The code at compound.py:85 requires the &#34;openbabel&#34; package openbabel can be installed with conda using: # conda install -c conda-forge openbabel or from source following instructions at: # http://openbabel.org/docs/current/UseTheLibrary/PythonInstall.html #################################################################### #################################################################### The code at compound.py:85 requires the &#34;openbabel&#34; package openbabel can be installed with conda using: # conda install -c conda-forge openbabel or from source following instructions at: # http://openbabel.org/docs/current/UseTheLibrary/PythonInstall.html #################################################################### #################################################################### The code at compound.py:85 requires the &#34;openbabel&#34; package openbabel can be installed with conda using: # conda install -c conda-forge openbabel or from source following instructions at: # http://openbabel.org/docs/current/UseTheLibrary/PythonInstall.html #################################################################### . energies . /home/ayang41/miniconda3/envs/torch37/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above. and should_run_async(code) . tensor([-197.1103], dtype=torch.float64, grad_fn=&lt;AddBackward0&gt;) . forces . /home/ayang41/miniconda3/envs/torch37/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above. and should_run_async(code) . tensor([[[ 6.5805e-02, 5.5707e-02, 4.9085e-02], [ 1.3603e-03, -2.1826e-02, -1.0588e-02], [ 1.5610e-02, -5.9448e-02, 1.4180e-03], [-7.4506e-09, 1.1921e-07, 1.1461e-02], [-7.2804e-03, 2.5767e-02, -6.2775e-04], [ 7.2804e-03, -2.5766e-02, -6.2775e-04], [-6.5805e-02, -5.5707e-02, 4.9085e-02], [-1.5610e-02, 5.9448e-02, 1.4180e-03], [-1.3604e-03, 2.1826e-02, -1.0588e-02], [ 6.9919e-02, 1.0938e-01, -4.7381e-02], [ 4.2583e-02, 1.5188e-01, -9.1655e-03], [-3.5887e-02, -5.4712e-03, 4.6396e-02], [ 3.4462e-03, 3.7552e-02, -3.4868e-02], [-6.9919e-02, -1.0938e-01, -4.7381e-02], [-4.2583e-02, -1.5188e-01, -9.1655e-03], [ 3.5887e-02, 5.4712e-03, 4.6396e-02], [-3.4462e-03, -3.7552e-02, -3.4868e-02]]]) . To be continued ... . One might imagine trying to incorporate ANI potentials into MD simulations (which has been done in ASE). However, the torchani-API is general enough that you could use any number of computational chemistry packages to feed into torchani. The output is also general enough you could imagine trying to apply your own integrators and make your own simulation. But... from the weird 2-atom interatomic potentials, some of these methods might require some debugging. . Files and environment can be found here . Reference . Xiang Gao, Farhad Ramezanghorbani, Olexandr Isayev, Justin S. Smith, and Adrian E. Roitberg. TorchANI: A Free and Open Source PyTorch Based Deep Learning Implementation of the ANI Neural Network Potentials. Journal of Chemical Information and Modeling 2020 60 (7), 3408-3415 .",
            "url": "https://ahy3nz.github.io/fastpayges/data%20science/molecular%20modeling/2020/08/15/torchanimd.html",
            "relUrl": "/data%20science/molecular%20modeling/2020/08/15/torchanimd.html",
            "date": " ‚Ä¢ Aug 15, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Downloading and studying my message behavior",
            "content": "The message data from Facebook is organized like this: . inbox/ chat1/ message1.json | message2.json | audio/ | files/ | gifs/ | photos/ | videos/ | . | chat2/ message1.json | . | . | . We can start with some basic tree-walking to identify which is the largest chat group . import os from pathlib import Path import json import multiprocessing from multiprocessing import Pool import dask from dask import delayed import pandas as pd import matplotlib %matplotlib inline import matplotlib.pyplot as plt import numpy as np def size_of_tree(p): if &#39;json&#39; in p.suffix: with open(p.as_posix()) as f: message_data = json.load(f) return len(message_data[&#39;messages&#39;]) elif p.is_dir(): return sum([size_of_tree(a) for a in p.iterdir()]) else: return 0 def parent_function(p): return {p: size_of_tree(p)} def parent_function_chunk(p): return {folder: size_of_tree(folder) for folder in p} p = Path(&#39;/mnt/c/Users/ayang/Downloads/facebook-ayang41/messages/inbox&#39;) all_dirs = [a for a in p.iterdir() if a.is_dir()] . Since this is an embarrassingly parallel situation, we can easily show the serial version is slower than the parallel version (using dask or multiprocessing), with or without some chunking . %%time sizes = [parent_function(folder) for folder in all_dirs] . CPU times: user 5.3 s, sys: 17.6 s, total: 22.9 s Wall time: 1min 30s . %%time all_delayed = [delayed(parent_function)(folder) for folder in all_dirs] results = dask.compute(all_delayed) . CPU times: user 9.65 s, sys: 1min 4s, total: 1min 14s Wall time: 30.4 s . %%time with Pool() as p: pool_results = p.map(parent_function, all_dirs) . CPU times: user 131 ms, sys: 171 ms, total: 302 ms Wall time: 27.5 s . %%time all_delayed = [delayed(parent_function_chunk)(all_dirs[i::6]) for i in range(6)] results = dask.compute(all_delayed) . CPU times: user 8.13 s, sys: 59.7 s, total: 1min 7s Wall time: 31.4 s . %%time with Pool() as p: pool_results = p.map(parent_function_chunk, [all_dirs[i::6] for i in range(6)]) . CPU times: user 242 ms, sys: 33.7 ms, total: 276 ms Wall time: 28.9 s . For those curious, I have a pretty skewed chat message distribution... . message_sizes = [size for chunk in results[0] for size in chunk.values()] . fig, ax = plt.subplots(1,1, figsize=(8,6)) ax.hist(message_sizes) ax.set_ylabel(&quot;Number of chats&quot;) ax.set_xlabel(&quot;Number of messages within chat&quot;) . Text(0.5, 0, &#39;Number of messages within chat&#39;) . fig, ax =plt.subplots(1,1, figsize=(8,6)) ax.hist(np.log(message_sizes)) ax.set_ylabel(&quot;Number of chats&quot;) ax.set_xlabel(&quot;Log number of messages within chat&quot;) . Text(0.5, 0, &#39;Log number of messages within chat&#39;) . We can make a small data pipeline for my message history by using two iterators, one after the other. The first iterator get_json_files_iter is simple, it will just burrow its way through each directory, grab all the json files, and spit out one at a time, returning a generator. The second iterator process_json_iter will take an item from the get_json_files_iter generator and actually process some information. In this case, getting information about the sender, timestamp, and length of message. . from typing import Iterator, Dict, Any, List import pathlib import json from datetime import datetime def get_json_files_iter(dirs) -&gt; Iterator[str]: &quot;&quot;&quot; For each dir, get the json files &quot;&quot;&quot; root = Path(&#39;.&#39;) for directory in dirs: subdir = root / Path(directory) for jsonfile in subdir.glob(&#39;*.json&#39;): yield Path(jsonfile) def process_json_iter(json_iter: Iterator[str]) -&gt; Iterator[List[Dict[Any, Any]]]: &quot;&quot;&quot; Given a json file, parse and summarize the message info&quot;&quot;&quot; for jsonfile in json_iter: with open(jsonfile.as_posix()) as f: message_data = json.load(f) for message in message_data[&#39;messages&#39;]: yield { &#39;sender&#39;: message[&#39;sender_name&#39;], &#39;timestamp&#39;: datetime.fromtimestamp(message[&#39;timestamp_ms&#39;]/1000), &#39;n_words&#39;: len(message[&#39;content&#39;]) if message.get(&#39;content&#39;, None) else None # Some messages have no text # like an image/emoji post } . process_json_iter(get_json_files_iter(all_dirs)) . &lt;generator object process_json_iter at 0x7f26228366d0&gt; . Getting through all the files (7 gb) isn&#39;t too bad . %%time extracted_messages = [*process_json_iter(get_json_files_iter(all_dirs))] . CPU times: user 6.01 s, sys: 0 ns, total: 6.01 s Wall time: 10.8 s . %%time df = pd.DataFrame(extracted_messages) . CPU times: user 909 ms, sys: 0 ns, total: 909 ms Wall time: 900 ms . Conveniently, we can pass the generator itself to create a dataframe. This doesn&#39;t provide much speedup, but it helps keep the code concise . %%time df = pd.DataFrame(process_json_iter(get_json_files_iter(all_dirs))) . CPU times: user 7.42 s, sys: 0 ns, total: 7.42 s Wall time: 13.7 s . df.columns . Index([&#39;sender&#39;, &#39;timestamp&#39;, &#39;n_words&#39;], dtype=&#39;object&#39;) . df.shape . (1003527, 3) . We can look at how my chat history has changed over the years... . df[&#39;date&#39;] = df.apply(lambda x: &#39;-&#39;.join([str(x[&#39;timestamp&#39;].year), str(x[&#39;timestamp&#39;].month), str(x[&#39;timestamp&#39;].day)]), axis=1) . grouped_by_date = df.groupby(&#39;date&#39;).agg(&#39;count&#39;) . fig, ax = plt.subplots(1,1, figsize=(18,10)) ax.plot(grouped_by_date.index.tolist(), grouped_by_date[&#39;sender&#39;]) ticks = np.linspace(0, len(grouped_by_date.index)-1, num=50, dtype=int) ax.set_xticks(ticks) ax.set_xticklabels([list(grouped_by_date.index)[i] for i in ticks], rotation=&#39;90&#39;, ha=&#39;right&#39;) ax.set_ylabel(&quot;Number of messages&quot;, size=18) . Text(0, 0.5, &#39;Number of messages&#39;) . Maybe trying to smooth things out. The timestamps aren&#39;t evenly distributed so the averages could be computed better, but they work well enough for now . rolling = grouped_by_date.rolling(10, min_periods=1).mean() fig, ax = plt.subplots(1,1, figsize=(18,10)) ax.plot(rolling.index.tolist(), rolling[&#39;sender&#39;]) ticks = np.linspace(0, len(rolling.index)-1, num=50, dtype=int) ax.set_xticks(ticks) ax.set_xticklabels([list(rolling.index)[i] for i in ticks], rotation=&#39;90&#39;, ha=&#39;right&#39;) ax.set_ylabel(&quot;Number of messages&quot;, size=18) . Text(0, 0.5, &#39;Number of messages&#39;) .",
            "url": "https://ahy3nz.github.io/fastpayges/data%20science/2020/08/07/fb_messages.html",
            "relUrl": "/data%20science/2020/08/07/fb_messages.html",
            "date": " ‚Ä¢ Aug 7, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Lessons learned from accelerating Foyer with Dask",
            "content": "Combining Foyer + Dask . More into the foray of combining modern molecular modeling tools with modern data science libraries... . Foyer uses graph algorithms to parametrize your molecular model . Given a system of molecules and atoms, how do we parametrize each atom according to our molecular model, our force field? The parameters for each atom depend on its bonded neighbors. Framing this as a graph problem (vertices are atoms and edges are bonds), subgraph isomorphisms are used to match our atom&#39;s bonding patterns to the template bonding patterns specified by our force field&#39;s atom-type bonding patterns . Dask helps distribute parallel workloads . Generally, most of these molecular modeling packages operate on a shared memory data structure - a list, a dictionary. To parallelize this atomtyping operation, we need to identify how we can parallelize this. For graph problems, sometimes each node (atom) needs to know every other node. We are left with a couple options . Broadcast the entire molecular graph to all workers, divy up which atoms each worker is reponsible for atomtyping. This risks some large overhead because the entire molecular graph can span tens of thousands (or more) nodes. | Broad only the relevant molecular graph to each worker, each worker becomes responsible for parametrizing that small subgraph. This one doesn&#39;t involve broadcasting large graphs, but now the problem becomes identifying what the relevant graph is. I refer readers to the concept of a graph component) | . What to expect in this notebook . First, I&#39;ll be breaking up the entire chemical system into smaller subgraphs. I&#39;ll try to atom-type each subgraph serially. Then, I&#39;ll try to distribute the workload of each subgraph using dask. I&#39;ll try to do some timings - against different numbers of homogeneous molecules and different numbers of heterogeneous molecules. Along the way, I&#39;ll be observing some friction points for using dask (casual user here) and for using foyer/parmed . Parallelization&#39;s value is hard to demonstrate in this use case . Dask did not show improvements compared to canonical foyer. With the data structures we, and foyer, usually deal with, there&#39;s some extra work in formatting them into easily-distributable data structures for parallelization. There&#39;s always communication issues for parallel workloads. Foyer has molecule caching that accelerates atom-typing for molecules you&#39;ve already atom-typed; this isn&#39;t leveraged well in a distributed scenario. Foyer uses networkx, which likely already comes with its own optimizations for simplifying the workload, so evaluating a singular large graph may not be as bad as we think compared to lots of small graphs. As written, the foyer code may be best utilized serially. Future foyer implementations and refactors might better exposed elements of parallelization . Distributing the workload: split a chemical system into smaller components, parametrize each molecule, in serial . Use mbuild to create our molecule, replicate to 10 molecules, foyer to apply the OPLS-AA force field . import mbuild as mb from mbuild.lib.recipes import Alkane import foyer import parmed as pmd import networkx as nx . ff = foyer.forcefields.load_OPLSAA() . /home/ayang41/programs/foyer/foyer/forcefield.py:449: UserWarning: No force field version number found in force field XML file. &#39;No force field version number found in force field XML file.&#39; /home/ayang41/programs/foyer/foyer/forcefield.py:461: UserWarning: No force field name found in force field XML file. &#39;No force field name found in force field XML file.&#39; /home/ayang41/programs/foyer/foyer/validator.py:132: ValidationWarning: You have empty smart definition(s) warn(&#34;You have empty smart definition(s)&#34;, ValidationWarning) . single = Alkane(n=5) cmpd = mb.fill_box(single, n_compounds=10, box=[10,10,10]) . /home/ayang41/programs/mbuild/mbuild/compound.py:2139: UserWarning: No simulation box detected for mdtraj.Trajectory &lt;mdtraj.Trajectory with 1 frames, 3 atoms, 1 residues, without unitcells&gt; &#34;mdtraj.Trajectory {}&#34;.format(traj) /home/ayang41/programs/mbuild/mbuild/compound.py:2139: UserWarning: No simulation box detected for mdtraj.Trajectory &lt;mdtraj.Trajectory with 1 frames, 4 atoms, 1 residues, without unitcells&gt; &#34;mdtraj.Trajectory {}&#34;.format(traj) /home/ayang41/programs/mbuild/mbuild/compound.py:2527: UserWarning: No box specified and no Compound.box detected. Using Compound.boundingbox + 0.5 nm buffer. Setting all box angles to 90 degrees. &#34;No box specified and no Compound.box detected. &#34; . view = single.visualize(backend=&#39;nglview&#39;) view . structure = cmpd.to_parmed() . Box of pentanes as parmed structures . import nglview nglview.show_parmed(structure) . Creating the molecule graph for all moleucles in our system . graph = nx.Graph() graph.add_nodes_from([a.idx for a in structure.atoms]) graph.add_edges_from([(b.atom1.idx, b.atom2.idx) for b in structure.bonds]) . Here we can see there&#39;s a few different graph connected components here, AKA each molecule . import matplotlib %matplotlib inline import matplotlib.pyplot as plt fig, ax = plt.subplots(1,1, figsize=(8,8), dpi=100) nx.draw_networkx(graph, node_size=100, with_labels=False, ax=ax) . Fortunately, networkx API has a connected components implementation. We have a list of sets of atom indices, where each set of atom indices refers to a connected component . individual_molecule_graphs = [*nx.connected_components(graph)] individual_molecule_graphs[0:3] . [{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}, {17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33}, {34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50}] . For each individual molecule graph, we can create a parmed structure. Our entire box of pentanes was one parmed structure, but now we&#39;re interested in creating N different parmed structures, one for each molecule. You could imagine creating another kind of object, like an mbuild compound or openmm topology, but to fit the foyer workflow, we operate on parmed structures. . all_substructures = [] for molecule_graph in individual_molecule_graphs: individual_structure = pmd.Structure() for idx in molecule_graph: individual_structure.add_atom(structure.atoms[idx], structure.atoms[idx].residue.name, structure.atoms[idx].residue.number) for neighbor_idx in graph[idx]: if idx &lt; neighbor_idx: individual_structure.bonds.append(pmd.Bond(structure.atoms[idx], structure.atoms[neighbor_idx])) all_substructures.append(individual_structure) . all_substructures . [&lt;Structure 17 atoms; 1 residues; 16 bonds; NOT parametrized&gt;, &lt;Structure 17 atoms; 1 residues; 16 bonds; NOT parametrized&gt;, &lt;Structure 17 atoms; 1 residues; 16 bonds; NOT parametrized&gt;, &lt;Structure 17 atoms; 1 residues; 16 bonds; NOT parametrized&gt;, &lt;Structure 17 atoms; 1 residues; 16 bonds; NOT parametrized&gt;, &lt;Structure 17 atoms; 1 residues; 16 bonds; NOT parametrized&gt;, &lt;Structure 17 atoms; 1 residues; 16 bonds; NOT parametrized&gt;, &lt;Structure 17 atoms; 1 residues; 16 bonds; NOT parametrized&gt;, &lt;Structure 17 atoms; 1 residues; 16 bonds; NOT parametrized&gt;, &lt;Structure 17 atoms; 1 residues; 16 bonds; NOT parametrized&gt;] . Simple iteration through each molecular subtructure, apply the force field to each . parametrized_substructures = [] for substructure in all_substructures: output_struc = ff.apply(substructure) parametrized_substructures.append(output_struc) . /home/ayang41/programs/foyer/foyer/forcefield.py:267: UserWarning: Parameters have not been assigned to all impropers. Total system impropers: 20, Parameterized impropers: 0. Note that if your system contains torsions of Ryckaert-Bellemans functional form, all of these torsions are processed as propers warnings.warn(msg) . parametrized_substructures . [&lt;Structure 17 atoms; 1 residues; 16 bonds; parametrized&gt;, &lt;Structure 17 atoms; 1 residues; 16 bonds; parametrized&gt;, &lt;Structure 17 atoms; 1 residues; 16 bonds; parametrized&gt;, &lt;Structure 17 atoms; 1 residues; 16 bonds; parametrized&gt;, &lt;Structure 17 atoms; 1 residues; 16 bonds; parametrized&gt;, &lt;Structure 17 atoms; 1 residues; 16 bonds; parametrized&gt;, &lt;Structure 17 atoms; 1 residues; 16 bonds; parametrized&gt;, &lt;Structure 17 atoms; 1 residues; 16 bonds; parametrized&gt;, &lt;Structure 17 atoms; 1 residues; 16 bonds; parametrized&gt;, &lt;Structure 17 atoms; 1 residues; 16 bonds; parametrized&gt;] . Because parmed structures override addition, we can combine structures via addition . parametrized_substructures[0] + parametrized_substructures[1] . &lt;Structure 34 atoms; 2 residues; 32 bonds; parametrized&gt; . Using functools, we can quickly and conveninetly combine all N parametrized structures into 1 structure . from functools import reduce parametrized_structure = reduce(lambda x,y: x+y, parametrized_substructures) . parametrized_structure . &lt;Structure 170 atoms; 10 residues; 160 bonds; parametrized&gt; . Rather than parametrize one, big parmed structure, we are parametrizing a bunch of small parmed structures, in serial. We&#39;re not distributing the workload, but we are simplifying the workload -- rather than match subgraphs among large, complex graphs of hundreds of nodes and edges, we are matching subgraphs among smaller, simpler graphs . Split a chemical system into smaller components, parametrize each molecule, in parallel . import dask from dask import delayed, bag as db . Streamline our code into functions that are mostly-compatible with dask. . The use of tuples over lists because tuples are hashable (important for dask) | Extra functions to map atomic indices to parmed Atoms. If we&#39;re going to create different parmed structures, we need to track parmed atoms | . from typing import List, Union, Set, Dict, Tuple def structure_to_graph(structure: pmd.Structure) -&gt; nx.Graph: graph = nx.Graph() graph.add_nodes_from([a.idx for a in structure.atoms]) graph.add_edges_from([(b.atom1.idx, b.atom2.idx) for b in structure.bonds]) return graph def separate_molecule_graphs(structure: pmd.Structure, graph: nx.Graph) -&gt; Tuple[Tuple[int,...]]: &quot;&quot;&quot; Use connected components to identify individual molecules&quot;&quot;&quot; individual_molecule_graphs = (tuple(a) for a in nx.connected_components(graph)) return individual_molecule_graphs def subselect_atoms(structure: pmd.Structure, indices: Tuple[int])-&gt; Dict[int, pmd.Atom]: &quot;&quot;&quot; Create a mapping of index to atom &quot;&quot;&quot; return {idx: structure.atoms[idx] for idx in indices} def make_structure_from_graph(molecule_vertices: Tuple[int], relevant_atoms: Dict[int, pmd.Atom], molecule_graph: nx.Graph) -&gt; pmd.Structure: &quot;&quot;&quot; From networkx graph and individal parmed atoms, make parmed structure&quot;&quot;&quot; individual_structure = pmd.Structure() for idx in molecule_vertices: individual_structure.add_atom(relevant_atoms[idx], relevant_atoms[idx].residue.name, relevant_atoms[idx].residue.number) for neighbor_idx in molecule_graph[idx]: if idx &lt; neighbor_idx: individual_structure.bonds.append(pmd.Bond(relevant_atoms[idx], relevant_atoms[neighbor_idx])) return individual_structure def parametrize(ff: foyer.Forcefield, structure: pmd.Structure, **kwargs) -&gt; pmd.Structure: return ff.apply(structure, **kwargs) . Exercising our functions in serial . We&#39;ll get to some timings later... . %%time single = Alkane(n=5) cmpd = mb.fill_box(single, n_compounds=10, box=[5,5,5]) structure = cmpd.to_parmed() big_graph = structure_to_graph(structure) individual_molecule_graphs = separate_molecule_graphs(structure, big_graph) individual_structures = [make_structure_from_graph(molecule_graph, subselect_atoms(structure, molecule_graph), big_graph) for molecule_graph in individual_molecule_graphs] parametrized_structures = [parametrize(ff, struc) for struc in individual_structures] parametrized_structure = reduce(lambda x,y: x+y, parametrized_structures) parametrized_structure . /home/ayang41/programs/mbuild/mbuild/compound.py:2139: UserWarning: No simulation box detected for mdtraj.Trajectory &lt;mdtraj.Trajectory with 1 frames, 3 atoms, 1 residues, without unitcells&gt; &#34;mdtraj.Trajectory {}&#34;.format(traj) /home/ayang41/programs/mbuild/mbuild/compound.py:2139: UserWarning: No simulation box detected for mdtraj.Trajectory &lt;mdtraj.Trajectory with 1 frames, 4 atoms, 1 residues, without unitcells&gt; &#34;mdtraj.Trajectory {}&#34;.format(traj) /home/ayang41/programs/mbuild/mbuild/compound.py:2527: UserWarning: No box specified and no Compound.box detected. Using Compound.boundingbox + 0.5 nm buffer. Setting all box angles to 90 degrees. &#34;No box specified and no Compound.box detected. &#34; . CPU times: user 2.69 s, sys: 56.1 ms, total: 2.75 s Wall time: 2.7 s . &lt;Structure 170 atoms; 10 residues; 160 bonds; parametrized&gt; . Here&#39;s a first attempt at daskifying everything with delayed objects. Once we&#39;ve created our entire system graph, we can start creating dask objects, starting with each molecule graph, and chaining the following operations: . From each molecule graph, grab the relevant parmed Atoms | From the molecule graph and parmed Atoms, create the (unparametrized) parmed Structure | . %%time single = Alkane(n=5) cmpd = mb.fill_box(single, n_compounds=10, box=[5,5,5]) structure = cmpd.to_parmed() big_graph = structure_to_graph(structure) individual_molecule_graphs = [*separate_molecule_graphs(structure, big_graph)] all_subselected_atoms = [delayed(subselect_atoms)(structure, molecule_graph) for molecule_graph in individual_molecule_graphs] raw_structures = [delayed(make_structure_from_graph)(molecule_graph, subselected_atoms, big_graph) for molecule_graph, subselected_atoms in zip(individual_molecule_graphs, all_subselected_atoms)] . CPU times: user 149 ms, sys: 23.2 ms, total: 173 ms Wall time: 60.4 ms . Pulse check, can we flush the task-graph and actually get our parametrized molecules? . %%time [a.compute() for a in raw_structures] . CPU times: user 18.8 ms, sys: 889 ¬µs, total: 19.7 ms Wall time: 11.8 ms . [&lt;Structure 17 atoms; 1 residues; 16 bonds; NOT parametrized&gt;, &lt;Structure 17 atoms; 1 residues; 16 bonds; NOT parametrized&gt;, &lt;Structure 17 atoms; 1 residues; 16 bonds; NOT parametrized&gt;, &lt;Structure 17 atoms; 1 residues; 16 bonds; NOT parametrized&gt;, &lt;Structure 17 atoms; 1 residues; 16 bonds; NOT parametrized&gt;, &lt;Structure 17 atoms; 1 residues; 16 bonds; NOT parametrized&gt;, &lt;Structure 17 atoms; 1 residues; 16 bonds; NOT parametrized&gt;, &lt;Structure 17 atoms; 1 residues; 16 bonds; NOT parametrized&gt;, &lt;Structure 17 atoms; 1 residues; 16 bonds; NOT parametrized&gt;, &lt;Structure 17 atoms; 1 residues; 16 bonds; NOT parametrized&gt;] . Next step, parametrization . %%time param_structures = [delayed(parametrize)(ff, struc) for struc in raw_structures] . CPU times: user 5.69 ms, sys: 594 ¬µs, total: 6.28 ms Wall time: 2.81 ms . (Another) pulse check, does the FF application work? . %%time all_parametrized = [op.compute() for op in param_structures] all_parametrized . CPU times: user 2.69 s, sys: 15.9 ms, total: 2.71 s Wall time: 2.7 s . [&lt;Structure 17 atoms; 1 residues; 16 bonds; parametrized&gt;, &lt;Structure 17 atoms; 1 residues; 16 bonds; parametrized&gt;, &lt;Structure 17 atoms; 1 residues; 16 bonds; parametrized&gt;, &lt;Structure 17 atoms; 1 residues; 16 bonds; parametrized&gt;, &lt;Structure 17 atoms; 1 residues; 16 bonds; parametrized&gt;, &lt;Structure 17 atoms; 1 residues; 16 bonds; parametrized&gt;, &lt;Structure 17 atoms; 1 residues; 16 bonds; parametrized&gt;, &lt;Structure 17 atoms; 1 residues; 16 bonds; parametrized&gt;, &lt;Structure 17 atoms; 1 residues; 16 bonds; parametrized&gt;, &lt;Structure 17 atoms; 1 residues; 16 bonds; parametrized&gt;] . Final step, putting the structures back together . %%time reduce(lambda x,y: x+y, all_parametrized) . CPU times: user 52.7 ms, sys: 23 ¬µs, total: 52.7 ms Wall time: 50.8 ms . &lt;Structure 170 atoms; 10 residues; 160 bonds; parametrized&gt; . %%time single = Alkane(n=5) cmpd = mb.fill_box(single, n_compounds=10, box=[5,5,5]) structure = cmpd.to_parmed() big_graph = structure_to_graph(structure) individual_molecule_graphs = [*separate_molecule_graphs(structure, big_graph)] all_subselected_atoms = [delayed(subselect_atoms)(structure, molecule_graph) for molecule_graph in individual_molecule_graphs] raw_structures = [delayed(make_structure_from_graph)(molecule_graph, subselected_atoms, big_graph) for molecule_graph, subselected_atoms in zip(individual_molecule_graphs, all_subselected_atoms)] param_structures = [delayed(parametrize)(ff, struc) for struc in raw_structures] . CPU times: user 65.5 ms, sys: 41.5 ms, total: 107 ms Wall time: 51.4 ms . Last step is to combine all the parametrized structures, we can try some dask fold/reduce operations . param_structures_bag = db.from_sequence(param_structures) param_structures_bag . dask.bag&lt;from_sequence, npartitions=10&gt; . Unfortuantely, some of these parmed AtomType objects are not hashable, so we cannot use dask to efficiently reduce parmed structures . from operator import add param_structures_bag.fold(add).compute() . TypeError Traceback (most recent call last) &lt;ipython-input-37-2285eb100c4e&gt; in &lt;module&gt; 1 from operator import add 2 -&gt; 3 param_structures_bag.fold(add).compute() ~/miniconda3/envs/md37/lib/python3.7/site-packages/dask/base.py in compute(self, **kwargs) 164 dask.base.compute 165 &#34;&#34;&#34; --&gt; 166 (result,) = compute(self, traverse=False, **kwargs) 167 return result 168 ~/miniconda3/envs/md37/lib/python3.7/site-packages/dask/base.py in compute(*args, **kwargs) 442 postcomputes.append(x.__dask_postcompute__()) 443 --&gt; 444 results = schedule(dsk, keys, **kwargs) 445 return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)]) 446 ~/miniconda3/envs/md37/lib/python3.7/site-packages/dask/multiprocessing.py in get(dsk, keys, num_workers, func_loads, func_dumps, optimize_graph, pool, **kwargs) 216 pack_exception=pack_exception, 217 raise_exception=reraise, --&gt; 218 **kwargs 219 ) 220 finally: ~/miniconda3/envs/md37/lib/python3.7/site-packages/dask/local.py in get_async(apply_async, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, **kwargs) 469 # Seed initial tasks into the thread pool 470 while state[&#34;ready&#34;] and len(state[&#34;running&#34;]) &lt; num_workers: --&gt; 471 fire_task() 472 473 # Main loop, wait on tasks to finish, insert new ones ~/miniconda3/envs/md37/lib/python3.7/site-packages/dask/local.py in fire_task() 458 args=( 459 key, --&gt; 460 dumps((dsk[key], data)), 461 dumps, 462 loads, ~/miniconda3/envs/md37/lib/python3.7/site-packages/cloudpickle/cloudpickle.py in dumps(obj, protocol) 1081 try: 1082 cp = CloudPickler(file, protocol=protocol) -&gt; 1083 cp.dump(obj) 1084 return file.getvalue() 1085 finally: ~/miniconda3/envs/md37/lib/python3.7/site-packages/cloudpickle/cloudpickle.py in dump(self, obj) 475 self.inject_addons() 476 try: --&gt; 477 return Pickler.dump(self, obj) 478 except RuntimeError as e: 479 if &#39;recursion&#39; in e.args[0]: ~/miniconda3/envs/md37/lib/python3.7/pickle.py in dump(self, obj) 435 if self.proto &gt;= 4: 436 self.framer.start_framing() --&gt; 437 self.save(obj) 438 self.write(STOP) 439 self.framer.end_framing() ~/miniconda3/envs/md37/lib/python3.7/pickle.py in save(self, obj, save_persistent_id) 502 f = self.dispatch.get(t) 503 if f is not None: --&gt; 504 f(self, obj) # Call unbound method with explicit self 505 return 506 ~/miniconda3/envs/md37/lib/python3.7/pickle.py in save_tuple(self, obj) 772 if n &lt;= 3 and self.proto &gt;= 2: 773 for element in obj: --&gt; 774 save(element) 775 # Subtle. Same as in the big comment below. 776 if id(obj) in memo: ~/miniconda3/envs/md37/lib/python3.7/pickle.py in save(self, obj, save_persistent_id) 502 f = self.dispatch.get(t) 503 if f is not None: --&gt; 504 f(self, obj) # Call unbound method with explicit self 505 return 506 ~/miniconda3/envs/md37/lib/python3.7/pickle.py in save_dict(self, obj) 857 858 self.memoize(obj) --&gt; 859 self._batch_setitems(obj.items()) 860 861 dispatch[dict] = save_dict ~/miniconda3/envs/md37/lib/python3.7/pickle.py in _batch_setitems(self, items) 888 k, v = tmp[0] 889 save(k) --&gt; 890 save(v) 891 write(SETITEM) 892 # else tmp is empty, and we&#39;re done ~/miniconda3/envs/md37/lib/python3.7/pickle.py in save(self, obj, save_persistent_id) 502 f = self.dispatch.get(t) 503 if f is not None: --&gt; 504 f(self, obj) # Call unbound method with explicit self 505 return 506 ~/miniconda3/envs/md37/lib/python3.7/pickle.py in save_list(self, obj) 817 818 self.memoize(obj) --&gt; 819 self._batch_appends(obj) 820 821 dispatch[list] = save_list ~/miniconda3/envs/md37/lib/python3.7/pickle.py in _batch_appends(self, items) 844 write(APPENDS) 845 elif n: --&gt; 846 save(tmp[0]) 847 write(APPEND) 848 # else tmp is empty, and we&#39;re done ~/miniconda3/envs/md37/lib/python3.7/pickle.py in save(self, obj, save_persistent_id) 547 548 # Save the reduce() output and finally memoize the object --&gt; 549 self.save_reduce(obj=obj, *rv) 550 551 def persistent_id(self, obj): ~/miniconda3/envs/md37/lib/python3.7/pickle.py in save_reduce(self, func, args, state, listitems, dictitems, obj) 660 661 if state is not None: --&gt; 662 save(state) 663 write(BUILD) 664 ~/miniconda3/envs/md37/lib/python3.7/pickle.py in save(self, obj, save_persistent_id) 502 f = self.dispatch.get(t) 503 if f is not None: --&gt; 504 f(self, obj) # Call unbound method with explicit self 505 return 506 ~/miniconda3/envs/md37/lib/python3.7/pickle.py in save_tuple(self, obj) 772 if n &lt;= 3 and self.proto &gt;= 2: 773 for element in obj: --&gt; 774 save(element) 775 # Subtle. Same as in the big comment below. 776 if id(obj) in memo: ~/miniconda3/envs/md37/lib/python3.7/pickle.py in save(self, obj, save_persistent_id) 547 548 # Save the reduce() output and finally memoize the object --&gt; 549 self.save_reduce(obj=obj, *rv) 550 551 def persistent_id(self, obj): ~/miniconda3/envs/md37/lib/python3.7/pickle.py in save_reduce(self, func, args, state, listitems, dictitems, obj) 660 661 if state is not None: --&gt; 662 save(state) 663 write(BUILD) 664 ~/miniconda3/envs/md37/lib/python3.7/pickle.py in save(self, obj, save_persistent_id) 502 f = self.dispatch.get(t) 503 if f is not None: --&gt; 504 f(self, obj) # Call unbound method with explicit self 505 return 506 ~/miniconda3/envs/md37/lib/python3.7/pickle.py in save_dict(self, obj) 857 858 self.memoize(obj) --&gt; 859 self._batch_setitems(obj.items()) 860 861 dispatch[dict] = save_dict ~/miniconda3/envs/md37/lib/python3.7/pickle.py in _batch_setitems(self, items) 883 for k, v in tmp: 884 save(k) --&gt; 885 save(v) 886 write(SETITEMS) 887 elif n: ~/miniconda3/envs/md37/lib/python3.7/pickle.py in save(self, obj, save_persistent_id) 502 f = self.dispatch.get(t) 503 if f is not None: --&gt; 504 f(self, obj) # Call unbound method with explicit self 505 return 506 ~/miniconda3/envs/md37/lib/python3.7/pickle.py in save_dict(self, obj) 857 858 self.memoize(obj) --&gt; 859 self._batch_setitems(obj.items()) 860 861 dispatch[dict] = save_dict ~/miniconda3/envs/md37/lib/python3.7/pickle.py in _batch_setitems(self, items) 883 for k, v in tmp: 884 save(k) --&gt; 885 save(v) 886 write(SETITEMS) 887 elif n: ~/miniconda3/envs/md37/lib/python3.7/pickle.py in save(self, obj, save_persistent_id) 502 f = self.dispatch.get(t) 503 if f is not None: --&gt; 504 f(self, obj) # Call unbound method with explicit self 505 return 506 ~/miniconda3/envs/md37/lib/python3.7/pickle.py in save_dict(self, obj) 857 858 self.memoize(obj) --&gt; 859 self._batch_setitems(obj.items()) 860 861 dispatch[dict] = save_dict ~/miniconda3/envs/md37/lib/python3.7/pickle.py in _batch_setitems(self, items) 888 k, v = tmp[0] 889 save(k) --&gt; 890 save(v) 891 write(SETITEM) 892 # else tmp is empty, and we&#39;re done ~/miniconda3/envs/md37/lib/python3.7/pickle.py in save(self, obj, save_persistent_id) 502 f = self.dispatch.get(t) 503 if f is not None: --&gt; 504 f(self, obj) # Call unbound method with explicit self 505 return 506 ~/miniconda3/envs/md37/lib/python3.7/pickle.py in save_tuple(self, obj) 772 if n &lt;= 3 and self.proto &gt;= 2: 773 for element in obj: --&gt; 774 save(element) 775 # Subtle. Same as in the big comment below. 776 if id(obj) in memo: ~/miniconda3/envs/md37/lib/python3.7/pickle.py in save(self, obj, save_persistent_id) 547 548 # Save the reduce() output and finally memoize the object --&gt; 549 self.save_reduce(obj=obj, *rv) 550 551 def persistent_id(self, obj): ~/miniconda3/envs/md37/lib/python3.7/pickle.py in save_reduce(self, func, args, state, listitems, dictitems, obj) 660 661 if state is not None: --&gt; 662 save(state) 663 write(BUILD) 664 ~/miniconda3/envs/md37/lib/python3.7/pickle.py in save(self, obj, save_persistent_id) 502 f = self.dispatch.get(t) 503 if f is not None: --&gt; 504 f(self, obj) # Call unbound method with explicit self 505 return 506 ~/miniconda3/envs/md37/lib/python3.7/pickle.py in save_dict(self, obj) 857 858 self.memoize(obj) --&gt; 859 self._batch_setitems(obj.items()) 860 861 dispatch[dict] = save_dict ~/miniconda3/envs/md37/lib/python3.7/pickle.py in _batch_setitems(self, items) 883 for k, v in tmp: 884 save(k) --&gt; 885 save(v) 886 write(SETITEMS) 887 elif n: ~/miniconda3/envs/md37/lib/python3.7/pickle.py in save(self, obj, save_persistent_id) 547 548 # Save the reduce() output and finally memoize the object --&gt; 549 self.save_reduce(obj=obj, *rv) 550 551 def persistent_id(self, obj): ~/miniconda3/envs/md37/lib/python3.7/pickle.py in save_reduce(self, func, args, state, listitems, dictitems, obj) 654 655 if listitems is not None: --&gt; 656 self._batch_appends(listitems) 657 658 if dictitems is not None: ~/miniconda3/envs/md37/lib/python3.7/pickle.py in _batch_appends(self, items) 844 write(APPENDS) 845 elif n: --&gt; 846 save(tmp[0]) 847 write(APPEND) 848 # else tmp is empty, and we&#39;re done ~/miniconda3/envs/md37/lib/python3.7/pickle.py in save(self, obj, save_persistent_id) 547 548 # Save the reduce() output and finally memoize the object --&gt; 549 self.save_reduce(obj=obj, *rv) 550 551 def persistent_id(self, obj): ~/miniconda3/envs/md37/lib/python3.7/pickle.py in save_reduce(self, func, args, state, listitems, dictitems, obj) 660 661 if state is not None: --&gt; 662 save(state) 663 write(BUILD) 664 ~/miniconda3/envs/md37/lib/python3.7/pickle.py in save(self, obj, save_persistent_id) 502 f = self.dispatch.get(t) 503 if f is not None: --&gt; 504 f(self, obj) # Call unbound method with explicit self 505 return 506 ~/miniconda3/envs/md37/lib/python3.7/pickle.py in save_dict(self, obj) 857 858 self.memoize(obj) --&gt; 859 self._batch_setitems(obj.items()) 860 861 dispatch[dict] = save_dict ~/miniconda3/envs/md37/lib/python3.7/pickle.py in _batch_setitems(self, items) 883 for k, v in tmp: 884 save(k) --&gt; 885 save(v) 886 write(SETITEMS) 887 elif n: ~/miniconda3/envs/md37/lib/python3.7/pickle.py in save(self, obj, save_persistent_id) 502 f = self.dispatch.get(t) 503 if f is not None: --&gt; 504 f(self, obj) # Call unbound method with explicit self 505 return 506 ~/miniconda3/envs/md37/lib/python3.7/pickle.py in save_list(self, obj) 817 818 self.memoize(obj) --&gt; 819 self._batch_appends(obj) 820 821 dispatch[list] = save_list ~/miniconda3/envs/md37/lib/python3.7/pickle.py in _batch_appends(self, items) 841 write(MARK) 842 for x in tmp: --&gt; 843 save(x) 844 write(APPENDS) 845 elif n: ~/miniconda3/envs/md37/lib/python3.7/pickle.py in save(self, obj, save_persistent_id) 547 548 # Save the reduce() output and finally memoize the object --&gt; 549 self.save_reduce(obj=obj, *rv) 550 551 def persistent_id(self, obj): ~/miniconda3/envs/md37/lib/python3.7/pickle.py in save_reduce(self, func, args, state, listitems, dictitems, obj) 660 661 if state is not None: --&gt; 662 save(state) 663 write(BUILD) 664 ~/miniconda3/envs/md37/lib/python3.7/pickle.py in save(self, obj, save_persistent_id) 502 f = self.dispatch.get(t) 503 if f is not None: --&gt; 504 f(self, obj) # Call unbound method with explicit self 505 return 506 ~/miniconda3/envs/md37/lib/python3.7/pickle.py in save_dict(self, obj) 857 858 self.memoize(obj) --&gt; 859 self._batch_setitems(obj.items()) 860 861 dispatch[dict] = save_dict ~/miniconda3/envs/md37/lib/python3.7/pickle.py in _batch_setitems(self, items) 883 for k, v in tmp: 884 save(k) --&gt; 885 save(v) 886 write(SETITEMS) 887 elif n: ~/miniconda3/envs/md37/lib/python3.7/pickle.py in save(self, obj, save_persistent_id) 533 # Check for string returned by reduce(), meaning &#34;save as global&#34; 534 if isinstance(rv, str): --&gt; 535 self.save_global(obj, rv) 536 return 537 ~/miniconda3/envs/md37/lib/python3.7/site-packages/cloudpickle/cloudpickle.py in save_global(self, obj, name, pack) 828 elif obj is type(NotImplemented): 829 return self.save_reduce(type, (NotImplemented,), obj=obj) --&gt; 830 elif obj in _BUILTIN_TYPE_NAMES: 831 return self.save_reduce( 832 _builtin_type, (_BUILTIN_TYPE_NAMES[obj],), obj=obj) TypeError: unhashable type: &#39;_UnassignedAtomType&#39; . At this point, we can use dask to parallelize most of the steps in our process, but we still need to collect all of our parametrized structures prior to summing them all up . Timing isn&#39;t so great but we&#39;ll see how this scales . %%time computed_parametrized_structures = [d.compute() for d in param_structures] final_structure = reduce(lambda x,y: x+y, computed_parametrized_structures) final_structure . CPU times: user 2.75 s, sys: 0 ns, total: 2.75 s Wall time: 2.74 s . &lt;Structure 170 atoms; 10 residues; 160 bonds; parametrized&gt; . Putting all of our parallelized code together ... . %%time # Make our molecular system single = Alkane(n=5) cmpd = mb.fill_box(single, n_compounds=10, box=[5,5,5]) structure = cmpd.to_parmed() # Convert to graphs big_graph = structure_to_graph(structure) individual_molecule_graphs = [*separate_molecule_graphs(structure, big_graph)] # Grab parmed atoms for each node in the graph all_subselected_atoms = [delayed(subselect_atoms)(structure, molecule_graph) for molecule_graph in individual_molecule_graphs] # Generate parmed structures for each molecule raw_structures = [delayed(make_structure_from_graph)(molecule_graph, subselected_atoms, big_graph) for molecule_graph, subselected_atoms in zip(individual_molecule_graphs, all_subselected_atoms)] # Parametrize with our force field param_structures = [delayed(parametrize)(ff, struc) for struc in raw_structures] computed_parametrized_structures = [d.compute() for d in param_structures] final_structure = reduce(lambda x,y: x+y, computed_parametrized_structures) . CPU times: user 2.7 s, sys: 58.3 ms, total: 2.76 s Wall time: 2.7 s . Visualizing our task graph . param_structures[0].visualize() . Before moving to timing comparisons, it&#39;s important to observe the residue_map functionality for foyer. If a &quot;residue&quot; (molecule type) has already been parametrized within this foyer apply function stack, we don&#39;t need to re-iterate and re-discover the atom-types; the parametrization is effectively cached. As multiple foyer apply functions get called, this caching doesn&#39;t get leveraged. . Timing comparisons . We have 3 methods to compare: . Canonical foyer, the standard way to use foyer on a single parmed structure that represents your entire molecular system. This actualy takes most advantage of the use_residue_map functionality . | Distributed foyer in serial, divide your parmed structure into smaller parmed structures, parametrize individually . | Distributed foyer in parallel, divide your parmed structure into smaller parmed structures, parametrize individually. . | We&#39;ll notice the number of residues in the final, parametrized strucutres are different -- this is a consequnce of how parmed.structure.__add__ and parmed.structure.__iadd__ work when you try to combine different parmed structures. What&#39;s important is that the number of atoms and bonds are consistent . def canonical_foyer(ff, structure, **kwargs): &quot;&quot;&quot; Standard way of using foyer, no parallelization&quot;&quot;&quot; return ff.apply(structure, **kwargs) def distributed_foyer_serial(ff, structure): &quot;&quot;&quot; Apply foyer N times to N different molecules in serial&quot;&quot;&quot; big_graph = structure_to_graph(structure) individual_molecule_graphs = separate_molecule_graphs(structure, big_graph) individual_structures = [make_structure_from_graph(molecule_graph, subselect_atoms(structure, molecule_graph), big_graph) for molecule_graph in individual_molecule_graphs] parametrized_structures = [parametrize(ff, struc) for struc in individual_structures] parametrized_structure = reduce(lambda x,y: x+y, parametrized_structures) return parametrized_structure def distributed_foyer_parallel(ff, structure): &quot;&quot;&quot;Apply foyer N times to N different molecules in parallel&quot;&quot;&quot; big_graph = structure_to_graph(structure) individual_molecule_graphs = [*separate_molecule_graphs(structure, big_graph)] # Grab parmed atoms for each node in the graph all_subselected_atoms = [delayed(subselect_atoms)(structure, molecule_graph) for molecule_graph in individual_molecule_graphs] # Generate parmed structures for each molecule raw_structures = [delayed(make_structure_from_graph)(molecule_graph, subselected_atoms, big_graph) for molecule_graph, subselected_atoms in zip(individual_molecule_graphs, all_subselected_atoms)] # Parametrize with our force field param_structures = [delayed(parametrize)(ff, struc) for struc in raw_structures] computed_parametrized_structures = [d.compute() for d in param_structures] final_structure = reduce(lambda x,y: x+y, computed_parametrized_structures) return final_structure . Small, homogeneous system . 10 pentane molecules . Method Time . Canonical foyer | 2.53 s | . Distributed foyer serial | 3.15 s | . Distributed foyer parallel | 3.22 s | . %%time ff = foyer.forcefields.load_OPLSAA() single = Alkane(n=5) cmpd = mb.fill_box(single, n_compounds=10, box=[10,10,10]) structure = cmpd.to_parmed() canonical_foyer(ff, structure) . CPU times: user 2.52 s, sys: 57.7 ms, total: 2.58 s Wall time: 2.53 s . /home/ayang41/programs/foyer/foyer/forcefield.py:267: UserWarning: Parameters have not been assigned to all impropers. Total system impropers: 200, Parameterized impropers: 0. Note that if your system contains torsions of Ryckaert-Bellemans functional form, all of these torsions are processed as propers warnings.warn(msg) . &lt;Structure 170 atoms; 1 residues; 160 bonds; PBC (orthogonal); parametrized&gt; . %%time ff = foyer.forcefields.load_OPLSAA() single = Alkane(n=5) cmpd = mb.fill_box(single, n_compounds=10, box=[10,10,10]) structure = cmpd.to_parmed() distributed_foyer_serial(ff, structure) . CPU times: user 3.17 s, sys: 28.5 ms, total: 3.2 s Wall time: 3.15 s . &lt;Structure 170 atoms; 10 residues; 160 bonds; parametrized&gt; . %%time ff = foyer.forcefields.load_OPLSAA() single = Alkane(n=5) cmpd = mb.fill_box(single, n_compounds=10, box=[10,10,10]) structure = cmpd.to_parmed() distributed_foyer_parallel(ff, structure) . CPU times: user 3.21 s, sys: 69.5 ms, total: 3.28 s Wall time: 3.22 s . &lt;Structure 170 atoms; 10 residues; 160 bonds; parametrized&gt; . Large, homogeneous system . 100 pentane molecules . Method Time . Canonical foyer | 21.1 s | . Distributed foyer serial | 35.1 s | . Distributed foyer parallel | 34.7 s | . %%time ff = foyer.forcefields.load_OPLSAA() single = Alkane(n=5) cmpd = mb.fill_box(single, n_compounds=100, box=[1000,1000,1000]) structure = cmpd.to_parmed() canonical_foyer(ff, structure) . CPU times: user 20.9 s, sys: 196 ms, total: 21.1 s Wall time: 21 s . /home/ayang41/programs/foyer/foyer/forcefield.py:267: UserWarning: Parameters have not been assigned to all impropers. Total system impropers: 2000, Parameterized impropers: 0. Note that if your system contains torsions of Ryckaert-Bellemans functional form, all of these torsions are processed as propers warnings.warn(msg) . &lt;Structure 1700 atoms; 1 residues; 1600 bonds; PBC (orthogonal); parametrized&gt; . %%time ff = foyer.forcefields.load_OPLSAA() single = Alkane(n=5) cmpd = mb.fill_box(single, n_compounds=100, box=[1000,1000,1000]) structure = cmpd.to_parmed() distributed_foyer_serial(ff, structure) . CPU times: user 35.1 s, sys: 124 ms, total: 35.2 s Wall time: 35.1 s . &lt;Structure 1700 atoms; 100 residues; 1600 bonds; parametrized&gt; . %%time ff = foyer.forcefields.load_OPLSAA() single = Alkane(n=5) cmpd = mb.fill_box(single, n_compounds=100, box=[1000,1000,1000]) structure = cmpd.to_parmed() distributed_foyer_parallel(ff, structure) . CPU times: user 34.8 s, sys: 159 ms, total: 34.9 s Wall time: 34.7 s . &lt;Structure 1700 atoms; 100 residues; 1600 bonds; parametrized&gt; . Small, heterogeneous system . 10 pentane, 10 decane, 10 nonadecane (C20-ane) . Method Time . Canonical foyer | 14 s | . Distributed foyer serial | 16.9 s | . Distributed foyer parallel | 16.6 s | . %%time ff = foyer.forcefields.load_OPLSAA() templates = [Alkane(n=5), Alkane(n=10), Alkane(n=20)] cmpd = mb.fill_box(templates, n_compounds=[10,10,10], box=[100,100,100]) structure = cmpd.to_parmed() canonical_foyer(ff, structure) . CPU times: user 14.2 s, sys: 130 ms, total: 14.3 s Wall time: 14 s . /home/ayang41/programs/foyer/foyer/forcefield.py:267: UserWarning: Parameters have not been assigned to all impropers. Total system impropers: 1400, Parameterized impropers: 0. Note that if your system contains torsions of Ryckaert-Bellemans functional form, all of these torsions are processed as propers warnings.warn(msg) . &lt;Structure 1110 atoms; 1 residues; 1080 bonds; PBC (orthogonal); parametrized&gt; . %%time ff = foyer.forcefields.load_OPLSAA() templates = [Alkane(n=5), Alkane(n=10), Alkane(n=20)] cmpd = mb.fill_box(templates, n_compounds=[10,10,10], box=[100,100,100]) structure = cmpd.to_parmed() distributed_foyer_serial(ff, structure) . /home/ayang41/programs/foyer/foyer/forcefield.py:267: UserWarning: Parameters have not been assigned to all impropers. Total system impropers: 40, Parameterized impropers: 0. Note that if your system contains torsions of Ryckaert-Bellemans functional form, all of these torsions are processed as propers warnings.warn(msg) /home/ayang41/programs/foyer/foyer/forcefield.py:267: UserWarning: Parameters have not been assigned to all impropers. Total system impropers: 80, Parameterized impropers: 0. Note that if your system contains torsions of Ryckaert-Bellemans functional form, all of these torsions are processed as propers warnings.warn(msg) . CPU times: user 17.1 s, sys: 170 ms, total: 17.3 s Wall time: 16.9 s . &lt;Structure 1110 atoms; 30 residues; 1080 bonds; parametrized&gt; . %%time ff = foyer.forcefields.load_OPLSAA() templates = [Alkane(n=5), Alkane(n=10), Alkane(n=20)] cmpd = mb.fill_box(templates, n_compounds=[10,10,10], box=[100,100,100]) structure = cmpd.to_parmed() distributed_foyer_parallel(ff, structure) . CPU times: user 16.7 s, sys: 222 ms, total: 16.9 s Wall time: 16.6 s . &lt;Structure 1110 atoms; 30 residues; 1080 bonds; parametrized&gt; . Large, heterogeneous system . 100 pentane, 100 decane, 100 nonadecane . Method Time . Canonical foyer | 2 min 31 s | . Distributed foyer serial | 4 min 20 s | . Distributed foyer parallel | 4 min 17 s | . %%time ff = foyer.forcefields.load_OPLSAA() templates = [Alkane(n=5), Alkane(n=10), Alkane(n=20)] cmpd = mb.fill_box(templates, n_compounds=[100,100,100], box=[1000,1000,1000]) structure = cmpd.to_parmed() canonical_foyer(ff, structure) . CPU times: user 2min 30s, sys: 1.27 s, total: 2min 31s Wall time: 2min 31s . /home/ayang41/programs/foyer/foyer/forcefield.py:267: UserWarning: Parameters have not been assigned to all impropers. Total system impropers: 14000, Parameterized impropers: 0. Note that if your system contains torsions of Ryckaert-Bellemans functional form, all of these torsions are processed as propers warnings.warn(msg) . &lt;Structure 11100 atoms; 1 residues; 10800 bonds; PBC (orthogonal); parametrized&gt; . %%time ff = foyer.forcefields.load_OPLSAA() templates = [Alkane(n=5), Alkane(n=10), Alkane(n=20)] cmpd = mb.fill_box(templates, n_compounds=[100,100,100], box=[1000,1000,1000]) structure = cmpd.to_parmed() distributed_foyer_serial(ff, structure) . CPU times: user 4min 20s, sys: 1.05 s, total: 4min 21s Wall time: 4min 20s . &lt;Structure 11100 atoms; 300 residues; 10800 bonds; parametrized&gt; . %%time ff = foyer.forcefields.load_OPLSAA() templates = [Alkane(n=5), Alkane(n=10), Alkane(n=20)] cmpd = mb.fill_box(templates, n_compounds=[100,100,100], box=[1000,1000,1000]) structure = cmpd.to_parmed() distributed_foyer_parallel(ff, structure) . CPU times: user 4min 17s, sys: 972 ms, total: 4min 18s Wall time: 4min 17s . &lt;Structure 11100 atoms; 300 residues; 10800 bonds; parametrized&gt; . Random heterogeneous system . Method Time . Canonical foyer | 1 min 38 s | . Distributed foyer serial | 2 min 56 s | . Distributed foyer parallel | 3 min 1 s | . import numpy as np random_compounds = mb.Compound(subcompounds=[Alkane(n=i) for i in np.random.randint(5, high=20, size=200)]) . /home/ayang41/programs/mbuild/mbuild/compound.py:2139: UserWarning: No simulation box detected for mdtraj.Trajectory &lt;mdtraj.Trajectory with 1 frames, 3 atoms, 1 residues, without unitcells&gt; &#34;mdtraj.Trajectory {}&#34;.format(traj) /home/ayang41/programs/mbuild/mbuild/compound.py:2139: UserWarning: No simulation box detected for mdtraj.Trajectory &lt;mdtraj.Trajectory with 1 frames, 4 atoms, 1 residues, without unitcells&gt; &#34;mdtraj.Trajectory {}&#34;.format(traj) . %%time ff = foyer.forcefields.load_OPLSAA() structure = random_compounds.to_parmed() canonical_foyer(ff, structure, use_residue_map=False) . CPU times: user 1min 37s, sys: 158 ms, total: 1min 37s Wall time: 1min 37s . &lt;Structure 7663 atoms; 1 residues; 7463 bonds; PBC (orthogonal); parametrized&gt; . %%time ff = foyer.forcefields.load_OPLSAA() structure = random_compounds.to_parmed() distributed_foyer_serial(ff, structure) . /home/ayang41/programs/foyer/foyer/forcefield.py:267: UserWarning: Parameters have not been assigned to all impropers. Total system impropers: 28, Parameterized impropers: 0. Note that if your system contains torsions of Ryckaert-Bellemans functional form, all of these torsions are processed as propers warnings.warn(msg) /home/ayang41/programs/foyer/foyer/forcefield.py:267: UserWarning: Parameters have not been assigned to all impropers. Total system impropers: 44, Parameterized impropers: 0. Note that if your system contains torsions of Ryckaert-Bellemans functional form, all of these torsions are processed as propers warnings.warn(msg) /home/ayang41/programs/foyer/foyer/forcefield.py:267: UserWarning: Parameters have not been assigned to all impropers. Total system impropers: 60, Parameterized impropers: 0. Note that if your system contains torsions of Ryckaert-Bellemans functional form, all of these torsions are processed as propers warnings.warn(msg) /home/ayang41/programs/foyer/foyer/forcefield.py:267: UserWarning: Parameters have not been assigned to all impropers. Total system impropers: 24, Parameterized impropers: 0. Note that if your system contains torsions of Ryckaert-Bellemans functional form, all of these torsions are processed as propers warnings.warn(msg) /home/ayang41/programs/foyer/foyer/forcefield.py:267: UserWarning: Parameters have not been assigned to all impropers. Total system impropers: 56, Parameterized impropers: 0. Note that if your system contains torsions of Ryckaert-Bellemans functional form, all of these torsions are processed as propers warnings.warn(msg) /home/ayang41/programs/foyer/foyer/forcefield.py:267: UserWarning: Parameters have not been assigned to all impropers. Total system impropers: 52, Parameterized impropers: 0. Note that if your system contains torsions of Ryckaert-Bellemans functional form, all of these torsions are processed as propers warnings.warn(msg) /home/ayang41/programs/foyer/foyer/forcefield.py:267: UserWarning: Parameters have not been assigned to all impropers. Total system impropers: 64, Parameterized impropers: 0. Note that if your system contains torsions of Ryckaert-Bellemans functional form, all of these torsions are processed as propers warnings.warn(msg) /home/ayang41/programs/foyer/foyer/forcefield.py:267: UserWarning: Parameters have not been assigned to all impropers. Total system impropers: 68, Parameterized impropers: 0. Note that if your system contains torsions of Ryckaert-Bellemans functional form, all of these torsions are processed as propers warnings.warn(msg) /home/ayang41/programs/foyer/foyer/forcefield.py:267: UserWarning: Parameters have not been assigned to all impropers. Total system impropers: 36, Parameterized impropers: 0. Note that if your system contains torsions of Ryckaert-Bellemans functional form, all of these torsions are processed as propers warnings.warn(msg) /home/ayang41/programs/foyer/foyer/forcefield.py:267: UserWarning: Parameters have not been assigned to all impropers. Total system impropers: 32, Parameterized impropers: 0. Note that if your system contains torsions of Ryckaert-Bellemans functional form, all of these torsions are processed as propers warnings.warn(msg) /home/ayang41/programs/foyer/foyer/forcefield.py:267: UserWarning: Parameters have not been assigned to all impropers. Total system impropers: 76, Parameterized impropers: 0. Note that if your system contains torsions of Ryckaert-Bellemans functional form, all of these torsions are processed as propers warnings.warn(msg) /home/ayang41/programs/foyer/foyer/forcefield.py:267: UserWarning: Parameters have not been assigned to all impropers. Total system impropers: 72, Parameterized impropers: 0. Note that if your system contains torsions of Ryckaert-Bellemans functional form, all of these torsions are processed as propers warnings.warn(msg) /home/ayang41/programs/foyer/foyer/forcefield.py:267: UserWarning: Parameters have not been assigned to all impropers. Total system impropers: 48, Parameterized impropers: 0. Note that if your system contains torsions of Ryckaert-Bellemans functional form, all of these torsions are processed as propers warnings.warn(msg) . CPU times: user 2min 56s, sys: 839 ms, total: 2min 57s Wall time: 2min 56s . &lt;Structure 7663 atoms; 200 residues; 7463 bonds; parametrized&gt; . %%time ff = foyer.forcefields.load_OPLSAA() structure = random_compounds.to_parmed() distributed_foyer_parallel(ff, structure) . CPU times: user 3min 1s, sys: 551 ms, total: 3min 2s Wall time: 3min 1s . &lt;Structure 7663 atoms; 200 residues; 7463 bonds; parametrized&gt; . Making individual structures . Parallelization is fantastically slowing down our operations. I have a hunch this might be due to the extra steps involved in splitting up the molecular graphs. . When molecular modelers make these systems, we already know which collection of atoms and bonds forms a molecule, so we can use that to circumvent any use of connected components. In this iteration, we&#39;ve added a shortcut where we already know the individual structures. . Canonical foyer is still faster. For a parallel library comparison, I tried using multiprocessing but got infinite recursion errors, so multiprocessing was not as easy to use as dask for this particular application . random_compounds = mb.Compound(subcompounds=[Alkane(n=i) for i in np.random.randint(5, high=20, size=200)]) . %%time individual_structures = [cmpd.to_parmed() for cmpd in random_compounds.children] param_structures = [delayed(parametrize)(ff, struc) for struc in individual_structures] computed_parametrized_structures = [d.compute() for d in param_structures] final_structure = reduce(lambda x,y: x+y, computed_parametrized_structures) final_structure . /home/ayang41/programs/mbuild/mbuild/compound.py:2527: UserWarning: No box specified and no Compound.box detected. Using Compound.boundingbox + 0.5 nm buffer. Setting all box angles to 90 degrees. &#34;No box specified and no Compound.box detected. &#34; /home/ayang41/programs/foyer/foyer/forcefield.py:267: UserWarning: Parameters have not been assigned to all impropers. Total system impropers: 76, Parameterized impropers: 0. Note that if your system contains torsions of Ryckaert-Bellemans functional form, all of these torsions are processed as propers warnings.warn(msg) /home/ayang41/programs/foyer/foyer/forcefield.py:267: UserWarning: Parameters have not been assigned to all impropers. Total system impropers: 36, Parameterized impropers: 0. Note that if your system contains torsions of Ryckaert-Bellemans functional form, all of these torsions are processed as propers warnings.warn(msg) /home/ayang41/programs/foyer/foyer/forcefield.py:267: UserWarning: Parameters have not been assigned to all impropers. Total system impropers: 24, Parameterized impropers: 0. Note that if your system contains torsions of Ryckaert-Bellemans functional form, all of these torsions are processed as propers warnings.warn(msg) /home/ayang41/programs/foyer/foyer/forcefield.py:267: UserWarning: Parameters have not been assigned to all impropers. Total system impropers: 72, Parameterized impropers: 0. Note that if your system contains torsions of Ryckaert-Bellemans functional form, all of these torsions are processed as propers warnings.warn(msg) /home/ayang41/programs/foyer/foyer/forcefield.py:267: UserWarning: Parameters have not been assigned to all impropers. Total system impropers: 68, Parameterized impropers: 0. Note that if your system contains torsions of Ryckaert-Bellemans functional form, all of these torsions are processed as propers warnings.warn(msg) /home/ayang41/programs/foyer/foyer/forcefield.py:267: UserWarning: Parameters have not been assigned to all impropers. Total system impropers: 60, Parameterized impropers: 0. Note that if your system contains torsions of Ryckaert-Bellemans functional form, all of these torsions are processed as propers warnings.warn(msg) /home/ayang41/programs/foyer/foyer/forcefield.py:267: UserWarning: Parameters have not been assigned to all impropers. Total system impropers: 32, Parameterized impropers: 0. Note that if your system contains torsions of Ryckaert-Bellemans functional form, all of these torsions are processed as propers warnings.warn(msg) /home/ayang41/programs/foyer/foyer/forcefield.py:267: UserWarning: Parameters have not been assigned to all impropers. Total system impropers: 64, Parameterized impropers: 0. Note that if your system contains torsions of Ryckaert-Bellemans functional form, all of these torsions are processed as propers warnings.warn(msg) /home/ayang41/programs/foyer/foyer/forcefield.py:267: UserWarning: Parameters have not been assigned to all impropers. Total system impropers: 28, Parameterized impropers: 0. Note that if your system contains torsions of Ryckaert-Bellemans functional form, all of these torsions are processed as propers warnings.warn(msg) /home/ayang41/programs/foyer/foyer/forcefield.py:267: UserWarning: Parameters have not been assigned to all impropers. Total system impropers: 56, Parameterized impropers: 0. Note that if your system contains torsions of Ryckaert-Bellemans functional form, all of these torsions are processed as propers warnings.warn(msg) /home/ayang41/programs/foyer/foyer/forcefield.py:267: UserWarning: Parameters have not been assigned to all impropers. Total system impropers: 52, Parameterized impropers: 0. Note that if your system contains torsions of Ryckaert-Bellemans functional form, all of these torsions are processed as propers warnings.warn(msg) /home/ayang41/programs/foyer/foyer/forcefield.py:267: UserWarning: Parameters have not been assigned to all impropers. Total system impropers: 40, Parameterized impropers: 0. Note that if your system contains torsions of Ryckaert-Bellemans functional form, all of these torsions are processed as propers warnings.warn(msg) /home/ayang41/programs/foyer/foyer/forcefield.py:267: UserWarning: Parameters have not been assigned to all impropers. Total system impropers: 48, Parameterized impropers: 0. Note that if your system contains torsions of Ryckaert-Bellemans functional form, all of these torsions are processed as propers warnings.warn(msg) /home/ayang41/programs/foyer/foyer/forcefield.py:267: UserWarning: Parameters have not been assigned to all impropers. Total system impropers: 44, Parameterized impropers: 0. Note that if your system contains torsions of Ryckaert-Bellemans functional form, all of these torsions are processed as propers warnings.warn(msg) . CPU times: user 3min 19s, sys: 1.04 s, total: 3min 20s Wall time: 3min 19s . &lt;Structure 7735 atoms; 200 residues; 7535 bonds; PBC (orthogonal); parametrized&gt; . param_structures[0].visualize(rankdir=&#39;LR&#39;) . %%time one_structure = random_compounds.to_parmed() canonical_foyer(ff, one_structure) . /home/ayang41/programs/mbuild/mbuild/compound.py:2527: UserWarning: No box specified and no Compound.box detected. Using Compound.boundingbox + 0.5 nm buffer. Setting all box angles to 90 degrees. &#34;No box specified and no Compound.box detected. &#34; . CPU times: user 1min 50s, sys: 1.05 s, total: 1min 51s Wall time: 1min 51s . /home/ayang41/programs/foyer/foyer/forcefield.py:267: UserWarning: Parameters have not been assigned to all impropers. Total system impropers: 9780, Parameterized impropers: 0. Note that if your system contains torsions of Ryckaert-Bellemans functional form, all of these torsions are processed as propers warnings.warn(msg) . &lt;Structure 7735 atoms; 1 residues; 7535 bonds; PBC (orthogonal); parametrized&gt; . Lessons and Takeaways . This was a little disheartening, any attempt to distribute foyer atom-typing or combine with dask did NOT accelerate anything. This can probably be explained in a variety of ways: . We had to convert our structure to a graph, run a connected components algorithm (which has its own scaling issues), create separate parmed structures, then re-join/add the individual structures together. Each of those steps is bound to slow things down. Data communication also plays a role here -- communicating the molecular graphs and the entire structure to each dask worker will add some slowness to our pipeline. Doing everything in one foyer function allows the use of caching, which we lose when executing the function lots of different times. Even simplifying the pipeline didn&#39;t show much improvement for the dask implementation . There probably is room for the foyer API to be more accommodating for dask and other parallel computations, but it might require a refactoring effort to properly expose the functions-to-parallelize and utilize data structures/approaches more amenable to parallelization. Breaking up a large chemical system into smaller substructures didn&#39;t seem to help. . In all honesty since most molecular systems usually have less than a dozen different molecular species, just replicated into thousands of molecules, the best bet is to parametrize each molecular species once, then propagate the parameters appropriately, all in the canonical foyer style without any parallelization. The current foyer implementation already has implicit acceleration with caching and networkx may already have some graph optimizations for subgraph isomorphisms, mitigating any need for us to explicitly decompose one big graph into lots of small connected components .",
            "url": "https://ahy3nz.github.io/fastpayges/molecular%20modeling/data%20science/2020/06/21/foyer-dask.html",
            "relUrl": "/molecular%20modeling/data%20science/2020/06/21/foyer-dask.html",
            "date": " ‚Ä¢ Jun 21, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Big data tools for MD simulation analysis",
            "content": "Trajectories are sets of coordinates over time. While the act of gathering data and conducting simulations are exhaustively parallelized, some analysis methods are not. Speaking from experience, parallelizing analysis using Python multiprocessing can get very messy if you don&#39;t have a clear idea of how you want to parallelize the analysis, and how exactly you&#39;re going to code it up. . Here, I&#39;m going to attempt to use some parallel librareis for MD trajectory analysis . Some big data tools . Since grad school, I&#39;ve been exposed to a variety of big data tools (Dask, Spark, Rapids), and it&#39;s been a point of interest to test their utility to molecular simulation. Each tool comes with its own sets of advantages and disadvantages, and I encourage everyone to actively try each to see which is most appropriate for the desired application. . Rapids is very fast, but requires GPUs. Depending on your tech stack and tech constraints, you may or may not have cheap and easy access to sufficient GPUs. Rapids is a little more sensitive to data types than others - but as an amateur, I could be misusing the libraries. | Spark is fast, but requires some hadoop and Spark knowhow to stand up properly. Many tech stacks and constraints seem to be well-suited for spark applications. Spark scales out well, very flexible with datatypes, and eschews a lot of parallel programming-knowhow. At my own work, some primitive tests have shown that spark outperforms dask for dataframe operations on strings and some ML operations - but as an amateur, there is probably some Dask tuning that could be done | Dask is also fast, but your mileage may vary. Some tech stacks are suitable for Dask, but cloud resources/tech constraints might make Dask adoption hard. Dask exposes various levels of parallelism, so proper Dask-users will end up learning a lot about parallel computing along the way. | . I defer to this pydata video for a Dask, Rapids, Spark comparison . For those like me who are not used to setting up parallel compute . The one thing I will observe as I dabble away on my personal computer - I am neither familiar with setting up a Hadoop cluster nor am I familiar with exposing my WSL to my GPU, and single-node pyspark is not going to useful for the overhead. If given the proper infrastructure and resources, I can use these libraries, but at this moment it would take time for me to set up the resources to properly utilizes Spark or Rapids on my PC. Dask, in my case, seems like the simplest parallel compute library to use. If you&#39;re a grad student or a data scientist unfamiliar with software environments and infrastructure beyond Conda environments, Dask might also be easiest for adoption. . Computing atomic distances from a molecular dynamics simulation . Trivial MD analysis involves looking at each atom within a frame, and not having to look at time correlations from frame to frame. I&#39;m going to use MDTraj to load in a trajectory, and look at distances between atoms in each frame. I&#39;ll do this serial, with just MDTraj, and I&#39;ll do this with using one level of Dask parallelism, Dask delayed . import itertools as it from pathlib import Path import numpy as np import mdtraj import dask from dask import delayed import dask.bag as db . Saving myself the effort of generating my own trajectory, I will use one of the trajectories in MDTraj&#39;s unit tests . path_to_data = Path(&#39;/home/ayang41/programs/mdtraj/tests/data&#39;) tip3p_xtc = Path.joinpath(path_to_data/&#39;tip3p_300K_1ATM.xtc&#39;) tip3p_pdb = Path.joinpath(path_to_data/&#39;tip3p_300K_1ATM.pdb&#39;) . This trajectory is only 401 frames - parallel analysis incurs too much overhead to be useful. I&#39;m going to artificially lengthen the trajectory out to 1604 frames, where the gain from parallelization will hopefully be more apparent. In reality, most grad students will have many, many more frames to analyze. . traj = mdtraj.load(tip3p_xtc.as_posix(), top=tip3p_pdb.as_posix()) for i in range(2): traj = traj.join(traj) traj . &lt;mdtraj.Trajectory with 1604 frames, 774 atoms, 258 residues, and unitcells at 0x7f9bf4cce150&gt; . Additionally, to load up the computational expense, I&#39;ll look at all pairwise atomic distances in each frame . atom_pairs = [*it.permutations(np.arange(0, traj.n_atoms),2)] . Simple implementation with MDTraj . On my PC with 6 cores, this took about 23 seconds (and also nearly froze my computer). . It should be noted that MDTraj already does a lot of parallelization and acceleration under their hood with some C optimizations. &quot;Simple&quot; in this case, is a user depending on MDTraj&#39;s optimizations . %%time displacements = mdtraj.compute_displacements(traj, atom_pairs) . CPU times: user 5.94 s, sys: 17.5 s, total: 23.5 s Wall time: 23.7 s . Combining Dask with MDTraj . Like most parallel computing applications, it&#39;s important to recognize how and what you will be parallelizing/distributing. In this case, we will be distributing our one trajectory across 4 partitions, creating Delayed objects. Each Delayed object isn&#39;t an actual execution - it&#39;s a scheduled operation (like queueing something up in SLURM or PBS). . It helps that mdtraj.Trajectory objects are iterable, so we can easily break up the trajectory into 4 even-sized chunks with some python list comprehensions . %%time chunksize = int(traj.n_frames/4) bag = db.from_sequence([traj[chunksize*i: chunksize*(i+1)] for i in range(4)] , npartitions=4) bunch_of_delayed = bag.to_delayed() . CPU times: user 62.5 ms, sys: 172 ms, total: 234 ms Wall time: 293 ms . bag . dask.bag&lt;from_sequence, npartitions=4&gt; . bunch_of_delayed . [Delayed((&#39;from_sequence-b688539387c3c167fe82241b18a1670a&#39;, 0)), Delayed((&#39;from_sequence-b688539387c3c167fe82241b18a1670a&#39;, 1)), Delayed((&#39;from_sequence-b688539387c3c167fe82241b18a1670a&#39;, 2)), Delayed((&#39;from_sequence-b688539387c3c167fe82241b18a1670a&#39;, 3))] . If we wanted to, we can still pluck out and execute the Delayed objects, and parse the number of atoms in MDTraj-like syntax . bunch_of_delayed[0].compute()[0].n_atoms . 774 . We can also validate that each Delayed object is computing a quarter of our trajectory . bunch_of_delayed[0].compute(), bunch_of_delayed[1].compute() . ([&lt;mdtraj.Trajectory with 401 frames, 774 atoms, 258 residues, and unitcells at 0x7fb555e2df50&gt;], [&lt;mdtraj.Trajectory with 401 frames, 774 atoms, 258 residues, and unitcells at 0x7fb2a1a12d10&gt;]) . To queue up additional computations, we will take each Delayed object, and add on one additional operation - mdtraj.compute_displacements. Now the delayed objects have two operations - distributing the trajectory and computing the displacements. It&#39;s worth noting that none of these operations involved rewriting MDTraj code or adding function decorators. These MDTraj functions are wrapped using the Delayed objects . Again, the computation has not been performed yet . %%time all_displacements = [delayed(mdtraj.compute_displacements)(traj[0], atom_pairs) for traj in bunch_of_delayed] all_displacements . CPU times: user 26.5 s, sys: 2.55 s, total: 29 s Wall time: 29.2 s . [Delayed(&#39;compute_displacements-c1ef5c08-6bb2-4508-8f1a-166000d2cd3e&#39;), Delayed(&#39;compute_displacements-5a9fd8cd-2993-4c4b-be90-a2523e47c09a&#39;), Delayed(&#39;compute_displacements-35c48042-fecf-4eb4-adc5-931c097b6e8d&#39;), Delayed(&#39;compute_displacements-d8699960-98e0-4b74-a320-2b2e1f3870a9&#39;)] . If we want to &quot;flush&quot; the queue and run all our Delayed computations, we use Dask to finally compute them. . At this point, the actual calculation took 3min 6s (hey, this is terrible!), but the overhead involved 27 seconds . %%time displacements = dask.compute(all_displacements) . CPU times: user 17.8 s, sys: 27.9 s, total: 45.7 s Wall time: 3min 6s . The returned object is 4 different results, and each result is a numpy array 401 x 598302 x 3 (n_frames x n_atompairs x n_spatialdimensions) . len(displacements[0]) . 4 . displacements[0][1].shape . (401, 598302, 3) . Visualizing the dask graph . Spark and Dask both use task graphs to schedule function after function, with Spark doing some implicit optimizations. . Dask has a nice visualize functionality to show what the task graphs and parallelization look like for two of our Delayed objects . dask.visualize(all_displacements[0:2]) . This Dask parallelization slowed the MDTraj operation down! What gives? . MDTraj is very well-optimized, so any attempts to distribute work end up slowing down the array multiplications . We&#39;ll use our own, crude distance function that has no optimizations (and doesn&#39;t obey the minimum image convention) . def crude_distances(traj, atom_pairs): all_distances = [] for frame in traj: distances =[] for pair in atom_pairs: distance = np.sqrt(np.dot(frame.xyz[0, pair[0], :], frame.xyz[0, pair[1], :])) distances.append(distance) all_distances.append(distances) return np.array(all_distances) . %%time traj = mdtraj.load(tip3p_xtc.as_posix(), top=tip3p_pdb.as_posix()) chunksize = int(traj.n_frames/4) bag = db.from_sequence([traj[chunksize*i: chunksize*(i+1)] for i in range(4)] , npartitions=4) bunch_of_delayed = bag.to_delayed() . CPU times: user 125 ms, sys: 0 ns, total: 125 ms Wall time: 505 ms . atom_pairs = [*it.combinations(np.arange(0,100),2)] . %%time all_displacements = [delayed(crude_distances)(traj[0], atom_pairs) for traj in bunch_of_delayed] all_displacements . CPU times: user 156 ms, sys: 46.9 ms, total: 203 ms Wall time: 169 ms . [Delayed(&#39;crude_distances-fb865e6f-232a-4a24-8a37-0b0f6ce13f22&#39;), Delayed(&#39;crude_distances-438627d2-a181-4127-85a1-1cfbe99f64f6&#39;), Delayed(&#39;crude_distances-543f6412-6dcc-4a30-922b-2f963e978a5d&#39;), Delayed(&#39;crude_distances-78883eb2-520e-4c75-9af9-d06b82b746d1&#39;)] . %%time output = dask.compute(all_displacements) . CPU times: user 54.6 s, sys: 1min, total: 1min 55s Wall time: 1min 7s . %%time output = crude_distances(traj, atom_pairs) . CPU times: user 1min 28s, sys: 1min 40s, total: 3min 8s Wall time: 1min 51s . So there was ~47 second speedup from the crude function - that&#39;s a small win. . And here&#39;s the task graph for one of the Delayed objects . all_displacements[0].visualize() . Aiming for memory-efficiency . Up until now, we&#39;ve had the whole trajectory loaded into memory prior to any parallelization with Dask. We can use MDTraj&#39;s iterload function to reduce the size of the trajectory, but still pass different chunks around. . As another consideration for parallelization, increasing the number of disk reads will slow down your process, so make sure the gain from parallelization makes it worth it . %%time delayed_load = db.from_sequence(a for a in mdtraj.iterload(tip3p_xtc.as_posix(), top=tip3p_pdb.as_posix())).to_delayed() . CPU times: user 172 ms, sys: 172 ms, total: 344 ms Wall time: 312 ms . Confirming that each Delayed object has different frames . delayed_load[0].compute()[0].time, delayed_load[1].compute()[0].time . (array([ 0., 1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11., 12., 13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25., 26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38., 39., 40., 41., 42., 43., 44., 45., 46., 47., 48., 49., 50., 51., 52., 53., 54., 55., 56., 57., 58., 59., 60., 61., 62., 63., 64., 65., 66., 67., 68., 69., 70., 71., 72., 73., 74., 75., 76., 77., 78., 79., 80., 81., 82., 83., 84., 85., 86., 87., 88., 89., 90., 91., 92., 93., 94., 95., 96., 97., 98., 99.], dtype=float32), array([100., 101., 102., 103., 104., 105., 106., 107., 108., 109., 110., 111., 112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122., 123., 124., 125., 126., 127., 128., 129., 130., 131., 132., 133., 134., 135., 136., 137., 138., 139., 140., 141., 142., 143., 144., 145., 146., 147., 148., 149., 150., 151., 152., 153., 154., 155., 156., 157., 158., 159., 160., 161., 162., 163., 164., 165., 166., 167., 168., 169., 170., 171., 172., 173., 174., 175., 176., 177., 178., 179., 180., 181., 182., 183., 184., 185., 186., 187., 188., 189., 190., 191., 192., 193., 194., 195., 196., 197., 198., 199.], dtype=float32)) . %%time all_displacements = [delayed(crude_distances)(traj[0], atom_pairs) for traj in delayed_load] all_displacements . CPU times: user 188 ms, sys: 93.8 ms, total: 281 ms Wall time: 294 ms . [Delayed(&#39;crude_distances-d2f8fad8-663a-41b4-a97c-9277cc086fba&#39;), Delayed(&#39;crude_distances-4a9116d4-96f0-4c35-bca1-0525622976c8&#39;), Delayed(&#39;crude_distances-6aa987e4-0e71-462f-9293-55e6deed1425&#39;), Delayed(&#39;crude_distances-cd230400-cf2f-4ad9-9cc7-ab573848e397&#39;), Delayed(&#39;crude_distances-627969a1-2726-4f7e-87a9-7a97665c46b0&#39;)] . Still ~40 second gain with the crude distance calculation with Dask . %%time out = dask.compute(all_displacements) . CPU times: user 52.1 s, sys: 1min 3s, total: 1min 55s Wall time: 1min 10s . %%time all_displacements = [] for traj in mdtraj.iterload(tip3p_xtc.as_posix(), top=tip3p_pdb.as_posix()): all_displacements.append(crude_distances(traj, atom_pairs)) . CPU times: user 1min 26s, sys: 1min 46s, total: 3min 13s Wall time: 1min 51s . atom_pairs = [*it.combinations(np.arange(0, traj.n_atoms),2)] . delayed_load = db.from_sequence(a for a in mdtraj.iterload(tip3p_xtc.as_posix(), top=tip3p_pdb.as_posix())).to_delayed() . %%time all_displacements = [delayed(mdtraj.compute_displacements)(traj[0], atom_pairs) for traj in delayed_load] . CPU times: user 15.6 s, sys: 688 ms, total: 16.3 s Wall time: 16.4 s . %%time out = dask.compute(all_displacements) . CPU times: user 7.98 s, sys: 938 ms, total: 8.92 s Wall time: 8.92 s . %%time all_displacements = [] for traj in mdtraj.iterload(tip3p_xtc.as_posix(), top=tip3p_pdb.as_posix()): all_displacements.append(mdtraj.compute_displacements(traj, atom_pairs)) . CPU times: user 1.17 s, sys: 1.09 s, total: 2.27 s Wall time: 2.26 s . Trying Dask distributed . We could try another level of parallelism using Dask&#39;s distributed framework on a single node, but there appear to be Dask distributed issues with WSL. . Regardless, we can still see what happens . from distributed import Client client = Client() client . distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 92] Protocol not available distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 92] Protocol not available distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 92] Protocol not available distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 92] Protocol not available distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 92] Protocol not available distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 92] Protocol not available distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 92] Protocol not available distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 92] Protocol not available distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 92] Protocol not available . Client . Scheduler: tcp://127.0.0.1:54022 | Dashboard: http://127.0.0.1:8787/status | . | Cluster . Workers: 3 | Cores: 6 | Memory: 17.11 GB | . | . distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 92] Protocol not available distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 92] Protocol not available distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 92] Protocol not available . With default settings, we&#39;re working with 3 workers across 6 cores. . We can see from the Dask dashboard that there are certainly concurrent operations, but the yellow operation (disk-read-compute_displacements) is adding a lot of overhead beyond that purple operation (the actual compute_displacements) . %%time delayed_load = db.from_sequence(a for a in mdtraj.iterload(tip3p_xtc.as_posix(), top=tip3p_pdb.as_posix())).to_delayed() all_displacements = [delayed(mdtraj.compute_displacements)(traj[0], atom_pairs) for traj in delayed_load] out = dask.compute(all_displacements) . distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 92] Protocol not available distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 92] Protocol not available distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 92] Protocol not available distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 92] Protocol not available distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 92] Protocol not available distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 92] Protocol not available distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 92] Protocol not available distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 92] Protocol not available distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 92] Protocol not available distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 92] Protocol not available distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 92] Protocol not available distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 92] Protocol not available . CPU times: user 37.6 s, sys: 12.4 s, total: 50 s Wall time: 57.6 s . client.close() client = Client(processes=False) client . distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 92] Protocol not available distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 92] Protocol not available . Client . Scheduler: inproc://192.168.0.15/667/12 | Dashboard: http://192.168.0.15:8787/status | . | Cluster . Workers: 1 | Cores: 6 | Memory: 17.11 GB | . | . Running all workers on the same process, there&#39;s still some room for multithreading, but the same slow-downs rear their heads . %%time delayed_load = db.from_sequence(a for a in mdtraj.iterload(tip3p_xtc.as_posix(), top=tip3p_pdb.as_posix())).to_delayed() all_displacements = [delayed(mdtraj.compute_displacements)(traj[0], atom_pairs) for traj in delayed_load] out = dask.compute(all_displacements) . distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%) distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%) distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%) distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%) distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%) distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%) distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%) . CPU times: user 51 s, sys: 1.22 s, total: 52.2 s Wall time: 52.9 s . Takeaways from some Dask tests . The observations here were surprising, but maybe a good lesson before anyone immediately tries to jump into some big data tools . MDTraj is really performant . If you&#39;re able to use MDTraj-optimized functions, use those. If you want to be memory efficient and stream trajectory data, use MDTraj for that; you don&#39;t need to schedule loading different slices of a trajectory with Dask. . An optimized library can beat the bloat of a scheduler . Combining Dask + MDTraj was worse in all cases than just using MDTraj exclusively. Dask&#39;s parallelization didn&#39;t make anything run faster, and Dask&#39;s delayed scheduling didn&#39;t introduce anything better compared to MDTraj&#39;s iterloading. This might be because of multiple reads, communication between workers, or overhead of building out the task scheduler. . If the opportunity, resources, and need exist, optimizing a library can go farther than trying to lump Dask on top of any code. Dask + my-bad-distance-code made things faster than my-bad-distance-code exclusively, but my bad-distance-code was completely devoid of optimization. But throw an optimized library like MDTraj in, and you likely won&#39;t need Dask (or your poorly-written code!). . If you have a particularly unique function you don&#39;t know how to optimize, then it&#39;s time to think about what dask can offer . MDTraj is great because it provides a set of common, optimized functions. For a lot of work in this field, there will be unique analyses that are not common to many MD libraries, and if they are, they may not be optimized. If these two hold true to your particular studies, then your options become . 1) Optimize your analysis code. Simplify routines for time and space complexity, reduce for-loops if you can, reduce the amount of read/write operations, write Cython/C/Cuda/compiled code . 2) Use a parallel/scheduler framework like Dask . If you&#39;re not a (parallel) programming wiz or lack the time to become one, then option 2 may be for you . It doesn&#39;t help that we&#39;re working with different data . A lot of Dask use-cases and API are built around arrays and dataframes, so there&#39;s already a lot of built-in optimization for those data structures. There may be room to build a Dask-trajectory object that creates room for computational optimization (rather than stringing together a bunch of non-dask operations) that might be able to beat MDTraj .",
            "url": "https://ahy3nz.github.io/fastpayges/molecular%20modeling/data%20science/2020/05/13/dask-mdtraj.html",
            "relUrl": "/molecular%20modeling/data%20science/2020/05/13/dask-mdtraj.html",
            "date": " ‚Ä¢ May 13, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Digging through some Folding@Home data",
            "content": "Learning cheminformatics from some Folding@Home data . 2020-05-06 - 2020-05-11 . I have no formal training in cheminformatics, so I am going to be stumbling and learning as I wade through this dataset. I welcome any learning lessons from experts. . This will be an ongoing foray . Source: https://github.com/FoldingAtHome/covid-moonshot . Introduction . Folding@Home is a distributed computing project - allowing molecular simulations to be run in parallel across thousands of different computers with minimal communication. This, combined with other molecular modeling methods, has yielded a lot of open data for others to examine. In particular, I&#39;m interested in the docking screens and compounds targeted by the F@H and postera collaborations . import pandas as pd import numpy as np import matplotlib import matplotlib.pyplot as plt pd.options.display.max_columns = 999 moonshot_df = pd.read_csv(&#39;moonshot-submissions/covid_submissions_all_info.csv&#39;) . moonshot_df.head() . SMILES CID creator fragments link real_space SCR BB extended_real_space in_molport_or_mcule in_ultimate_mcule in_emolecules covalent_frag covalent_warhead acrylamide acrylamide_adduct chloroacetamide chloroacetamide_adduct vinylsulfonamide vinylsulfonamide_adduct nitrile nitrile_adduct MW cLogP HBD HBA TPSA num_criterion_violations BMS Dundee Glaxo Inpharmatica LINT MLSMR PAINS SureChEMBL PostEra ORDERED MADE ASSAYED . 0 CCN(Cc1cccc(-c2ccncc2)c1)C(=O)Cn1nnc2ccccc21 | AAR-POS-8a4e0f60-1 | Aaron Morris, PostEra | x0072 | https://covid.postera.ai/covid/submissions/AAR... | Z1260533612 | FALSE | FALSE | FALSE | False | False | False | False | False | False | False | False | False | False | False | False | False | 371.444 | 3.5420 | 0 | 5 | 63.91 | 0 | PASS | PASS | PASS | PASS | PASS | PASS | PASS | PASS | PASS | True | False | False | . 1 O=C(Cn1nnc2ccccc21)NCc1ccc(Oc2cccnc2)c(F)c1 | AAR-POS-8a4e0f60-10 | Aaron Morris, PostEra | x0072 | https://covid.postera.ai/covid/submissions/AAR... | Z826180044 | FALSE | FALSE | s_22____1723102____13206668 | False | False | False | False | False | False | False | False | False | False | False | False | False | 377.379 | 3.0741 | 1 | 6 | 81.93 | 0 | PASS | PASS | PASS | PASS | PASS | PASS | PASS | PASS | PASS | True | False | False | . 2 CN(Cc1nnc2ccccn12)C(=O)N(Cc1cccs1)c1ccc(Br)cc1 | AAR-POS-8a4e0f60-11 | Aaron Morris, PostEra | x0072 | https://covid.postera.ai/covid/submissions/AAR... | FALSE | FALSE | FALSE | FALSE | False | False | False | False | False | False | False | False | False | False | False | False | False | 456.369 | 4.8119 | 0 | 5 | 53.74 | 0 | PASS | PASS | PASS | Filter9_metal | aryl bromide | PASS | PASS | PASS | PASS | True | False | False | . 3 CCN(Cc1cccc(-c2ccncc2)c1)C(=O)Cc1noc2ccccc12 | AAR-POS-8a4e0f60-2 | Aaron Morris, PostEra | x0072 | https://covid.postera.ai/covid/submissions/AAR... | Z1260535907 | FALSE | FALSE | FALSE | False | False | False | False | False | False | False | False | False | False | False | False | False | 371.440 | 4.4810 | 0 | 4 | 59.23 | 0 | PASS | PASS | PASS | PASS | PASS | PASS | PASS | PASS | PASS | True | False | False | . 4 O=C(NCc1noc2ccccc12)N(Cc1cccs1)c1ccc(F)cc1 | AAR-POS-8a4e0f60-3 | Aaron Morris, PostEra | x0072 | https://covid.postera.ai/covid/submissions/AAR... | FALSE | FALSE | FALSE | s_272164____9388766____17338746 | False | False | False | False | False | False | False | False | False | False | False | False | False | 381.432 | 4.9448 | 1 | 4 | 58.37 | 0 | PASS | PASS | PASS | PASS | PASS | PASS | PASS | PASS | PASS | True | False | False | . The moonshot data has a lot of logging/metadata information, some one-hot-encoding information about functional groups, and some additional columns about Glaxo, Dundee, BMS, Lint, PAINS, SureChEMBL - I&#39;m not sure what those additional coluns mean, but the values are binary values, possibly the results of some other test or availability in another databases. . I&#39;m going to focus on the molecular properties: MW, cLogP, HBD, HBA, TPSA . MW: Molecular Weight | cLogP: The logarithm of the partition coefficient (ratio of concentrations in octanol vs water, $ log{ frac{c_{octanol}}{c_{water}}}$) | HBD: Hydrogen bond donors | HBA: Hydrogen bond acceptors | TPSA: Topological polar surface area | . Some of the correlations make some chemical sense - heavier molecules have more heavy atoms (O, N, F, etc.), but these heavier atoms are also the hydrogen bond acceptors. By that logic, more heavy atoms also coincides with more electronegative atoms, increasing your TPSA. It&#39;s a little convoluted because TPSA looks at the surface, not necessarily the volume of the compound; geometry/shape will influence TPSA. There don&#39;t appear to be any strong correlations with cLogP. Partition coefficients are a complex function of polarity, size/sterics, and shape - a 1:1 correlation with a singular, other variable will be hard to pinpoint . This csv file doesn&#39;t have much other numerical data, but maybe some of those true/false, pass/fail data might be relevant...but I definitely need more context here . fig, ax = plt.subplots(1,1, figsize=(8,6), dpi=100) cols = [&#39;MW&#39;, &#39;cLogP&#39;, &#39;HBD&#39;, &#39;HBA&#39;, &#39;TPSA&#39;] ax.matshow(moonshot_df[cols].corr(), cmap=&#39;RdBu&#39;) ax.set_xticks([i for i,_ in enumerate(cols)]) ax.set_xticklabels(cols) ax.set_yticks([i for i,_ in enumerate(cols)]) ax.set_yticklabels(cols) for i, (rowname, row) in enumerate(moonshot_df[cols].corr().iterrows()): for j, (key, val) in enumerate(row.iteritems()): ax.annotate(f&quot;{val:0.2f}&quot;, xy=(i,j), xytext=(-10, -5), textcoords=&quot;offset points&quot;) . Some docking results . Okay here&#39;s a couple other CSVs I found, these include some docking scores . Repurposing scores: &quot;The Drug Repurposing Hub is a curated and annotated collection of FDA-approved drugs, clinical trial drugs, and pre-clinical tool compounds with a companion information resource&quot; source here, so a public dataset of some drugs | Redock scores: &quot;This directory contains experiments in redocking all screened fragments into the entire ensemble of X-ray structures.&quot; Taking fragments and re-docking them | . repurposing_df = pd.read_csv(&#39;repurposing-screen/drugset-docked.csv&#39;) redock_df = pd.read_csv(&#39;redock-fragments/all-screened-fragments-docked.csv&#39;) . SMILES strings, names, docking scores . repurposing_df.head() . SMILES TITLE Hybrid2 docked_fragment Mpro-_dock site . 0 C[C@@H](c1ccc-2c(c1)Cc3c2cccc3)C(=O)[O-] | CHEMBL2104122 | -11.519580 | x0749 | 0.509349 | active-covalent | . 1 C[C@]12CC[C@H]3[C@H]([C@@H]1CC[C@]2(C#C)O)CCC4... | CHEMBL1387 | -10.580162 | x0749 | 2.706928 | active-covalent | . 2 CC(C)(C)c1cc(cc(c1O)C(C)(C)C)/C=C 2/C(=O)NC(=[... | CHEMBL275835 | -10.557229 | x0107 | 1.801830 | active-noncovalent | . 3 C[C@]12CC[C@@H]3[C@H]4CCCCC4=CC[C@H]3[C@@H]1CC... | CHEMBL2104104 | -10.480992 | x0749 | 3.791700 | active-covalent | . 4 CC(=O)[C@]1(CC[C@@H]2[C@@]1(CCC3=C4CCC(=O)C=C4... | CHEMBL2104231 | -10.430775 | x0749 | 4.230903 | active-covalent | . Hybrid2 looks like a docking method provided via OpenEye. Mpro likely refers to COVID-19 main protease. I&#39;m not entirely sure what the receptor for &quot;Hybrid2&quot; is, but there seem to be multiple &quot;sites&quot; or &quot;fragments&quot; for docking. There are lots of different fragments, but very few sites. For each site-fragment combination, multiple small molecules may have been tested. . repurposing_df[&#39;docked_fragment&#39;].value_counts() . x0195 114 x0749 69 x0678 58 x0397 45 x0104 24 x0161 21 x1077 19 x0072 14 x0874 13 x0354 13 x0689 10 x1382 7 x0708 4 x0434 4 x1093 3 x1392 2 x0395 2 x1402 2 x0831 2 x0107 2 x1385 2 x1418 2 x0387 2 x0830 2 x1478 1 x0786 1 x1187 1 x0692 1 x0967 1 x0426 1 x0305 1 x0946 1 x1386 1 x0759 1 Name: docked_fragment, dtype: int64 . repurposing_df[&#39;site&#39;].value_counts() . active-noncovalent 338 active-covalent 107 dimer-interface 1 Name: site, dtype: int64 . repurposing_df.groupby([&quot;docked_fragment&quot;, &quot;site&quot;]).count() . SMILES TITLE Hybrid2 Mpro-_dock . docked_fragment site . x0072 active-noncovalent 14 | 14 | 14 | 14 | . x0104 active-noncovalent 24 | 24 | 24 | 24 | . x0107 active-noncovalent 2 | 2 | 2 | 2 | . x0161 active-noncovalent 21 | 21 | 21 | 21 | . x0195 active-noncovalent 114 | 114 | 114 | 114 | . x0305 active-noncovalent 1 | 1 | 1 | 1 | . x0354 active-noncovalent 13 | 13 | 13 | 13 | . x0387 active-noncovalent 2 | 2 | 2 | 2 | . x0395 active-noncovalent 2 | 2 | 2 | 2 | . x0397 active-noncovalent 45 | 45 | 45 | 45 | . x0426 active-noncovalent 1 | 1 | 1 | 1 | . x0434 active-noncovalent 4 | 4 | 4 | 4 | . x0678 active-noncovalent 58 | 58 | 58 | 58 | . x0689 active-covalent 10 | 10 | 10 | 10 | . x0692 active-covalent 1 | 1 | 1 | 1 | . x0708 active-covalent 4 | 4 | 4 | 4 | . x0749 active-covalent 69 | 69 | 69 | 69 | . x0759 active-covalent 1 | 1 | 1 | 1 | . x0786 active-covalent 1 | 1 | 1 | 1 | . x0830 active-covalent 2 | 2 | 2 | 2 | . x0831 active-covalent 2 | 2 | 2 | 2 | . x0874 active-noncovalent 13 | 13 | 13 | 13 | . x0946 active-noncovalent 1 | 1 | 1 | 1 | . x0967 active-noncovalent 1 | 1 | 1 | 1 | . x1077 active-noncovalent 19 | 19 | 19 | 19 | . x1093 active-noncovalent 3 | 3 | 3 | 3 | . x1187 dimer-interface 1 | 1 | 1 | 1 | . x1382 active-covalent 7 | 7 | 7 | 7 | . x1385 active-covalent 2 | 2 | 2 | 2 | . x1386 active-covalent 1 | 1 | 1 | 1 | . x1392 active-covalent 2 | 2 | 2 | 2 | . x1402 active-covalent 2 | 2 | 2 | 2 | . x1418 active-covalent 2 | 2 | 2 | 2 | . x1478 active-covalent 1 | 1 | 1 | 1 | . Some molecules show up multiple times - why? Upon further investigation, this is mainly due to the molecule&#39;s presence in multiple databases . repurposing_df.groupby([&#39;SMILES&#39;]).count().sort_values(&quot;TITLE&quot;) . TITLE Hybrid2 docked_fragment Mpro-_dock site . SMILES . B(CCCC)(O)O 1 | 1 | 1 | 1 | 1 | . CCCc1ccccc1N 1 | 1 | 1 | 1 | 1 | . CCCc1cc(=O)[nH]c(=S)[nH]1 1 | 1 | 1 | 1 | 1 | . CCC[N@@H+]1CCO[C@H]2[C@H]1CCc3c2cc(cc3)O 1 | 1 | 1 | 1 | 1 | . CCC[N@@H+]1CCC[C@H]2[C@H]1Cc3c[nH]nc3C2 1 | 1 | 1 | 1 | 1 | . ... ... | ... | ... | ... | ... | . C[C@]12CC[C@H]3[C@H]([C@@H]1CC[C@]2(C#C)O)CCC4=CC(=O)CC[C@H]34 2 | 2 | 2 | 2 | 2 | . C[C@]12CC[C@H]3[C@H]([C@@H]1CCC2=O)CC(=C)C4=CC(=O)C=C[C@]34C 2 | 2 | 2 | 2 | 2 | . CC(C)C[C@@H](C1(CCC1)c2ccc(cc2)Cl)[NH+](C)C 2 | 2 | 2 | 2 | 2 | . CC[C@](/C=C/Cl)(C#C)O 2 | 2 | 2 | 2 | 2 | . CC[C@]12CC[C@H]3[C@H]([C@@H]1CC[C@@]2(C#C)O)CCC4=CC(=O)CC[C@H]34 2 | 2 | 2 | 2 | 2 | . 432 rows √ó 5 columns . repurposing_df[repurposing_df[&#39;SMILES&#39;]==&quot;CC[C@]12CC[C@H]3[C@H]([C@@H]1CC[C@@]2(C#C)O)CCC4=CC(=O)CC[C@H]34&quot;] . SMILES TITLE Hybrid2 docked_fragment Mpro-_dock site . 82 CC[C@]12CC[C@H]3[C@H]([C@@H]1CC[C@@]2(C#C)O)CC... | CHEMBL2107797 | -9.002963 | x0749 | 2.616094 | active-covalent | . 105 CC[C@]12CC[C@H]3[C@H]([C@@H]1CC[C@@]2(C#C)O)CC... | EDRUG178 | -8.705896 | x0104 | 2.248707 | active-noncovalent | . There doesn&#39;t seem to be a very good correlation between the two docking scores - if these are docking scores to different receptors, that would help explain things. It&#39;s worth noting that we&#39;re not seeing if the two numbers agree for each molecule, but if the trends persist (both scores go up for this molecule, but go down for this other molecule). The weak correlation suggests the trends do not persist between the two docking measures . repurposing_df[[&#39;Hybrid2&#39;, &#39;Mpro-_dock&#39;]].corr() . Hybrid2 Mpro-_dock . Hybrid2 1.000000 | 0.581966 | . Mpro-_dock 0.581966 | 1.000000 | . Redocking dataframe: SMILES, names, data collection information, docking scores . redock_df.head() . SMILES TITLE fragments CompoundCode Unnamed: 4 covalent_warhead MountingResult DataCollectionOutcome DataProcessingResolutionHigh RefinementOutcome Deposition_PDB_ID Hybrid2 docked_fragment Mpro-x0500_dock site . 0 c1ccc(c(c1)NCc2ccn[nH]2)F | x0500 | x0500 | Z1545196403 | NaN | False | OK: No comment:No comment | success | 2.19 | 7 - Analysed &amp; Rejected | NaN | -11.881923 | x0678 | -2.501554 | active-noncovalent | . 1 Cc1ccccc1OCC(=O)Nc2ncccn2 | x0415 | x0415 | Z53834613 | NaN | False | OK: No comment:No comment | success | 1.62 | 7 - Analysed &amp; Rejected | NaN | -11.622278 | x0678 | NaN | active-noncovalent | . 2 Cc1csc(n1)CNC(=O)c2ccn[nH]2 | x0356 | x0356 | Z466628048 | NaN | False | OK: No comment:No comment | success | 3.25 | 7 - Analysed &amp; Rejected | NaN | -11.435024 | x0678 | NaN | active-noncovalent | . 3 Cc1csc(n1)CNC(=O)c2ccn[nH]2 | x1113 | x1113 | Z466628048 | NaN | False | OK: No comment:No comment | success | 1.57 | 7 - Analysed &amp; Rejected | NaN | -11.435024 | x0678 | NaN | active-noncovalent | . 4 c1cc(cnc1)NC(=O)CC2CCCCC2 | x0678 | x0678 | Z31792168 | NaN | False | Mounted_Clear | success | 1.83 | 6 - Deposited | 5R84 | -11.355046 | x0678 | NaN | active-noncovalent | . There don&#39;t seem to be many Mpro docking scores in this dataset (only one molecule has a non-null Mpro docking score) . redock_df[redock_df[&#39;Mpro-x0500_dock&#39;].isnull()].count() . SMILES 1452 TITLE 1452 fragments 1452 CompoundCode 1452 Unnamed: 4 0 covalent_warhead 1452 MountingResult 1452 DataCollectionOutcome 1452 DataProcessingResolutionHigh 1357 RefinementOutcome 1306 Deposition_PDB_ID 78 Hybrid2 1452 docked_fragment 1452 Mpro-x0500_dock 0 site 1452 dtype: int64 . redock_df[~redock_df[&#39;Mpro-x0500_dock&#39;].isnull()].count() . SMILES 1 TITLE 1 fragments 1 CompoundCode 1 Unnamed: 4 0 covalent_warhead 1 MountingResult 1 DataCollectionOutcome 1 DataProcessingResolutionHigh 1 RefinementOutcome 1 Deposition_PDB_ID 0 Hybrid2 1 docked_fragment 1 Mpro-x0500_dock 1 site 1 dtype: int64 . Are there overlaps in the molecules in each of these datasets? . repurpose_redock = repurposing_df.merge(redock_df, on=&#39;SMILES&#39;, how=&#39;inner&#39;,suffixes=(&quot;_L&quot;, &quot;_R&quot;)) . moonshot_redock = moonshot_df.merge(redock_df, on=&#39;SMILES&#39;, how=&#39;inner&#39;,suffixes=(&quot;_L&quot;, &quot;_R&quot;)) . repurpose_redock . SMILES TITLE_L Hybrid2_L docked_fragment_L Mpro-_dock site_L TITLE_R fragments CompoundCode Unnamed: 4 covalent_warhead MountingResult DataCollectionOutcome DataProcessingResolutionHigh RefinementOutcome Deposition_PDB_ID Hybrid2_R docked_fragment_R Mpro-x0500_dock site_R . 0 Cc1cc(=O)n([nH]1)c2ccccc2 | CHEMBL290916 | -7.889587 | x0195 | -2.068452 | active-noncovalent | x0297 | x0297 | Z50145861 | NaN | False | OK: No comment:No comment | success | 1.98 | 7 - Analysed &amp; Rejected | NaN | -7.889587 | x0195 | NaN | active-noncovalent | . 1 CC(C)Nc1ncccn1 | CHEMBL1740513 | -7.178702 | x0072 | -1.248482 | active-noncovalent | x0583 | x0583 | Z31190928 | NaN | False | OK: No comment:No comment | success | 3.08 | 7 - Analysed &amp; Rejected | NaN | -7.293537 | x1093 | NaN | active-noncovalent | . 2 CC(C)Nc1ncccn1 | CHEMBL1740513 | -7.178702 | x0072 | -1.248482 | active-noncovalent | x1102 | x1102 | Z31190928 | NaN | False | OK: No comment:No comment | success | 1.46 | 7 - Analysed &amp; Rejected | NaN | -7.293537 | x1093 | NaN | active-noncovalent | . 3 C[C@H](C(=O)[O-])O | CHEMBL1200559 | -5.675188 | x0397 | -0.179049 | active-noncovalent | x1035 | x1035 | Z1741982441 | NaN | False | OK: No comment:No comment | Failed - no diffraction | NaN | NaN | NaN | -6.505556 | x0397 | NaN | active-noncovalent | . 4 CC(=O)C(=O)[O-] | DB00119 | -5.448891 | x0689 | -0.494791 | active-covalent | x1037 | x1037 | Z1741977082 | NaN | False | OK: No comment:No comment | Failed - no diffraction | NaN | NaN | NaN | -5.448891 | x0689 | NaN | active-covalent | . 5 CCC(=O)[O-] | CHEMBL14021 | -5.374838 | x0397 | -0.555688 | active-noncovalent | x1029 | x1029 | Z955123616 | NaN | False | OK: No comment:No comment | success | 1.73 | 7 - Analysed &amp; Rejected | NaN | -5.135675 | x0689 | NaN | active-covalent | . 6 C1CNCC[NH2+]1 | CHEMBL1412 | -5.079155 | x0354 | 1.716032 | active-noncovalent | x0996 | x0996 | Z1245537944 | NaN | False | OK: No comment:No comment | success | 1.96 | 7 - Analysed &amp; Rejected | NaN | -4.675085 | x0354 | NaN | active-noncovalent | . We joined on SMILES string, and now we can compare the docking scores between the repurposing and redocking datasets. . Some Hybrid2 scores look quantitatively similar, but for those that don&#39;t, the ranking is still there. Looking at the COVID-19 main protease (Mpro I believe?), the docking scores don&#39;t follow similar rankings - docking scores aren&#39;t transferable to different receptors (this might be a fairly obvious observation) . repurpose_redock[[&#39;SMILES&#39;, &quot;TITLE_L&quot;, &quot;TITLE_R&quot;, &quot;Hybrid2_L&quot;, &quot;Hybrid2_R&quot;, &#39;Mpro-_dock&#39;, &#39;Mpro-x0500_dock&#39;]] . SMILES TITLE_L TITLE_R Hybrid2_L Hybrid2_R Mpro-_dock Mpro-x0500_dock . 0 Cc1cc(=O)n([nH]1)c2ccccc2 | CHEMBL290916 | x0297 | -7.889587 | -7.889587 | -2.068452 | NaN | . 1 CC(C)Nc1ncccn1 | CHEMBL1740513 | x0583 | -7.178702 | -7.293537 | -1.248482 | NaN | . 2 CC(C)Nc1ncccn1 | CHEMBL1740513 | x1102 | -7.178702 | -7.293537 | -1.248482 | NaN | . 3 C[C@H](C(=O)[O-])O | CHEMBL1200559 | x1035 | -5.675188 | -6.505556 | -0.179049 | NaN | . 4 CC(=O)C(=O)[O-] | DB00119 | x1037 | -5.448891 | -5.448891 | -0.494791 | NaN | . 5 CCC(=O)[O-] | CHEMBL14021 | x1029 | -5.374838 | -5.135675 | -0.555688 | NaN | . 6 C1CNCC[NH2+]1 | CHEMBL1412 | x0996 | -5.079155 | -4.675085 | 1.716032 | NaN | . Joining the moonshot submission and redocking datasets does not yield too many overlapping molecules . moonshot_redock . SMILES CID creator fragments_L link real_space SCR BB extended_real_space in_molport_or_mcule in_ultimate_mcule in_emolecules covalent_frag covalent_warhead_L acrylamide acrylamide_adduct chloroacetamide chloroacetamide_adduct vinylsulfonamide vinylsulfonamide_adduct nitrile nitrile_adduct MW cLogP HBD HBA TPSA num_criterion_violations BMS Dundee Glaxo Inpharmatica LINT MLSMR PAINS SureChEMBL PostEra ORDERED MADE ASSAYED TITLE fragments_R CompoundCode Unnamed: 4 covalent_warhead_R MountingResult DataCollectionOutcome DataProcessingResolutionHigh RefinementOutcome Deposition_PDB_ID Hybrid2 docked_fragment Mpro-x0500_dock site . 0 CC(C)Nc1cccnc1 | MAK-UNK-2c1752f0-4 | Maksym Voznyy | x1093 | https://covid.postera.ai/covid/submissions/MAK... | FALSE | Z2574930241 | EN300-56005 | FALSE | False | False | False | False | False | False | False | False | False | False | False | False | False | 136.198 | 1.9019 | 1 | 2 | 24.92 | 0 | PASS | PASS | PASS | PASS | PASS | PASS | PASS | PASS | PASS | False | False | False | x1098 | x1098 | Z1259341037 | NaN | False | OK: No comment:No comment | success | 1.66 | 7 - Analysed &amp; Rejected | NaN | -7.474369 | x0678 | NaN | active-noncovalent | . 1 CC(C)Nc1cccnc1 | MAK-UNK-2c1752f0-4 | Maksym Voznyy | x1093 | https://covid.postera.ai/covid/submissions/MAK... | FALSE | Z2574930241 | EN300-56005 | FALSE | False | False | False | False | False | False | False | False | False | False | False | False | False | 136.198 | 1.9019 | 1 | 2 | 24.92 | 0 | PASS | PASS | PASS | PASS | PASS | PASS | PASS | PASS | PASS | False | False | False | x0572 | x0572 | Z1259341037 | NaN | False | OK: No comment:No comment | success | 2.98 | 7 - Analysed &amp; Rejected | NaN | -7.474369 | x0678 | NaN | active-noncovalent | . 2 CCS(=O)(=O)Nc1ccccc1F | MAK-UNK-2c1752f0-5 | Maksym Voznyy | x1093 | https://covid.postera.ai/covid/submissions/MAK... | FALSE | Z53825177 | EN300-116204 | FALSE | False | True | False | False | False | False | False | False | False | False | False | False | False | 203.238 | 1.5873 | 1 | 2 | 46.17 | 0 | PASS | PASS | PASS | PASS | PASS | Hetero_hetero | PASS | PASS | PASS | False | False | False | x0247 | x0247 | Z53825177 | NaN | False | OK: No comment:No comment | success | 1.83 | 7 - Analysed &amp; Rejected | NaN | -7.413380 | x0678 | NaN | active-noncovalent | . Comparing other databases . CHEMBL, DrugBank, and &quot;EDrug&quot;(?) look to be the 3 prefixes in the &quot;TITLE&quot; column . from chembl_webresource_client.new_client import new_client molecule = new_client.molecule res = molecule.search(&#39;CHEMBL1387&#39;) . res_df = pd.DataFrame.from_dict(res) . res_df.columns . Index([&#39;atc_classifications&#39;, &#39;availability_type&#39;, &#39;biotherapeutic&#39;, &#39;black_box_warning&#39;, &#39;chebi_par_id&#39;, &#39;chirality&#39;, &#39;cross_references&#39;, &#39;dosed_ingredient&#39;, &#39;first_approval&#39;, &#39;first_in_class&#39;, &#39;helm_notation&#39;, &#39;indication_class&#39;, &#39;inorganic_flag&#39;, &#39;max_phase&#39;, &#39;molecule_chembl_id&#39;, &#39;molecule_hierarchy&#39;, &#39;molecule_properties&#39;, &#39;molecule_structures&#39;, &#39;molecule_synonyms&#39;, &#39;molecule_type&#39;, &#39;natural_product&#39;, &#39;oral&#39;, &#39;parenteral&#39;, &#39;polymer_flag&#39;, &#39;pref_name&#39;, &#39;prodrug&#39;, &#39;score&#39;, &#39;structure_type&#39;, &#39;therapeutic_flag&#39;, &#39;topical&#39;, &#39;usan_stem&#39;, &#39;usan_stem_definition&#39;, &#39;usan_substem&#39;, &#39;usan_year&#39;, &#39;withdrawn_class&#39;, &#39;withdrawn_country&#39;, &#39;withdrawn_flag&#39;, &#39;withdrawn_reason&#39;, &#39;withdrawn_year&#39;], dtype=&#39;object&#39;) . res_df[[&#39;chirality&#39;, &#39;molecule_properties&#39;, &#39;molecule_structures&#39;, &#39;score&#39;]] . chirality molecule_properties molecule_structures score . 0 1 | {&#39;alogp&#39;: &#39;3.64&#39;, &#39;aromatic_rings&#39;: 0, &#39;cx_log... | {&#39;canonical_smiles&#39;: &#39;C#C[C@]1(O)CC[C@H]2[C@@H... | 17.0 | . res_df[[&#39;molecule_properties&#39;]].values[0] . array([{&#39;alogp&#39;: &#39;3.64&#39;, &#39;aromatic_rings&#39;: 0, &#39;cx_logd&#39;: &#39;2.81&#39;, &#39;cx_logp&#39;: &#39;2.81&#39;, &#39;cx_most_apka&#39;: None, &#39;cx_most_bpka&#39;: None, &#39;full_molformula&#39;: &#39;C20H26O2&#39;, &#39;full_mwt&#39;: &#39;298.43&#39;, &#39;hba&#39;: 2, &#39;hba_lipinski&#39;: 2, &#39;hbd&#39;: 1, &#39;hbd_lipinski&#39;: 1, &#39;heavy_atoms&#39;: 22, &#39;molecular_species&#39;: None, &#39;mw_freebase&#39;: &#39;298.43&#39;, &#39;mw_monoisotopic&#39;: &#39;298.1933&#39;, &#39;num_lipinski_ro5_violations&#39;: 0, &#39;num_ro5_violations&#39;: 0, &#39;psa&#39;: &#39;37.30&#39;, &#39;qed_weighted&#39;: &#39;0.55&#39;, &#39;ro3_pass&#39;: &#39;N&#39;, &#39;rtb&#39;: 0}], dtype=object) . res_df[&#39;molecule_properties&#39;].apply(pd.Series) . alogp aromatic_rings cx_logd cx_logp cx_most_apka cx_most_bpka full_molformula full_mwt hba hba_lipinski hbd hbd_lipinski heavy_atoms molecular_species mw_freebase mw_monoisotopic num_lipinski_ro5_violations num_ro5_violations psa qed_weighted ro3_pass rtb . 0 3.64 | 0 | 2.81 | 2.81 | None | None | C20H26O2 | 298.43 | 2 | 2 | 1 | 1 | 22 | None | 298.43 | 298.1933 | 0 | 0 | 37.30 | 0.55 | N | 0 | . all_results = [molecule.search(a) for a in repurposing_df[&#39;TITLE&#39;]] . Here&#39;s a big Python function tangent. . For each chembl molecule, we&#39;ve searched for it within the chembl, returning us a list (of length 1) containing a dictionary of properties. . All molecules have been compiled into a list, so we have a list of lists of dicionatires. . For sanity, we can use a Python filter to only retain the non-None results. . We can chain that with a Python map function to parse the first item from each molecule&#39;s list. Recall, each molecule was a list with just one element, a dictionary. We can boil this down to only returning the dictionary (eliminating the list wrapper). . For validation, I&#39;ve called next to look at the results . filtered = map(lambda x: x[0], filter(lambda x: x is not None, all_results)) . next(filtered) . {&#39;atc_classifications&#39;: [], &#39;availability_type&#39;: -1, &#39;biotherapeutic&#39;: None, &#39;black_box_warning&#39;: 0, &#39;chebi_par_id&#39;: None, &#39;chirality&#39;: 0, &#39;cross_references&#39;: [], &#39;dosed_ingredient&#39;: False, &#39;first_approval&#39;: None, &#39;first_in_class&#39;: 0, &#39;helm_notation&#39;: None, &#39;indication_class&#39;: &#39;Anti-Inflammatory&#39;, &#39;inorganic_flag&#39;: 0, &#39;max_phase&#39;: 0, &#39;molecule_chembl_id&#39;: &#39;CHEMBL2104122&#39;, &#39;molecule_hierarchy&#39;: {&#39;molecule_chembl_id&#39;: &#39;CHEMBL2104122&#39;, &#39;parent_chembl_id&#39;: &#39;CHEMBL2104122&#39;}, &#39;molecule_properties&#39;: {&#39;alogp&#39;: &#39;3.45&#39;, &#39;aromatic_rings&#39;: 2, &#39;cx_logd&#39;: &#39;1.26&#39;, &#39;cx_logp&#39;: &#39;3.92&#39;, &#39;cx_most_apka&#39;: &#39;4.68&#39;, &#39;cx_most_bpka&#39;: None, &#39;full_molformula&#39;: &#39;C16H14O2&#39;, &#39;full_mwt&#39;: &#39;238.29&#39;, &#39;hba&#39;: 1, &#39;hba_lipinski&#39;: 2, &#39;hbd&#39;: 1, &#39;hbd_lipinski&#39;: 1, &#39;heavy_atoms&#39;: 18, &#39;molecular_species&#39;: &#39;ACID&#39;, &#39;mw_freebase&#39;: &#39;238.29&#39;, &#39;mw_monoisotopic&#39;: &#39;238.0994&#39;, &#39;num_lipinski_ro5_violations&#39;: 0, &#39;num_ro5_violations&#39;: 0, &#39;psa&#39;: &#39;37.30&#39;, &#39;qed_weighted&#39;: &#39;0.74&#39;, &#39;ro3_pass&#39;: &#39;N&#39;, &#39;rtb&#39;: 2}, &#39;molecule_structures&#39;: {&#39;canonical_smiles&#39;: &#39;CC(C(=O)O)c1ccc2c(c1)Cc1ccccc1-2&#39;, &#39;molfile&#39;: &#39; n RDKit 2D n n 18 20 0 0 0 0 0 0 0 0999 V2000 n -0.5375 0.0250 0.0000 C 0 0 0 0 0 0 0 0 0 0 0 0 n -0.5375 1.1083 0.0000 C 0 0 0 0 0 0 0 0 0 0 0 0 n -2.4458 1.1083 0.0000 C 0 0 0 0 0 0 0 0 0 0 0 0 n -2.4458 0.0250 0.0000 C 0 0 0 0 0 0 0 0 0 0 0 0 n 1.3625 0.0250 0.0000 C 0 0 0 0 0 0 0 0 0 0 0 0 n -1.4875 -0.5125 0.0000 C 0 0 0 0 0 0 0 0 0 0 0 0 n 0.4125 -0.5125 0.0000 C 0 0 0 0 0 0 0 0 0 0 0 0 n 3.3292 0.0250 0.0000 C 0 0 0 0 0 0 0 0 0 0 0 0 n 0.4125 1.6500 0.0000 C 0 0 0 0 0 0 0 0 0 0 0 0 n 2.3417 -0.5292 0.0000 C 0 0 0 0 0 0 0 0 0 0 0 0 n 1.3625 1.1083 0.0000 C 0 0 0 0 0 0 0 0 0 0 0 0 n 3.3500 1.1958 0.0000 O 0 0 0 0 0 0 0 0 0 0 0 0 n 4.2167 -0.6292 0.0000 O 0 0 0 0 0 0 0 0 0 0 0 0 n -3.3958 1.6500 0.0000 C 0 0 0 0 0 0 0 0 0 0 0 0 n -3.3958 -0.5125 0.0000 C 0 0 0 0 0 0 0 0 0 0 0 0 n 2.3417 -1.6417 0.0000 C 0 0 0 0 0 0 0 0 0 0 0 0 n -4.3458 1.1083 0.0000 C 0 0 0 0 0 0 0 0 0 0 0 0 n -4.3458 0.0250 0.0000 C 0 0 0 0 0 0 0 0 0 0 0 0 n 2 1 2 0 n 3 2 1 0 n 4 6 1 0 n 5 7 2 0 n 6 1 1 0 n 7 1 1 0 n 8 10 1 0 n 9 2 1 0 n 10 5 1 0 n 11 5 1 0 n 12 8 2 0 n 13 8 1 0 n 14 3 1 0 n 15 4 1 0 n 16 10 1 0 n 17 14 2 0 n 18 15 2 0 n 3 4 2 0 n 9 11 2 0 n 17 18 1 0 nM END n n&gt; &lt;chembl_id&gt; nCHEMBL2104122 n n&gt; &lt;chembl_pref_name&gt; nCICLOPROFEN n n&#39;, &#39;standard_inchi&#39;: &#39;InChI=1S/C16H14O2/c1-10(16(17)18)11-6-7-15-13(8-11)9-12-4-2-3-5-14(12)15/h2-8,10H,9H2,1H3,(H,17,18)&#39;, &#39;standard_inchi_key&#39;: &#39;LRXFKKPEBXIPMW-UHFFFAOYSA-N&#39;}, &#39;molecule_synonyms&#39;: [{&#39;molecule_synonym&#39;: &#39;Cicloprofen&#39;, &#39;syn_type&#39;: &#39;BAN&#39;, &#39;synonyms&#39;: &#39;CICLOPROFEN&#39;}, {&#39;molecule_synonym&#39;: &#39;Cicloprofen&#39;, &#39;syn_type&#39;: &#39;INN&#39;, &#39;synonyms&#39;: &#39;CICLOPROFEN&#39;}, {&#39;molecule_synonym&#39;: &#39;Cicloprofen&#39;, &#39;syn_type&#39;: &#39;USAN&#39;, &#39;synonyms&#39;: &#39;CICLOPROFEN&#39;}, {&#39;molecule_synonym&#39;: &#39;SQ-20824&#39;, &#39;syn_type&#39;: &#39;RESEARCH_CODE&#39;, &#39;synonyms&#39;: &#39;SQ 20824&#39;}], &#39;molecule_type&#39;: &#39;Small molecule&#39;, &#39;natural_product&#39;: 0, &#39;oral&#39;: False, &#39;parenteral&#39;: False, &#39;polymer_flag&#39;: False, &#39;pref_name&#39;: &#39;CICLOPROFEN&#39;, &#39;prodrug&#39;: 0, &#39;score&#39;: 16.0, &#39;structure_type&#39;: &#39;MOL&#39;, &#39;therapeutic_flag&#39;: False, &#39;topical&#39;: False, &#39;usan_stem&#39;: &#39;-profen&#39;, &#39;usan_stem_definition&#39;: &#39;anti-inflammatory/analgesic agents (ibuprofen type)&#39;, &#39;usan_substem&#39;: &#39;-profen&#39;, &#39;usan_year&#39;: 1974, &#39;withdrawn_class&#39;: None, &#39;withdrawn_country&#39;: None, &#39;withdrawn_flag&#39;: False, &#39;withdrawn_reason&#39;: None, &#39;withdrawn_year&#39;: None} . For now, I&#39;m only really interested in the molecule_properties dictionary . filtered = [a[0][&#39;molecule_properties&#39;] for a in all_results if len(a) &gt; 0] . chembl_df = pd.DataFrame(filtered) chembl_df[&#39;TITLE&#39;] = repurposing_df[&#39;TITLE&#39;] . Molecular properties contained in the chembl database . Here are the definitions I can dig up . alogp: (lipophilicity) partition coefficient | aromatic_rings: number of aromatic rings | cx_logd: distribution coefficient taking into account ionized and non-ionized forms | cx_most_apka: acidic pka | cx_most_bpka: basic pka | full_mwt: molecular weight (and also free base and monoisotopic masses) | hba: hydrogen bond acceptors (and hba_lipinski for lipinski definitiosn) | hbd: hydrogen bond donors (and hbd_lipinski) | heavy_atoms: number of heavy atoms | num_lipinski_ro5_violations: how many times this molecule violated Lipinski&#39;s rule of five | num_ro5_violations: not sure, seems similar to lipinski rule of 5 | psa: protein sequence alignment | qed_weighted: &quot;quantitative estimate of druglikeness&quot; (ranges between 0 and 1, with 1 being more favorable). This is based on a quantitatve mean of drugability functions | ro3_pass: rule of three | rtb: number of rotatable bonds | . chembl_df.head() . alogp aromatic_rings cx_logd cx_logp cx_most_apka cx_most_bpka full_molformula full_mwt hba hba_lipinski hbd hbd_lipinski heavy_atoms molecular_species mw_freebase mw_monoisotopic num_lipinski_ro5_violations num_ro5_violations psa qed_weighted ro3_pass rtb TITLE . 0 3.45 | 2.0 | 1.26 | 3.92 | 4.68 | None | C16H14O2 | 238.29 | 1.0 | 2.0 | 1.0 | 1.0 | 18.0 | ACID | 238.29 | 238.0994 | 0.0 | 0.0 | 37.30 | 0.74 | N | 2.0 | CHEMBL2104122 | . 1 3.64 | 0.0 | 2.81 | 2.81 | None | None | C20H26O2 | 298.43 | 2.0 | 2.0 | 1.0 | 1.0 | 22.0 | None | 298.43 | 298.1933 | 0.0 | 0.0 | 37.30 | 0.55 | N | 0.0 | CHEMBL1387 | . 2 3.92 | 1.0 | 4.25 | 4.25 | 10.15 | 2.86 | C18H24N2O2S | 332.47 | 4.0 | 4.0 | 2.0 | 3.0 | 23.0 | NEUTRAL | 332.47 | 332.1558 | 0.0 | 0.0 | 75.68 | 0.76 | N | 1.0 | CHEMBL275835 | . 3 4.31 | 0.0 | 4.04 | 4.04 | None | None | C20H28O | 284.44 | 1.0 | 1.0 | 1.0 | 1.0 | 21.0 | None | 284.44 | 284.2140 | 0.0 | 0.0 | 20.23 | 0.52 | N | 0.0 | CHEMBL2104104 | . 4 4.79 | 0.0 | 3.96 | 3.96 | None | None | C21H28O2 | 312.45 | 2.0 | 2.0 | 0.0 | 0.0 | 23.0 | None | 312.45 | 312.2089 | 0.0 | 0.0 | 34.14 | 0.70 | N | 1.0 | CHEMBL2104231 | . chembl_df.columns . Index([&#39;alogp&#39;, &#39;aromatic_rings&#39;, &#39;cx_logd&#39;, &#39;cx_logp&#39;, &#39;cx_most_apka&#39;, &#39;cx_most_bpka&#39;, &#39;full_molformula&#39;, &#39;full_mwt&#39;, &#39;hba&#39;, &#39;hba_lipinski&#39;, &#39;hbd&#39;, &#39;hbd_lipinski&#39;, &#39;heavy_atoms&#39;, &#39;molecular_species&#39;, &#39;mw_freebase&#39;, &#39;mw_monoisotopic&#39;, &#39;num_lipinski_ro5_violations&#39;, &#39;num_ro5_violations&#39;, &#39;psa&#39;, &#39;qed_weighted&#39;, &#39;ro3_pass&#39;, &#39;rtb&#39;, &#39;TITLE&#39;], dtype=&#39;object&#39;) . chembl_df.corr() . aromatic_rings hba hba_lipinski hbd hbd_lipinski heavy_atoms num_lipinski_ro5_violations num_ro5_violations rtb . aromatic_rings 1.000000 | 0.192569 | 0.178507 | 0.014928 | 0.036106 | 0.249022 | 0.031094 | 0.031094 | 0.229124 | . hba 0.192569 | 1.000000 | 0.868859 | 0.084553 | 0.054409 | 0.451560 | -0.047705 | -0.047705 | -0.023690 | . hba_lipinski 0.178507 | 0.868859 | 1.000000 | 0.348600 | 0.294276 | 0.295864 | -0.070783 | -0.070783 | 0.021812 | . hbd 0.014928 | 0.084553 | 0.348600 | 1.000000 | 0.935710 | -0.172866 | -0.060462 | -0.060462 | 0.040505 | . hbd_lipinski 0.036106 | 0.054409 | 0.294276 | 0.935710 | 1.000000 | -0.211899 | -0.085660 | -0.085660 | 0.084225 | . heavy_atoms 0.249022 | 0.451560 | 0.295864 | -0.172866 | -0.211899 | 1.000000 | 0.397240 | 0.397240 | 0.259011 | . num_lipinski_ro5_violations 0.031094 | -0.047705 | -0.070783 | -0.060462 | -0.085660 | 0.397240 | 1.000000 | 1.000000 | 0.345308 | . num_ro5_violations 0.031094 | -0.047705 | -0.070783 | -0.060462 | -0.085660 | 0.397240 | 1.000000 | 1.000000 | 0.345308 | . rtb 0.229124 | -0.023690 | 0.021812 | 0.040505 | 0.084225 | 0.259011 | 0.345308 | 0.345308 | 1.000000 | . At a glance, no definite linear correlations among this crowd besides pKas, partition coefficients, mwt/hba . corr_df = chembl_df.corr() cols = chembl_df.columns fig, ax = plt.subplots(1,1, figsize=(8,6), dpi=100) ax.imshow(chembl_df.corr(), cmap=&#39;RdBu&#39;) ax.set_xticklabels([&#39;&#39;]+cols) ax.tick_params(axis=&#39;x&#39;, rotation=90) ax.set_yticklabels(cols) for i, (rowname, row) in enumerate(corr_df.iterrows()): for j, (key, val) in enumerate(row.iteritems()): ax.annotate(f&quot;{val:0.2f}&quot;, xy=(i,j), xytext=(-10, -5), textcoords=&quot;offset points&quot;) . Maybe there are higher-order correlations and relationship more appropriate for clustering and decomposition . cols = [&#39;aromatic_rings&#39;, &#39;cx_logp&#39;, &#39;full_mwt&#39;, &#39;hba&#39;] cleaned = (chembl_df[~chembl_df[cols] .isnull() .all(axis=&#39;columns&#39;, skipna=False)][cols] .astype(&#39;float&#39;) .fillna(0, axis=&#39;columns&#39;)) . from sklearn import preprocessing normalized = preprocessing.scale(cleaned) . Appears to be maybe 4 clusters of these compounds examined by the covid-moonshot group . from sklearn.decomposition import PCA from sklearn.manifold import TSNE tsne_analysis = TSNE(n_components=2) output = tsne_analysis.fit_transform(normalized) fig,ax = plt.subplots(1,1) ax.scatter(output[:,0], output[:,1]) ax.set_title(&quot;Aromatic rings, cx_logp, mwt, hba&quot;) . Text(0.5, 1.0, &#39;Aromatic rings, cx_logp, mwt, hba&#39;) . By taking turns leaving out some features, it looks like leaving out aromatic rings or hydrogen bond acceptors will diminish the cluster distinction. . Aromatic rings are huge and bulky components to small molecules, it makes sense that a chunk of the behavior corresponds to the aromatic rings. Similarly, hydrogen bond acceptors (heavy molecules) also induce van der Waals and electrostatics influences on small molecules. Left with only weight and partition coefficient, there&#39;s mainly a continous behavior . def clean_df(cols): cleaned = (chembl_df[~chembl_df[cols] .isnull() .all(axis=&#39;columns&#39;, skipna=False)][cols] .astype(&#39;float&#39;) .fillna(0, axis=&#39;columns&#39;)) normalized = preprocessing.scale(cleaned) return normalized cols = [&#39;cx_logp&#39;, &#39;full_mwt&#39;, &#39;hba&#39;] normalized = clean_df(cols) tsne_analysis = TSNE(n_components=2) output = tsne_analysis.fit_transform(normalized) fig,ax = plt.subplots(3,1, figsize=(8,8)) ax[0].scatter(output[:,0], output[:,1]) ax[0].set_title(&quot;cx_logp, mwt, hba&quot;) cols = [&#39;cx_logp&#39;, &#39;full_mwt&#39;, &#39;aromatic_rings&#39;] normalized = clean_df(cols) tsne_analysis = TSNE(n_components=2) output = tsne_analysis.fit_transform(normalized) ax[1].scatter(output[:,0], output[:,1]) ax[1].set_title(&quot;aromatic_rings, cx_logp, mwt&quot;) cols = [&#39;cx_logp&#39;, &#39;full_mwt&#39;] normalized = clean_df(cols) ax[2].scatter(normalized[:,0], normalized[:,1]) ax[2].set_title(&quot;cx_logp, mwt&quot;) fig.tight_layout() . DrugBank . I found someone had already downloaded the database. I may double-over these dataframes, but query the drugbank dataset rather than chembl . Some docking data . We have some smiles strings, molecular properties, docking scores, and information about the docking fragments . moonshot = pd.read_csv(&#39;moonshot-submissions/covid_submissions_all_info-docked-overlap.csv&#39;) . moonshot . SMILES TITLE creator fragments link real_space SCR BB extended_real_space in_molport_or_mcule in_ultimate_mcule in_emolecules covalent_frag covalent_warhead acrylamide acrylamide_adduct chloroacetamide chloroacetamide_adduct vinylsulfonamide vinylsulfonamide_adduct nitrile nitrile_adduct MW cLogP HBD HBA TPSA num_criterion_violations BMS Dundee Glaxo Inpharmatica LINT MLSMR PAINS SureChEMBL PostEra ORDERED MADE ASSAYED Hybrid2 docked_fragment Mpro-x1418_dock site number_of_overlapping_fragments overlapping_fragments overlap_score volume . 0 c1ccc(cc1)n2c3cc(c(cc3c(=O)c(c2[O-])c4cccnc4)F)Cl | MAK-UNK-9e4a73aa-2 | Maksym Voznyy | x1418 | https://covid.postera.ai/covid/submissions/MAK... | FALSE | FALSE | FALSE | FALSE | False | False | False | True | False | False | False | False | False | False | False | False | False | 366.779 | 4.51890 | 0 | 3 | 50.27 | 0 | PASS | beta-keto/anhydride | PASS | PASS | PASS | Ketone, Dye 11 | PASS | PASS | PASS | False | False | False | -11.881256 | x1418 | 1.206534 | active-covalent | 3 | x0434,x0678,x0830 | 3.208124 | 271.986084 | . 1 Cc1ccncc1n2c(=O)ccc3c2CCCN3CC(=[NH2+])N | KIM-UNI-60f168f5-7 | Kim Tai Tran, University of Copenhagen | x0107,x0991 | https://covid.postera.ai/covid/submissions/KIM... | FALSE | FALSE | FALSE | FALSE | False | False | False | False | False | False | False | False | False | False | False | False | False | 297.362 | 1.22949 | 2 | 5 | 88.00 | 0 | PASS | imine, imine | PASS | PASS | acyclic C=N-H | Imine 3 | PASS | PASS | PASS | False | False | False | -11.654112 | x0107 | NaN | active-noncovalent | 3 | x0107,x1412,x1392 | 4.753475 | 232.815506 | . 2 c1ccc(cc1)n2c3cc(c(cc3c(=O)n(c2=O)c4cnccn4)F)Cl | MAK-UNK-9e4a73aa-14 | Maksym Voznyy | x1418 | https://covid.postera.ai/covid/submissions/MAK... | FALSE | FALSE | FALSE | FALSE | False | False | False | True | False | False | False | False | False | False | False | False | False | 368.755 | 2.72410 | 0 | 6 | 69.78 | 0 | PASS | PASS | PASS | PASS | PASS | PASS | PASS | PASS | PASS | False | False | False | -10.460650 | x0678 | 2.716276 | active-noncovalent | 3 | x0678,x1412,x1392 | 5.520980 | 266.688721 | . 3 Cc1ccncc1N(C=C)[C@H]([C@@H](C)[C@@H]2CN=Cc3c2c... | AUS-WAB-916db9c0-1 | Austin D. Chivington, Wabash College | x0107,x1077,x1374 | https://covid.postera.ai/covid/submissions/AUS... | FALSE | FALSE | FALSE | FALSE | False | False | False | True | False | False | False | False | False | False | False | False | False | 351.450 | 3.51932 | 1 | 5 | 57.95 | 0 | non_ring_acetal | het-C-het not in ring | PASS | Filter10_Terminal_vinyl | PASS | PASS | PASS | PASS | PASS | False | False | False | -9.516450 | x0678 | NaN | active-noncovalent | 3 | x0434,x0831,x0678 | 3.446572 | 284.195312 | . 4 c1ccc2c(c1)ncc(n2)/C=C/C(=O)c3cccc(c3)O | DRV-DNY-ae159ed1-12 | Dr. Vidya Desai, Dnyanprassarak Mandals Colleg... | x1249 | https://covid.postera.ai/covid/submissions/DRV... | FALSE | FALSE | FALSE | FALSE | False | False | False | False | False | False | False | False | False | False | False | False | False | 276.295 | 3.23150 | 1 | 4 | 63.08 | 0 | PASS | PASS | PASS | Filter44_michael_acceptor2 | PASS | Ketone, Dye 9, vinyl michael acceptor1 | PASS | PASS | PASS | False | False | False | -9.243208 | x0678 | NaN | active-noncovalent | 3 | x0434,x0678,x0830 | 2.865147 | 220.275421 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 4630 C[C@H]([C@@H](C(=O)N[C@H](Cc1ccccc1)C(=O)N[C@@... | PAU-UNI-6d15a9f5-4 | paul brear, University of cambridge | x1086 | https://covid.postera.ai/covid/submissions/PAU... | FALSE | FALSE | FALSE | FALSE | False | False | False | False | False | False | False | False | False | False | False | False | False | 714.821 | -0.91270 | 8 | 11 | 256.10 | 4 | PASS | PASS | PASS | PASS | PASS | Long aliphatic chain, Dipeptide | PASS | PASS | PASS | False | False | False | 3.175111 | x0305 | NaN | active-noncovalent | 0 | NaN | 5.297134 | 548.583191 | . 4631 c1cc2cc(c(cc2c(c1)S(=O)(=O)N3CC[NH+](CC3)Cc4cc... | MAK-UNK-e05327b2-2 | Maksym Voznyy | x1402 | https://covid.postera.ai/covid/submissions/MAK... | FALSE | FALSE | FALSE | FALSE | False | False | False | True | True | False | False | True | False | False | False | False | False | 837.964 | 6.63190 | 0 | 9 | 98.31 | 2 | PASS | PASS | PASS | PASS | PASS | Hetero_hetero | PASS | PASS | PASS | False | False | False | 3.561681 | x1392 | NaN | active-covalent | 0 | NaN | 3.297014 | 591.877563 | . 4632 Cc1cccc(c1)C[NH+]2CCN(CC2)C(=O)c3ccc(cc3)C#Cc4... | MAK-UNK-e4a48a85-16 | Maksym Voznyy | x0387,x0692 | https://covid.postera.ai/covid/submissions/MAK... | FALSE | FALSE | FALSE | FALSE | False | False | False | True | False | False | False | False | False | False | False | False | False | 574.794 | 6.18892 | 0 | 5 | 39.68 | 2 | PASS | triple bond | PASS | PASS | PASS | PASS | PASS | PASS | PASS | False | False | False | 4.056698 | x0978 | NaN | active-covalent | 0 | NaN | 4.360606 | 470.944824 | . 4633 c1cc2cc(c(cc2c(c1)S(=O)(=O)N3CC[NH+](CC3)Cc4cc... | MAK-UNK-e05327b2-6 | Maksym Voznyy | x1402 | https://covid.postera.ai/covid/submissions/MAK... | FALSE | FALSE | FALSE | FALSE | False | False | False | True | True | False | False | True | False | False | False | False | False | 990.183 | 5.19160 | 0 | 12 | 138.93 | 3 | alpha_halo_heteroatom, secondary_halide_sulfate | PASS | PASS | PASS | PASS | Hetero_hetero | PASS | Dithiomethylene_acetal | Alkyl Halide | False | False | False | 4.242827 | x0731 | NaN | active-covalent | 0 | NaN | 4.193186 | 694.333069 | . 4634 Cc1cccc(c1)C[NH+]2CCN(CC2)c3cc(c(c(c3)Cl)c4cc5... | MAK-UNK-e4a48a85-15 | Maksym Voznyy | x0387,x0692 | https://covid.postera.ai/covid/submissions/MAK... | FALSE | FALSE | FALSE | FALSE | False | False | False | True | False | False | False | False | False | False | False | False | False | 659.687 | 7.36362 | 1 | 7 | 68.36 | 2 | PASS | PASS | PASS | PASS | PASS | PASS | PASS | PASS | PASS | False | False | False | 5.966927 | x0705 | NaN | active-covalent | 0 | NaN | 1.473711 | 503.583801 | . 4635 rows √ó 48 columns . moonshot.head(5) . SMILES TITLE creator fragments link real_space SCR BB extended_real_space in_molport_or_mcule in_ultimate_mcule in_emolecules covalent_frag covalent_warhead acrylamide acrylamide_adduct chloroacetamide chloroacetamide_adduct vinylsulfonamide vinylsulfonamide_adduct nitrile nitrile_adduct MW cLogP HBD HBA TPSA num_criterion_violations BMS Dundee Glaxo Inpharmatica LINT MLSMR PAINS SureChEMBL PostEra ORDERED MADE ASSAYED Hybrid2 docked_fragment Mpro-x1418_dock site number_of_overlapping_fragments overlapping_fragments overlap_score volume . 0 c1ccc(cc1)n2c3cc(c(cc3c(=O)c(c2[O-])c4cccnc4)F)Cl | MAK-UNK-9e4a73aa-2 | Maksym Voznyy | x1418 | https://covid.postera.ai/covid/submissions/MAK... | FALSE | FALSE | FALSE | FALSE | False | False | False | True | False | False | False | False | False | False | False | False | False | 366.779 | 4.51890 | 0 | 3 | 50.27 | 0 | PASS | beta-keto/anhydride | PASS | PASS | PASS | Ketone, Dye 11 | PASS | PASS | PASS | False | False | False | -11.881256 | x1418 | 1.206534 | active-covalent | 3 | x0434,x0678,x0830 | 3.208124 | 271.986084 | . 1 Cc1ccncc1n2c(=O)ccc3c2CCCN3CC(=[NH2+])N | KIM-UNI-60f168f5-7 | Kim Tai Tran, University of Copenhagen | x0107,x0991 | https://covid.postera.ai/covid/submissions/KIM... | FALSE | FALSE | FALSE | FALSE | False | False | False | False | False | False | False | False | False | False | False | False | False | 297.362 | 1.22949 | 2 | 5 | 88.00 | 0 | PASS | imine, imine | PASS | PASS | acyclic C=N-H | Imine 3 | PASS | PASS | PASS | False | False | False | -11.654112 | x0107 | NaN | active-noncovalent | 3 | x0107,x1412,x1392 | 4.753475 | 232.815506 | . 2 c1ccc(cc1)n2c3cc(c(cc3c(=O)n(c2=O)c4cnccn4)F)Cl | MAK-UNK-9e4a73aa-14 | Maksym Voznyy | x1418 | https://covid.postera.ai/covid/submissions/MAK... | FALSE | FALSE | FALSE | FALSE | False | False | False | True | False | False | False | False | False | False | False | False | False | 368.755 | 2.72410 | 0 | 6 | 69.78 | 0 | PASS | PASS | PASS | PASS | PASS | PASS | PASS | PASS | PASS | False | False | False | -10.460650 | x0678 | 2.716276 | active-noncovalent | 3 | x0678,x1412,x1392 | 5.520980 | 266.688721 | . 3 Cc1ccncc1N(C=C)[C@H]([C@@H](C)[C@@H]2CN=Cc3c2c... | AUS-WAB-916db9c0-1 | Austin D. Chivington, Wabash College | x0107,x1077,x1374 | https://covid.postera.ai/covid/submissions/AUS... | FALSE | FALSE | FALSE | FALSE | False | False | False | True | False | False | False | False | False | False | False | False | False | 351.450 | 3.51932 | 1 | 5 | 57.95 | 0 | non_ring_acetal | het-C-het not in ring | PASS | Filter10_Terminal_vinyl | PASS | PASS | PASS | PASS | PASS | False | False | False | -9.516450 | x0678 | NaN | active-noncovalent | 3 | x0434,x0831,x0678 | 3.446572 | 284.195312 | . 4 c1ccc2c(c1)ncc(n2)/C=C/C(=O)c3cccc(c3)O | DRV-DNY-ae159ed1-12 | Dr. Vidya Desai, Dnyanprassarak Mandals Colleg... | x1249 | https://covid.postera.ai/covid/submissions/DRV... | FALSE | FALSE | FALSE | FALSE | False | False | False | False | False | False | False | False | False | False | False | False | False | 276.295 | 3.23150 | 1 | 4 | 63.08 | 0 | PASS | PASS | PASS | Filter44_michael_acceptor2 | PASS | Ketone, Dye 9, vinyl michael acceptor1 | PASS | PASS | PASS | False | False | False | -9.243208 | x0678 | NaN | active-noncovalent | 3 | x0434,x0678,x0830 | 2.865147 | 220.275421 | . moonshot[&#39;Mpro-x1418_dock&#39;].isnull().sum() # Lots of missing Mpro dock scores . 4586 . While there are a lot of different fragments to which the small molecule can bind, there are two &quot;classes&quot;, active-covalent and active-noncovalent (possibly referring to sites that covalently bond?) . This presents a way to logically bisect the data based on some fundamental chemistry of the binding pocket. . moonshot[&#39;docked_fragment&#39;].value_counts() . x0678 940 x0749 771 x0104 347 x0831 283 x0830 281 x0195 269 x0161 252 x0107 201 x0072 172 x1077 127 x1392 107 x1093 107 x0434 105 x0874 81 x1385 69 x1418 58 x1334 50 x0967 46 x0397 42 x0946 38 x0692 37 x0759 37 x1386 35 x0395 29 x0305 24 x1311 16 x0708 13 x0774 12 x1380 10 x1412 7 x1374 7 x1348 6 x0770 5 x1249 5 x0387 5 x0736 4 x0705 4 x1358 3 x0426 3 x1375 3 x0734 3 x0540 3 x0354 3 x1382 3 x0755 1 x1458 1 x0689 1 x0769 1 x0981 1 x0978 1 x0731 1 x1493 1 x0771 1 x1478 1 x1384 1 x1351 1 Name: docked_fragment, dtype: int64 . moonshot[&#39;site&#39;].value_counts() . active-noncovalent 2799 active-covalent 1836 Name: site, dtype: int64 . We can examine the same correlations, but now for each type of site, and look at the hybrid docking score correlations. . The biggest trend differences appear with the partition coefficient and number of hydrogen bond donors, but still the correlations are extremely weak . site_type = &#39;active-noncovalent&#39; fig, ax = plt.subplots(1,1, figsize=(8,6), dpi=100) cols = [&#39;MW&#39;, &#39;cLogP&#39;, &#39;HBD&#39;, &#39;HBA&#39;, &#39;TPSA&#39;, &#39;Hybrid2&#39;] ax.matshow(moonshot[moonshot[&#39;site&#39;]==site_type][cols].corr(), cmap=&#39;RdBu&#39;) ax.set_xticks([i for i,_ in enumerate(cols)]) ax.set_xticklabels(cols) ax.set_yticks([i for i,_ in enumerate(cols)]) ax.set_yticklabels(cols) for i, (rowname, row) in enumerate(moonshot[moonshot[&#39;site&#39;]==site_type][cols].corr().iterrows()): for j, (key, val) in enumerate(row.iteritems()): ax.annotate(f&quot;{val:0.2f}&quot;, xy=(i,j), xytext=(-10, -5), textcoords=&quot;offset points&quot;) ax.set_title(f&quot;Docking to {site_type}&quot;) . Text(0.5, 1.05, &#39;Docking to active-noncovalent&#39;) . site_type = &#39;active-covalent&#39; fig, ax = plt.subplots(1,1, figsize=(8,6), dpi=100) cols = [&#39;MW&#39;, &#39;cLogP&#39;, &#39;HBD&#39;, &#39;HBA&#39;, &#39;TPSA&#39;, &#39;Hybrid2&#39;] ax.matshow(moonshot[moonshot[&#39;site&#39;]==site_type][cols].corr(), cmap=&#39;RdBu&#39;) ax.set_xticks([i for i,_ in enumerate(cols)]) ax.set_xticklabels(cols) ax.set_yticks([i for i,_ in enumerate(cols)]) ax.set_yticklabels(cols) for i, (rowname, row) in enumerate(moonshot[moonshot[&#39;site&#39;]==site_type][cols].corr().iterrows()): for j, (key, val) in enumerate(row.iteritems()): ax.annotate(f&quot;{val:0.2f}&quot;, xy=(i,j), xytext=(-10, -5), textcoords=&quot;offset points&quot;) ax.set_title(f&quot;Docking to {site_type}&quot;) . Text(0.5, 1.05, &#39;Docking to active-covalent&#39;) . In general, lower docking score seem better, so the noncovalent sites might present more optimal binding locations (see histogram below). This seems non-intuitive because, if active-covalent really means sites that bond covalently, then covalent bonds would seem more energetically favorable than non-covalent interactions. Alternatively, forming covalent bonds might suggest an unstable region of the complex that could be shielded from the surroundings, inhibiting any sort of small molecule from binding the pocket? Expert opinion would be much appreciated here . fig, ax = plt.subplots(1,1, figsize=(8,6), dpi=100) covalent_mean = moonshot[moonshot[&#39;site&#39;]==&#39;active-covalent&#39;][&#39;Hybrid2&#39;].mean() noncovalent_mean = moonshot[moonshot[&#39;site&#39;]==&#39;active-noncovalent&#39;][&#39;Hybrid2&#39;].mean() ax.hist(moonshot[moonshot[&#39;site&#39;]==&#39;active-covalent&#39;][&#39;Hybrid2&#39;], alpha=0.5, label=f&#39;active-covalent (mean={covalent_mean:.3f})&#39;) ax.hist(moonshot[moonshot[&#39;site&#39;]==&#39;active-noncovalent&#39;][&#39;Hybrid2&#39;], alpha=0.5, label=f&#39;active-noncovalent (mean={noncovalent_mean:.3f})&#39;) ax.set_title(f&quot;Hybrid2 histogram&quot;) ax.set_xlabel(&quot;Hybrid2 score&quot;) ax.legend() . &lt;matplotlib.legend.Legend at 0x7fac6b459850&gt; . from rdkit import Chem . rdkit_smiles = [Chem.MolFromSmiles(a) for a in moonshot.sort_values(&#39;Hybrid2&#39;, ascending=True)[&#39;SMILES&#39;].head(10)] scores = [f&quot;{a:.3f}&quot; for a in moonshot.sort_values(&#39;Hybrid2&#39;, ascending=True)[&#39;Hybrid2&#39;].head(10)] . img=Chem.Draw.MolsToGridImage(rdkit_smiles,molsPerRow=5,subImgSize=(200,200), legends=scores) . img .",
            "url": "https://ahy3nz.github.io/fastpayges/molecular%20modeling/scientific%20computing/data%20science/2020/05/06/study_covid_moonshot.html",
            "relUrl": "/molecular%20modeling/scientific%20computing/data%20science/2020/05/06/study_covid_moonshot.html",
            "date": " ‚Ä¢ May 6, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Is being "clutch" a myth?",
            "content": "Are some players more ‚Äúclutch‚Äù than others? . Clutch time is defined as ‚Äúthe last 5 minutes of a game in which the point differential is 5 or less‚Äù. Do some players really rise to the challenge and perform better in the clutch? . To address this, we can use some nba_api functionality to get clutch stats and compare them to regular season stats. Due to the small sample size of clutch stats, we look at the total field goal percentage (total field goals made divided by total field goals attempted during clutch time). However, for regular season stats, we look at the field goal percentage per game, averaged over all games. This allows us to get a sense of the game-by-game variation of a player‚Äôs field goal percentage . import numpy as np import pandas as pd import matplotlib %matplotlib inline import matplotlib.pyplot as plt import time import ballDontLie from ballDontLie.util.api_nba import find_player_id from nba_api.stats.endpoints import PlayerDashboardByClutch, PlayerGameLog, LeagueGameLog . Sample players and data pull . We will examine some famous players, some thought to be more ‚Äúclutch‚Äù than others. Further, we look at these players season-by-season; in particular, their MVP seasons. We could look into more than just their MVP seasons (some players didn‚Äôt win an MVP that season but still had some very historical regular seasons or playoff runs). Further, we could also expand to non-MVP players who have many clutch moments (Damian Lillard, Brandon Roy, among others) . players_mvps = { &#39;Michael Jordan&#39;: [&#39;1987-88&#39;, &#39;1990-91&#39;, &#39;1991-92&#39;, &#39;1995-96&#39;, &#39;1997-98&#39;], &#39;Kobe Bryant&#39;: [&#39;2007-08&#39;], &#39;LeBron James&#39;: [&#39;2008-09&#39;, &#39;2009-10&#39;, &#39;2011-12&#39;, &#39;2012-13&#39;], &#39;Kevin Durant&#39;: [&#39;2013-14&#39;], &#39;Russell Westbrook&#39;: [&#39;2016-17&#39;], &#39;Allen Iverson&#39;: [&#39;2000-01&#39;], &#39;Stephen Curry&#39;: [&#39;2014-15&#39;, &#39;2015-16&#39;], &#39;Derrick Rose&#39;: [&#39;2010-11&#39;], &#39;Steve Nash&#39;: [&#39;2004-05&#39;, &#39;2005-06&#39;] } . Some constraints due to NBA stat recording, some years don‚Äôt record clutch time stats, so we will have to account for the lack of data . df = pd.DataFrame() for i, (player, mvp_seasons) in enumerate(players_mvps.items()): for mvp_season in mvp_seasons: print(player, mvp_season) player_id = find_player_id(player)[0] season_results = {&#39;player&#39;: player, &#39;player_id&#39;: player_id, &#39;season&#39;: mvp_season} regular_season_game_log = PlayerGameLog(player_id, season=mvp_season, season_type_all_star=&#39;Regular Season&#39;) regular_season_clutch_games = PlayerDashboardByClutch(player_id, season=mvp_season, season_type_playoffs=&#39;Regular Season&#39;) season_results[&#39;regular_fg_pct&#39;] = regular_season_game_log.get_data_frames()[0][&#39;FG_PCT&#39;].mean() season_results[&#39;regular_fg_pct_std&#39;] = regular_season_game_log.get_data_frames()[0][&#39;FG_PCT&#39;].std() try: season_results[&#39;regular_clutch_fg_pct&#39;] = (regular_season_clutch_games .last5_min_plus_minus5_point_player_dashboard .get_data_frame()[&#39;FG_PCT&#39;].values[0]) except IndexError: season_results[&#39;regular_clutch_fg_pct&#39;] = -1.0 playoffs_game_log = PlayerGameLog(player_id, season=mvp_season, season_type_all_star=&#39;Playoffs&#39;) playoffs_clutch_games = PlayerDashboardByClutch(player_id, season=mvp_season, season_type_playoffs=&#39;Playoffs&#39;) season_results[&#39;playoff_fg_pct&#39;] = playoffs_game_log.get_data_frames()[0][&#39;FG_PCT&#39;].mean() season_results[&#39;playoff_fg_pct_std&#39;] = playoffs_game_log.get_data_frames()[0][&#39;FG_PCT&#39;].std() try: season_results[&#39;playoff_clutch_fg_pct&#39;] = (playoffs_clutch_games .last5_min_plus_minus5_point_player_dashboard .get_data_frame()[&#39;FG_PCT&#39;].values[0]) except IndexError: season_results[&#39;playoff_clutch_fg_pct&#39;] = -1.0 summary_dict = {i: season_results} df = df.append(pd.DataFrame.from_dict(summary_dict, orient=&#39;index&#39;)) time.sleep(10) . Michael Jordan 1987-88 Michael Jordan 1990-91 Michael Jordan 1991-92 Michael Jordan 1995-96 Michael Jordan 1997-98 Kobe Bryant 2007-08 LeBron James 2008-09 LeBron James 2009-10 LeBron James 2011-12 LeBron James 2012-13 Kevin Durant 2013-14 Russell Westbrook 2016-17 Allen Iverson 2000-01 Stephen Curry 2014-15 Stephen Curry 2015-16 Derrick Rose 2010-11 Steve Nash 2004-05 Steve Nash 2005-06 . Looking at the data, we have a somewhat neat dataframe of the players in their mvp seasons, and some information about their fg%. Unfortunately, we‚Äôre missing a lot of clutch information for Michael Jordan . df . player player_id season regular_fg_pct regular_fg_pct_std regular_clutch_fg_pct playoff_fg_pct playoff_fg_pct_std playoff_clutch_fg_pct . 0 Michael Jordan | 893 | 1987-88 | 0.536207 | 0.100253 | -1.000 | 0.526700 | 0.093797 | -1.000 | . 0 Michael Jordan | 893 | 1990-91 | 0.543220 | 0.098105 | -1.000 | 0.532059 | 0.105735 | -1.000 | . 0 Michael Jordan | 893 | 1991-92 | 0.517175 | 0.098628 | -1.000 | 0.496273 | 0.083527 | -1.000 | . 0 Michael Jordan | 893 | 1995-96 | 0.494646 | 0.098950 | -1.000 | 0.455333 | 0.104624 | -1.000 | . 0 Michael Jordan | 893 | 1997-98 | 0.463866 | 0.100443 | 0.430 | 0.468381 | 0.087482 | 0.440 | . 1 Kobe Bryant | 977 | 2007-08 | 0.464744 | 0.110210 | 0.448 | 0.485619 | 0.102819 | 0.484 | . 2 LeBron James | 2544 | 2008-09 | 0.490827 | 0.099416 | 0.556 | 0.512929 | 0.100286 | 0.526 | . 2 LeBron James | 2544 | 2009-10 | 0.499855 | 0.086847 | 0.488 | 0.487182 | 0.139674 | 0.714 | . 2 LeBron James | 2544 | 2011-12 | 0.533387 | 0.111258 | 0.453 | 0.500000 | 0.094863 | 0.370 | . 2 LeBron James | 2544 | 2012-13 | 0.572526 | 0.114762 | 0.442 | 0.496000 | 0.120758 | 0.440 | . 3 Kevin Durant | 201142 | 2013-14 | 0.510494 | 0.116384 | 0.379 | 0.460632 | 0.100532 | 0.515 | . 4 Russell Westbrook | 201566 | 2016-17 | 0.425136 | 0.119384 | 0.446 | 0.382400 | 0.078567 | 0.286 | . 5 Allen Iverson | 947 | 2000-01 | 0.412845 | 0.092642 | 0.441 | 0.380773 | 0.114597 | 0.306 | . 6 Stephen Curry | 201939 | 2014-15 | 0.483463 | 0.109280 | 0.441 | 0.456667 | 0.104391 | 0.381 | . 6 Stephen Curry | 201939 | 2015-16 | 0.499405 | 0.111825 | 0.442 | 0.436722 | 0.117035 | 0.538 | . 7 Derrick Rose | 201565 | 2010-11 | 0.447617 | 0.106040 | 0.402 | 0.400062 | 0.102861 | 0.409 | . 8 Steve Nash | 959 | 2004-05 | 0.508400 | 0.159107 | 0.447 | 0.510667 | 0.126669 | 0.444 | . 8 Steve Nash | 959 | 2005-06 | 0.513215 | 0.157780 | 0.425 | 0.500800 | 0.126879 | 0.385 | . Visualizing the results . We can plot the differences between the clutch fg% and the average fg% for each player‚Äôs season. If this number is above 0, then their clutch performances are better than their average performance. Evaluating statistical significance can be estimated if this difference is larger than the standard deviation of the player‚Äôs fg%. . Regular season . Lebron, Russ, and AI are the only players to show a clutch fg% higher than their average fg%. Unfortunately, these performance differences are very slight . Playoffs . Lebron, KD, Steph, and DRose show clutch fg%s higher than their average playoff fg% . import itertools as it fig, ax = plt.subplots(2,1, sharex=True, figsize=(8,6), dpi=100) unique_players = df[&#39;player&#39;].unique() for i, player in enumerate(unique_players): sub_df = df[df[&#39;player&#39;]==player] ax[0].errorbar([i]*len(sub_df), 100*(sub_df[&#39;regular_clutch_fg_pct&#39;] - sub_df[&#39;regular_fg_pct&#39;]), yerr=100*sub_df[&#39;regular_fg_pct_std&#39;], linestyle=&#39;&#39;, marker=&#39;o&#39;, capsize=3) ax[1].errorbar([i] * len(sub_df), 100*(sub_df[&#39;playoff_clutch_fg_pct&#39;] - sub_df[&#39;playoff_fg_pct&#39;]), yerr=100*sub_df[&#39;playoff_fg_pct_std&#39;], linestyle=&#39;&#39;, marker=&#39;o&#39;, capsize=3) ax[0].axhline(y=0, color=&#39;r&#39;, linestyle=&#39;--&#39;) ax[1].axhline(y=0, color=&#39;r&#39;, linestyle=&#39;--&#39;) ax[0].set_title(&quot;Clutch vs Average FG%&quot;) ax[1].set_xlim([-1, len(unique_players)]) ax[1].set_xticks(np.arange(0, len(unique_players))) ax[1].set_xticklabels(unique_players) ax[1].xaxis.set_tick_params(rotation=90) ax[0].set_ylabel(&quot;Regular season&quot;) ax[1].set_ylabel(&quot;Playoffs &quot;) ax[0].set_ylim([-30, 30]) ax[1].set_ylim([-30, 30]) . (-30, 30) . . Commentary on the analysis . It‚Äôs not particularly fair to just take the difference between clutch fg% and average fg%. During clutch time, it‚Äôs usually assumed the team will put the ball in their best player‚Äôs hands. For these players we sampled, this will naturally lower their fg% because defenses are focusing more strongly on them, not necessarily the pressure of the moment getting to them. Honestly, if your clutch fg% is the same as your average fg%, I‚Äôd be satisfied enough to call that player clutch. . It‚Äôs also at least fun to confirm that superstars play worse in the playoffs (if you compare the two columns in the dataframe). General concensus is that these players get guarded more tightly and schemed against, so their playoff fg% will be worse than regular season fg%. . To better evalaute ‚Äúclutch‚Äù, it might help to do this on a game-by-game basis. If a player had a hot hand and cooled off during the clutch, that‚Äôs bad. If a player was cold and hit some big shots during the clutch, that‚Äôs great. In the manner conducted here, these game-by-game fluctuations are avoided and averaged out. Looking at a more granular game-by-game method, we would witness more dramatic changes in a player‚Äôs fg% from game to game and also that player‚Äôs clutch fg% game to game (more noise in the data). . Averaging out all the game also eschews things like game severity/importance, the teams and players they were up against, and other important factors like the player-in-question‚Äôs state of mind when they went into the game or the pressure of the moment. For example, Lebron game 6 of the 2012 ECF was a very clutch performance (techincally not even during clutch time), but a performance like that just gets averaged out against all other games. Other moments like losing 3-1 leads should be very anti-clutch performances, but those get averaged out. . Conclusion . Yes, one could try to take a data-driven approach to study the clutch myth. At this day and age, there‚Äôs some data for someone to try to build a case and argue for its validity. However, I would argue there are still many ‚Äúunquantifiables‚Äù that prohibit the clutch myth to truly be scrutinized with numbers. All the complicated, ‚Äúyou had to be there‚Äù, test-your-compsure moments demonstrate the limitations of data-driven analytics. . This notebook can be found here .",
            "url": "https://ahy3nz.github.io/fastpayges/personal/data%20science/2020/04/05/clutch_myth.html",
            "relUrl": "/personal/data%20science/2020/04/05/clutch_myth.html",
            "date": " ‚Ä¢ Apr 5, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "NBA defensive schemes",
            "content": "Team defensive schemes . Does a team‚Äôs defensive scheme influence opponents‚Äô shot portfolios? I‚Äôm going to be using a different NBA API to query all the games for the 2018-2019 regular season. In each game, I‚Äôm going to log each ‚Äúmake‚Äù against a team‚Äôs defense. For example, Houston makes a 22-ft 3-point shot against Boston, I will log the distance at which Houston scored against Boston. . I‚Äôm going to be presenting this as a matrix within a pandas DataFrame, where each row is the defensive team, each column is the offensive team, and each entry is a list of distances for every successful make (excluding free throws). Conveniently, if you look across a row, you‚Äôll be examining that team‚Äôs defense against all other teams. Conversely, if you look down a column, you‚Äôll be examiniing that team‚Äôs offense against all other teams. . Some caveats to this analysis: . There are some errors to some game logs, but there are only a few, and those will have a negligible influence since each team plays 82 games anyways. | Player injuries may influence a team‚Äôs defensive/offensive play, but hopefully the team‚Äôs tactics still hold. | The game logs themselves may log each team‚Äôs acctions ‚Äúmessily‚Äù. If you look at the code, the logic to classify a make involves finding the word ‚Äòmake‚Äô and not finding the word ‚Äòfree throw‚Äô. On top of that, I‚Äôm doing some very rudimentary parsing to find the distance of each make. | Maybe I ought to normalize for pace or opposing team‚Äôs ‚Äúaverage‚Äù offensive skill/behavior, but just histogramming the counts of each make should hopefully be enough to get a sense of a defensive team‚Äôs allowed-shot portfolio. | Because we‚Äôre only looking at ‚Äúmakes‚Äù, a blocked shot and missed shot are both equally neglected when detecing ‚Äúmakes‚Äù, although, in reality, a blocked shot could say more about the defender while a missed shot might say more about the shooter. | . WARNING, the cell below takes over an hour to run . from basketball_reference_scraper.constants import TEAM_ABBR_TO_TEAM, TEAM_TO_TEAM_ABBR from basketball_reference_scraper.seasons import get_schedule from basketball_reference_scraper.shot_charts import get_shot_chart from basketball_reference_scraper.pbp import get_pbp import pandas as pd from pprint import pprint import time start = time.time() # Each row is the defensive team # Each column is the offensive team # Each element is a list of successful makes by the offensive team against the defensive team TEAMS = [&#39;ATLANTA&#39;, &#39;BOSTON&#39;, &#39;BROOKLYN&#39;, &#39;CHICAGO&#39;, &#39;CHARLOTTE&#39;, &#39;CLEVELAND&#39;, &#39;DALLAS&#39;, &#39;DENVER&#39;, &#39;DETROIT&#39;, &#39;GOLDEN STATE&#39;, &#39;HOUSTON&#39;, &#39;INDIANA&#39;, &#39;LA CLIPPERS&#39;, &#39;LA LAKERS&#39;, &#39;MEMPHIS&#39;, &#39;MIAMI&#39;, &#39;MILWAUKEE&#39;, &#39;MINNESOTA&#39;, &#39;NEW ORLEANS&#39;, &#39;NEW YORK&#39;, &#39;OKLAHOMA CITY&#39;, &#39;ORLANDO&#39;, &#39;PHILADELPHIA&#39;, &#39;PHOENIX&#39;, &#39;PORTLAND&#39;, &#39;SACRAMENTO&#39;, &#39;SAN ANTONIO&#39;, &#39;TORONTO&#39;, &#39;UTAH&#39;, &#39;WASHINGTON&#39;]# &#39;NEW JERSEY NETS&#39;, #&#39;NEW ORLEANS HORNETS&#39;, &#39;NEW ORLEANS OKLAHOMA CITY HORNETS&#39;, &#39;CHARLOTTE BOBCATS&#39;, &#39;SEATTLE SUPERSONICS&#39;, &#39;VANCOUVER GRIZZLIES&#39;] # Because of how pandas converts dictionaries into dataframes, # the &quot;first&quot; key is actually the column, while the &quot;second&quot; key is the row # team_a is defending team, team_b is offending team shots_dict = {team_a: {team_b:[] for team_b in TEAMS} for team_a in TEAMS} s = get_schedule(2019, playoffs=False) # Schedule of all games in the 2018-2019 regular season for i, series in s.iterrows(): # Get play by play for the given game try: pbp = get_pbp(series[&#39;DATE&#39;], TEAM_TO_TEAM_ABBR[series[&#39;HOME&#39;].upper()], TEAM_TO_TEAM_ABBR[series[&#39;VISITOR&#39;].upper()]) teams = series[[&#39;HOME&#39;, &#39;VISITOR&#39;]] # teams[&#39;HOME&#39;], teams[&#39;VISITOR&#39;] # Tidy up strings to format the team name and get the appropriation action if &quot;LAKERS&quot; in teams[&#39;HOME&#39;].upper(): home_team = &#39;LA LAKERS&#39; home_actions = pbp[&#39;LA LAKERS_ACTION&#39;] elif &quot;CLIPPERS&quot; in teams[&#39;HOME&#39;].upper(): home_team = &#39;LA CLIPPERS&#39; home_actions = pbp[&#39;LA CLIPPERS_ACTION&#39;] elif &quot;TRAIL BLAZERS&quot; in teams[&#39;HOME&#39;].upper(): home_team = &#39;PORTLAND&#39; home_actions = pbp[&#39;PORTLAND_ACTION&#39;] else: home_team = &#39; &#39;.join(a for a in teams[&#39;HOME&#39;].split(&#39; &#39;)[:-1]).upper() home_actions = pbp[home_team.upper()+ &quot;_ACTION&quot;] if &quot;LAKERS&quot; in teams[&#39;VISITOR&#39;].upper(): visitor_team = &#39;LA LAKERS&#39; visitor_actions = pbp[&#39;LA LAKERS_ACTION&#39;] elif &quot;CLIPPERS&quot; in teams[&#39;VISITOR&#39;].upper(): visitor_team = &#39;LA CLIPPERS&#39; visitor_actions = pbp[&#39;LA CLIPPERS_ACTION&#39;] elif &quot;TRAIL BLAZERS&quot; in teams[&#39;VISITOR&#39;].upper(): visitor_team = &#39;PORTLAND&#39; visitor_actions = pbp[&#39;PORTLAND_ACTION&#39;] else: visitor_team = &#39; &#39;.join(a for a in teams[&#39;VISITOR&#39;].split(&#39; &#39;)[:-1]).upper() visitor_actions = pbp[visitor_team.upper()+ &quot;_ACTION&quot;] # Look at the home team&#39;s actions, particularly those on offense for offense_action in home_actions[pd.notna(home_actions)]: words = offense_action.split(&#39; &#39;) success = &#39;makes&#39; in offense_action and &#39;free throw&#39; not in offense_action if success: if &#39;ft&#39; not in words: # &quot;makes 2-pt layup at rim&quot; shots_dict[home_team][visitor_team].append(0) else: # &quot;makes 2-pt jump shot/layup from 3 ft&quot; distance_index = words.index(&#39;ft&#39;) - 1 distance = int(words[distance_index]) shots_dict[home_team][visitor_team].append(distance) # Look at the visitor team&#39;s actions, particularly those on offense for offense_action in visitor_actions[pd.notna(visitor_actions)]: words = offense_action.split(&#39; &#39;) success = &#39;makes&#39; in offense_action and &#39;free throw&#39; not in offense_action if success: if &#39;ft&#39; not in words: # &quot;makes 2-pt layup at rim&quot; shots_dict[visitor_team][home_team].append(0) else: # &quot;makes 2-pt jump shot/layup from 3 ft&quot; distance_index = words.index(&#39;ft&#39;) - 1 distance = int(words[distance_index]) shots_dict[visitor_team][home_team].append(distance) except AttributeError: print(f&quot;Error looking at game {i}&quot;) continue end = time.time() print(&#39;Looping took {} seconds&#39;.format(end-start)) df = pd.DataFrame.from_dict(shots_dict) df.to_pickle(&#39;data.pkl&#39;) . Looping took 4483.57711315155 seconds . import pandas as pd df = pd.read_pickle(&#39;data.pkl&#39;) . # Plotting a team&#39;s defense versus other teams %matplotlib inline import matplotlib.pyplot as plt for i, series in df.iterrows(): fig, ax = plt.subplots(6,5, figsize=(12,10), sharex=True, sharey=True) for j, off_team in enumerate(series.index): row = j // 5 col = j % 5 ax[row, col].hist(series[off_team], bins=[0, 5, 15, 22, 35], density=False) ax[row,col].set_ylabel(f&quot;Makes by n{off_team}&quot;) ax[row,col].set_xlabel(&quot;Distance (ft)&quot;) ax[row,col].set_xticks([0, 5, 15, 22, 35]) ax[row,col].set_xticklabels([0, 5, 15, 22, 35]) ax[row,col].set_ylim([0, 100]) fig.tight_layout() simple_name = &#39;_&#39;.join(a for a in series.name.split(&#39; &#39;)) fig.savefig(f&quot;{simple_name}_defense.png&quot;, transparent=True) plt.close(fig) . Worst NBA regular season defenses . Comparing the worst defenses and best defenses (below), the best defenses allow very few short-range shots relative to allowing long-range shots. The worst defenses, however, seem to let a lot of layups and shots-in-the paint. . Cleveland‚Äôs Defense 18-19 . . Phoenix‚Äôs Defense 18-19 . . Atlanta‚Äôs Defense 18-19 . . Best NBA regular season defenses . Milwaukee‚Äôs Defense 18-19 . . Utah‚Äôs Defense 18-19 . . Indiana‚Äôs Defense 18-19 . . # Plotting a team&#39;s offense versus other teams %matplotlib inline import matplotlib.pyplot as plt for column in df: fig, ax = plt.subplots(6,5, figsize=(12,10), sharex=True, sharey=True) for j, def_team in enumerate(df[column].index): row = j // 5 col = j % 5 ax[row, col].hist(df[column][def_team], bins=[0, 5, 15, 22, 35], density=False) ax[row,col].set_ylabel(f&quot;Makes against n{def_team}&quot;) ax[row,col].set_xlabel(&quot;Distance (ft)&quot;) ax[row,col].set_xticks([0, 5, 15, 22, 35]) ax[row,col].set_xticklabels([0, 5, 15, 22, 35]) ax[row,col].set_ylim([0, 100]) fig.tight_layout() simple_name = &#39;_&#39;.join(a for a in column.split(&#39; &#39;)) fig.savefig(f&quot;{simple_name}_offense.png&quot;, transparent=True) plt.close(fig) . Worst NBA regular season offenses (by PPG) . Mempis‚Äô Offense 18-19 . . New York‚Äôs Offense 18-19 . . Cleveland‚Äôs Offense 18-19 . . Best NBA regular season offenses (by PPG) . We can see Milwaukee and New Orleans‚Äô offenses are very interior heavy, while Golden State has a relatively more balanced offense from all distances. . Milwaukee‚Äôs Offense 18-19 . . Golden State‚Äôs Offense 18-19 . . New Orleans‚Äô Offense 18-19 . . For a sanity check, Houston‚Äôs Offense 18-19 . To make sure this method was ‚Äúcorrect‚Äù we would expect Houston‚Äôs offense to be low in mid-range shots . Room for improvement . Histogramming shot distances into 1 dimension of distance definitely misses essential locations like shots from the elbow, shots from the corner, top of the key, etc. 2D-histogramming, which is almost exactly like a shot chart, would make this analysis more robust to locations on the court. Along these lines, one could dissect this player-by-player, further simplifying to a player‚Äôs shot chart. . Alternatively, one could look at time left on the shot clock, shooter‚Äôs distance from nearest defender, the style of play involved (off-ball screen, pick and roll, post-up, etc.) . This notebook can be found here .",
            "url": "https://ahy3nz.github.io/fastpayges/personal/data%20science/2020/03/31/teamDefense.html",
            "relUrl": "/personal/data%20science/2020/03/31/teamDefense.html",
            "date": " ‚Ä¢ Mar 31, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.‚Ü© . 2. This is the other footnote. You can even have a link!‚Ü© .",
            "url": "https://ahy3nz.github.io/fastpayges/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " ‚Ä¢ Feb 20, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a ‚Äúlevel 1 heading‚Äù in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here‚Äôs a footnote 1. Here‚Äôs a horizontal rule: . . Lists . Here‚Äôs a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes ‚Ä¶and‚Ä¶ . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote.¬†&#8617; . |",
            "url": "https://ahy3nz.github.io/fastpayges/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " ‚Ä¢ Jan 14, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Molecular Modeling Software: OpenMM (part 2)",
            "content": "Less-standard molecular modelling methods, combining rules, and OpenMM nonbonded forces . Almost every molecular modelling software will have some API to support harmonic bonds, harmonic angles, Lennard-Jones nonbonded interactions, and Coulombic nonbonded interactions. Pretty widely implemented, but maybe not as commonly implemented, periodic (CHARMM) dihedrals and Ryckaert-Bellemans/OPLS dihedrals. In a highly developmental field like molecular modelling, scientists will often come up with unique functional forms to express some sort of interaction. These unique functional forms could be hard to incorporate into traditional molecular simulation software; how do you incorporate them into middle-men molecular modelling API in a manner that is quick (quick to merge a PR) and reliable (just because the intermediate API can interpret a particular type of molecular model, how can you verify it converts well to another engine)? How do we make molecular modelling packages &quot;future-proof&quot; in this regard? . I am going to present the Lennard-Jones potential to introduce some nuances in molecular simulation. This is a pair potential, an interaction between two particles. For every pair, there is a $ sigma$ and $ epsilon$. In a molecular model, an atom type will prescribe a particular $ sigma$ and $ epsilon$ combination. In a system with a single atom type (thus only one $ sigma$ and $ epsilon$) . $ V_{LJ} = 4 epsilon [ ( frac{ sigma}{r})^{12} - ( frac{ sigma}{r})^6]$ . Combining (mixing) rules refer to computing unknown Lennard-Jones (LJ) sigma and epsilon values from LJ sigma and epsilon values you already know. If you imagine a system with N different atomtypes, that&#39;s N (sigmas, epsilons) for each atomtype. Now, those sigmas and epsilons will tell you how atomtype &quot;I&quot; interacts with another atomtype &quot;I&quot;, but what about how atomtype &quot;I&quot; interacts with different atomtype &quot;J&quot;? If you imagine a matrix, these cross-interactions are also known as off-diagonal elements, if you imagine sigmas/epsilons as an NxN matrix, where the diagonal elements are the self-LJ interactions. . UNLIKE harmonic bonds, where you write k (maybe account for a factor of two) and r_eq down in some way, shape, or form across nearly all molecular simulation engines, combining rules are handled vastly differently. In gromacs, this is a an integer corresponding to the comb-rule flag in the topology file. In parmed, this is a string in structure.combining_rule. In lammps, this is an input line in your run file. In openmm, this is a CustomNonbondedForce, with modifications to the NonbondedForce to prevent &quot;double accounting&quot;. In hoomd, I haven&#39;t even figured out the API/line to implement mixing rules, I&#39;ve had to unwrap, calculate, and set every single cross interaction . Lorentz-Berthelot mixing rules (generally AMBER and CHARMM force fields): . $ sigma_{ij} = frac{ sigma_i + sigma_j}{2}$ . $ epsilon_{ij} = sqrt{ epsilon_i * epsilon_j}$ . Geometric mixing rules (generally OPLS): . $ sigma_{ij} = sqrt{ sigma_i * sigma_j}$ . $ epsilon_{ij} = sqrt{ epsilon_i * epsilon_j}$ . There are other mixing rules, but these are two of the most common. These result in small numerical differences, but depending on your field within molecular simulation, this could be a big deal. For estimating bulk properties like diffusion or density, they probably won&#39;t make a huge difference, but something as sensitive as free energy calculations may be impacted by the slight difference in mixing rules. . import parmed as pmd import simtk.openmm as openmm . This is a gromacs file for a box of Lennard-Jones particles, with comb-rule=3 (geometric mixing rules) . !head lj3_bulk.top . [ defaults ] ; nbfunc comb-rule gen-pairs fudgeLJ fudgeQQ 1 3 yes 0.5 0.5 [ atomtypes ] LJP LJP 18 10.0000 0.000 A 4.00000e-01 1.0000 ; [ moleculetype ] ; Name nrexcl Lennard-Jones 1 . We turn to our handy-dandy ParmEd package to turn this file into a data object . structure = pmd.load_file(&#39;lj3_bulk.top&#39;, xyz=&#39;lj3_bulk.gro&#39;) . structure . &lt;GromacsTopologyFile 400 atoms; 400 residues; 0 bonds; PBC (orthogonal); NOT parametrized&gt; . Every atom in this system has the same atom type, this corresponds to the [atomtypes] directive in the gromacs file (the last two floats are the sigma (nm) and epsilon (kJ/mol)). ParmEd processed the sigmas and epsilons of the atomtypes just fine, and properly converted them to Angstrom and kcal/mol . structure.atoms[0].sigma, structure.atoms[0].epsilon . (4.0, 0.2390057361376673) . Combining rule was processed, here it is as a string . structure.combining_rule . &#39;geometric&#39; . Let&#39;s see how well this translates to OpenMM . system = structure.createSystem() . system . &lt;simtk.openmm.openmm.System; proxy of &lt;Swig Object of type &#39;OpenMM::System *&#39; at 0x105f07c30&gt; &gt; . Identifying the forces within the openmm.System, we see there is the openmm.NonbondedForce, which is to be expected if we have a system of particles whose sole interaction is via Lennard-Jones. The openmm.CMMotionRemover is not going to impact the energy evaluation of the model, but just used to prevent flying ice cube effects in a molecular simulation. . But what about the openmm.CustomNonbondedForce? In general, openmm.CustomXYZForce objects are openmm&#39;s way of accounting for any new potential interactions (at least, for bonds, angles, generalized Born, hydrogen bond, torsion, and nonbonded forces). There&#39;s some incredible work done to make sure these custom force objects are easy-to-create for users, but still fast to compute on GPUs. . system.getForces() . [&lt;simtk.openmm.openmm.NonbondedForce; proxy of &lt;Swig Object of type &#39;OpenMM::NonbondedForce *&#39; at 0x1070d8cf0&gt; &gt;, &lt;simtk.openmm.openmm.CustomNonbondedForce; proxy of &lt;Swig Object of type &#39;OpenMM::CustomNonbondedForce *&#39; at 0x1070d8f30&gt; &gt;, &lt;simtk.openmm.openmm.CMMotionRemover; proxy of &lt;Swig Object of type &#39;OpenMM::CMMotionRemover *&#39; at 0x1070d8db0&gt; &gt;] . OpenMM NonbondedForces . Before we look at the openmm.CustomNonbondedForce, let&#39;s look at the openmm.NonbondedForce. The openmm.NonbondedForce encompasses Lennard-Jones and Coulombic interactions. Not only that, it handles the treatment of these interactions (Long range treatments: cutoff, switching functions, PME, reaction field, and LJPME; Bonded exceptions: 1-2 exceptions, 1-3 exceptions, 1-4 exceptions and their scale factors). Note: if I refer to a &quot;nonbonded interaction&quot;, I am referring to the Coulombic (QQ) + Lennard-Jones (LJ) interactions. . I will avoid talking about long range treatments, but that is a very &quot;sensitive&quot; subject - molecular simulations can be very touchy to your long range treatment, every engine might do it differently, and many force fields prescribe different long range treatments, do your reading! . We will cover bonded exceptions as it pertains to building a molecular model. 1-2 interactions (nonbonded interactions between two particles that are directly bonded) are generally ignored by most force fields, and the only interaction is the bond interaction. The 1-2 exceptions mean the nonbonded interaction is neglected between two bonded atoms. 1-3 exceptions (two particles separated by a middle particle, think the H-H nonbonded interaction in H-O-H) are also generally ignored by most force fields. 1-4 exceptions are different; the Coulombic interactions can be scaled by an amount [0.0, 1.0), while the Lennard-Jones interactions can be scaled by another amount [0.0, 1.0). These scalings can be called fudge factors, 14 scalings, or something else, but be sure to follow the correct 14 scalings as prescribed by your force field. More on that later . Returning to the openmm.NonbondedForce, I have links to their documentation below. With the NonbondedForce, you create the Force object, but then you have to start specifying the entities for which the force will apply in addition to the parameters by which the force will behave. An example: you have to call NonbondedForce.addParticle(charge, sigma, epsilon) to specify the QQ and LJ parameters. If you call addParticle for the i-th time, you will be specifying nonbonded parameters for the i-th particle. So you will have to add nonbonded parameters sequentially, in order with how your chemical system is ordered. Later, you can call Nonbondedforce.setParticleParameters(index, charge, sigma, epsilon) to specify parameters for the i-th particle. Cross interactions are never specified as they are assumed to follow Lorentz-Berthelot (we will see how to get around this later). . There are functions for Nonbondedforce.addException(int particle1, int particle2, double chargeProd, double sigma, double epsilon, replace=False) to specify particle pairs who are exceptions to the rules - this will add an exception for a specific particle pair. To generally/globally add exceptions across your whole molecular model, you can call Nonbondedforce.createExceptsionFromBonds((int,int) bonds, coulomb14scale, lj14scale). This is more convenient as the 1,4 exceptions are universal for a given molecular model under a particular force field. . To see how parmed translates parmed nonbonded parameters into openmm nonbonded parameters go to their createSystem method. Note that their exceptions will specify a sigma of 0.5, but that doesn&#39;t matter since the chargeproduct and epsilon are 0, thereby nullifying both QQ and LJ interactions. . Openmm.NonbondedForce documentation | Python documentation | C++ documentation | . In our LJ particle system, we can get the particle parameters of the openmm.NonbondedForce. We are looking at the charge, sigma, and epsilon. . But wait - why are the charge and epsilon 0? The parmed.Stucture clearly had atomtypes specified, as did the gromacs files. . Are there 1,4 exceptions? Technically, there shouldn&#39;t be any because this LJ particle system has no bonds, so there are no 1,4 (or 1,2 or 1,3) interactions. . system.getForce(0).getParticleParameters(1) . [Quantity(value=0.0, unit=elementary charge), Quantity(value=0.5, unit=nanometer), Quantity(value=0.0, unit=kilojoule/mole)] . Hm, no 1,4 exceptions here... . system.getForce(0).getNumExceptions() . 0 . Let&#39;s look at the openmm.CustomNonbondedForce. There are 400 particles in our system, and this force has 400 particles, so every particle is interacting via this nonbonded force, and there are 2 parameters for each particle. . print(system.getForce(1).getNumParticles()) print(system.getForce(1).getNumPerParticleParameters()) . 400 2 . We can look at the actual energy function. This gives some hint into how a user could encode multiple-functions into a single string. In this string, the first function looks like a LJ potential, the second function explains what sigr6 is, the third function explains what sigr2 is, and the last function explains what sigc is - this last function is the geometric mixing rule for sigma! This is our Lennard Jones nonbonded force, but now following geometric mixing rules! . system.getForce(1).getEnergyFunction() . &#39;epsilon1*epsilon2*(sigr6^2-sigr6); sigr6=sigr2*sigr2*sigr2; sigr2=(sigc/r)^2; sigc=sigma1*sigma2&#39; . Quoting openmm documentation, &quot;The names of per-particle parameters have the suffix ‚Äú1‚Äù or ‚Äú2‚Äù appended to them to indicate the values for the two interacting particles. As seen in the above example, the expression may also involve intermediate quantities that are defined following the main expression, using ‚Äú;‚Äù as a separator.&quot; . So for these two-particle interactions, we can intelligently parse epsilon1 vs epsilon2 and sigma1 vs sigma2 as parameters for different particles, and also intelligently &quot;join&quot; multiple functions. I will observe that this is a combination of openmm&#39;s API and how parmed implemented geometric mixing rules for openmm. . In broad strokes of how parmed does it, the openmm.CustomNonbondedForce object is created, whose only argument is the string that is the multiple functions being employed. You specify the &quot;perParticleParameters&quot; that every particle should know. I&#39;ll ignore the nonbonded methods for now. Just like openmm.NonbondedForce, you add each particle sequentially (i.e. you add particle 1&#39;s parameters first, particle 3&#39;s parameters third, etc.). The parameters to add to each particle should be in the order you initially added them to the Force object - we call force.addPerParticleParameter(&#39;epsilon&#39;) and then force.addPerParticleParameter(&#39;sigma&#39;), this means we should add particles&#39; parameters as epsilon, then sigma. We iterate through each atom in our parmed.Structure, do a unit conversion on epsilon and sigma, then call force.addParticle((eps,sig)). Again, we have to add each particle in order, but we also have to add each parameter in order. Note: we did not have to do any manual code-calculation of the mixed-sigma or mixed-epsilon, this is handled in the openmm.CustomNonbondedForce object. At this point, we&#39;ve specified everything we need for a LJ-geometric-mixing-rule-force. . For cleanup, we have to post-process and back-track through the original LJ force object (the openmm.NonbondedForce). This means keeping the original charges, but setting the LJ interactions to 0. The LJ (geometric-mixing) interactions are handled by the CustomNonbondedForce, the LJ (lorentz-mixing) interactions are set to 0 via epsilon in the NonbondedForce, and the QQ interactions are un-touched in the NonbondedForce. Lastly, any 1,4 exclusions in the NonbondedForce need to get replicated and carried over as 1,4 exclusions in the CustomNonbondedForce. . Openmm.CustomNonbondedForce documentation | Python documentation | C++ documentation | . This looks like a lot of work for something fairly simple . Valid point. However, if you wanted to make a truly flexible engine that could accommodate some new, arbitrary molecular model feature, you&#39;ll have to do a lot of infrastructure/backend work to make sure this is done properly, reproducibly, cleanly, and done well. If you were just trying to make an engine that accommodated only the simplest molecular models, then that would be fairly simple to implement and the resultant API would be pretty simple, but you&#39;re no longer accounting for these &quot;edge case&quot; models. . Maybe this applies in general to software and data engineering, but flexibility will require complexity . Using this notebook . All notebooks within this webiste/repo can be found here .",
            "url": "https://ahy3nz.github.io/fastpayges/molecular%20modeling/scientific%20computing/grad%20school/2019/10/30/openmm2.html",
            "relUrl": "/molecular%20modeling/scientific%20computing/grad%20school/2019/10/30/openmm2.html",
            "date": " ‚Ä¢ Oct 30, 2019"
        }
        
    
  
    
        ,"post15": {
            "title": "Molecular Modeling Software: OpenMM",
            "content": "OpenMM is &quot;A high performance toolkit for molecular simulation. Use it as a library, or as an application. We include extensive language bindings for Python, C, C++, and even Fortran. The code is open source and actively maintained on Github, licensed under MIT and LGPL. Part of the Omnia suite of tools for predictive biomolecular simulation.&quot; Here&#39;s their GitHub repo, and Conda link, though I think they might be relocating their channel to conda-forge. . Here&#39;s my opinion, OpenMM is a very powerful, flexible engine that has integration with a variety of other MD engines, supports a variety of molecular models, excellent GPU support, active open-source development, and is the underlying molecular dynamics engine for OpenForceField efforts, but very easy to port to other MD engines via ParmEd. There&#39;s also support for enhanced sampling and integration with deep learning libraries. If there was a 21st century, best-software-practices, open-source software for molecular modelling and simulation, OpenMM (or HOOMD) would likely be it. . In reality, I&#39;m not sure how many graduate students/academic labs opt to use OpenMM if the lab has historically used another MD engine. Also, this is a somewhat unfounded observation, but I&#39;m curious if/how much the computer-aided drug design industry has adopted the use of OpenMM. More editorializing, but my graduate work never brought me into tight overlap with the OpenMM world/community, but it certainly seems like a vibrant community that is pushing the development and popularity of molecular modelling and simulation . The OpenMM Public API . I&#39;m mainly summarizing and regurgitating the OpenMM documentation. These are some important terms to know within the OpenMM API: . System - this object stores information about numbers of particles, particle masses, box information, constraints, and virtual site information. Note the lack of positions, bonding information, integrators, simulation run parameters. The System object also contains your Forces. | Force - Force objects describe how your particles interact with each other. This is where your force field gets implemented - outlining the molecular model forces in play, the treatment of long range interactions, and even your barostat. This is, broadly, what a Force object is, but there is much more in the details of specific Force objects, like an openmm.HarmonicBondForce. Upon implementation, it&#39;s interesting to note that the &quot;Container&quot; is the Force object, and it contains the parameters and particles the obey this force. Sort of turning this concept upside-down, Parmed&#39;s atoms and bonds are the objects that contain the interaction parameters of that force. from a Context | . | Integrator - This is the integration algorithm by which you progress your particle&#39;s positions and simulation over time. | Context - this object stores information about your particle coordinates, velocities, and specially-defined/parametrized Forces. When you run an actual simulation or produce a trajectory, you will have to start from a Context. Contexts contain information about integrators, which helps distinguish information about your molecular model of your System (forces, masses) from the things that will be used to run your simulation. | State - this is like a single frame/snapshot/checkpoint within your simulation. It&#39;s everything that was being calculated at that particular timestep. If you want peer into your simulation, you will be looking at its State. If you want to report some information, you will be parsing information from the State. | . There are numerous tutorials on running OpenMM simulations, but I want to focus on building the OpenMM objects and everything before you need to think about Integrators or States, as this is key for builting interoperability between molecular modelling software. . import simtk.unit as unit import simtk.openmm as openmm . In this bare-bones model, we will just create an OpenMM.System object, and the only forces interacting in the system will be the OpenMM.NonbondedForce. After we add the force to the system, we are returned the index of the force - if you wanted to find it within our system via system.getForces(), which is a list of force objects. Credit to the OpenMM documentation . system = openmm.System() # Create the openmm System nonbonded_force = openmm.NonbondedForce() # Create the Force object, specifically, a NonbondedForce object print(system.addForce(nonbonded_force)) # Returns the index of the force we just added print(system.getForces()) . 0 [&lt;simtk.openmm.openmm.NonbondedForce; proxy of &lt;Swig Object of type &#39;OpenMM::NonbondedForce *&#39; at 0x107166510&gt; &gt;] . As a brief foray into python-C++ interfaces, these two objects have slightly different (python) addresses, but we will see that they refer to the same C++ object . print(system.getForce(0)) print(nonbonded_force) . &lt;simtk.openmm.openmm.NonbondedForce; proxy of &lt;Swig Object of type &#39;OpenMM::NonbondedForce *&#39; at 0x1071664e0&gt; &gt; &lt;simtk.openmm.openmm.NonbondedForce; proxy of &lt;Swig Object of type &#39;OpenMM::NonbondedForce *&#39; at 0x103e46fc0&gt; &gt; . Next, we will start creating our particles and nonbonded interaction parameters. This code is contrived for sake of example, but you can imagine there are more sophisticated and relevant ways to add positions, masses, or nonbonded parameters . import itertools as it import numpy as np positions = [] # Create a running list of positions for x,y,z in it.product([0,1,2], repeat=3): # Looping through a 3-dimensional grid, 27 coordinates # Add to our running list of positions # Note that these are just ints, we will have to turn them into simtk.Quantity later positions.append([x,y,z]) # Add the particle&#39;s mass to the System object system.addParticle(39.95 * unit.amu) # Add nonbonded parameters to our NonbondedForce object - charge, LJ sigma, LJ epsilon nonbonded_force.addParticle(0*unit.elementary_charge, 0.3350 * unit.nanometer, 0.996 * unit.kilojoule_per_mole) . We can compare the two force objects from earlier - the NonbondedForce we created from code and the NonbondedForce that is returned when we access our system. Both refer to the same underlying OpenMM.NonbondedForce object and will reflect the same information. These are just two ways of accessing this object. The system also agrees with the number of particles we have added. . (system.getForce(0).getNumParticles(), nonbonded_force.getNumParticles(), system.getNumParticles()) . (27, 27, 27) . The next object to deal with is the OpenMM.Context, which specifies positions. First we need to convert our list of coordinates into a more-tractable numpy.ndarray of coordinates, and then turn that into a simtk.Quantity of our coordinates. Additionally, the OpenMM.Context constructor requires an integrator (at this point we are trying to build our simulation), and then we can specify the positions within that context . np_positions = np.asarray(positions) unit_positions = np_positions * unit.nanometer type(np_positions), type(unit_positions) integrator = openmm.VerletIntegrator(1.0) # 1 ps timestep context = openmm.Context(system, integrator) # create context context.setPositions(unit_positions) # specify positions within context . We can parse some information about our context, and this is done by getting the state of our context. Note how the time is 0.0 ps (we haven&#39;t run our simulation at all). But we can also parse the potential energy of our context - this is the potential energy given the positions we initialized and forces we specified. . print(context.getState().getTime()) print(context.getState(getEnergy=True).getPotentialEnergy()) . 0.0 ps -0.3682566285133362 kJ/mol . What happens to our state after we&#39;ve run for some amount of time? We will run for 10 time steps (or 10 ps since our timestep is 1 ps). We can see the the time reported by our state has changed, and so has the potentialEnergy . integrator.step(10) # Run for 10 timesteps . print(context.getState().getTime()) print(context.getState(getEnergy=True).getPotentialEnergy()) . 10.0 ps -0.5352763533592224 kJ/mol . type(system), type(context), type(integrator), type(nonbonded_force) . (simtk.openmm.openmm.System, simtk.openmm.openmm.Context, simtk.openmm.openmm.VerletIntegrator, simtk.openmm.openmm.NonbondedForce) . This summarizes how system, force, context, state, and integrator objects interact with each other within the OpenMM API. Side note, observe where in the API these are stored - at the base level openmm.XYZ, this next section will move &quot;up a level&quot; to some objects and API that build off these base level API . More practical OpenMM simulations . We just talked about some of the base-layer objects within OpenMM, but often people will &quot;wrap&quot; those base layer objects within an OpenMM.Simulation object, pass topological (bonding + box information) through a openmm.Topology object, attach reporter objects, and then run the simulation. . The Simulation wraps the topology, system, integrator, and hardware platforms and implicitly creates the Context. . The Topology contains information about the atoms, bonds, chains, and residues within your system, in addition to box information. . Reporter objects are used to print/save various information about the trajectory. . OpenMM.Simulation documentation | OpenMM.Topology documentation | OpenMM reporters | . Here&#39;s some contrived code to quickly make an ethane molecule, atomtype, and parametrize according to OPLSAA . import mbuild as mb import foyer import parmed as pmd from mbuild.examples import Ethane cmpd = Ethane() # mbuild compound ff = foyer.Forcefield(name=&#39;oplsaa&#39;) # foyer forcefield structure = ff.apply(cmpd) # apply forcefield to compound to get a pmd.Structure . /Users/ayang41/Programs/foyer/foyer/validator.py:132: ValidationWarning: You have empty smart definition(s) warn(&#34;You have empty smart definition(s)&#34;, ValidationWarning) /Users/ayang41/Programs/foyer/foyer/forcefield.py:248: UserWarning: Parameters have not been assigned to all impropers. Total system impropers: 8, Parameterized impropers: 0. Note that if your system contains torsions of Ryckaert-Bellemans functional form, all of these torsions are processed as propers warnings.warn(msg) . Now we have a parmed.Structure that has atomtypes and force field parameters. Conveniently, parmed.Structure can quickly create an openmm.app.topology object, and we can see some basic information like numbers of atoms and bonds. It&#39;s also worth observing that this is openmm.app.topology, within the &quot;application layer&quot;, one level above the base layer . print(structure.topology) # the parmed structure can create the openmm topology print(type(structure.topology)) [a for a in structure.topology.atoms()] . &lt;Topology; 1 chains, 1 residues, 8 atoms, 7 bonds&gt; &lt;class &#39;simtk.openmm.app.topology.Topology&#39;&gt; . [&lt;Atom 0 (C) of chain 0 residue 0 (RES)&gt;, &lt;Atom 1 (H) of chain 0 residue 0 (RES)&gt;, &lt;Atom 2 (H) of chain 0 residue 0 (RES)&gt;, &lt;Atom 3 (H) of chain 0 residue 0 (RES)&gt;, &lt;Atom 4 (C) of chain 0 residue 0 (RES)&gt;, &lt;Atom 5 (H) of chain 0 residue 0 (RES)&gt;, &lt;Atom 6 (H) of chain 0 residue 0 (RES)&gt;, &lt;Atom 7 (H) of chain 0 residue 0 (RES)&gt;] . We can now build out some other relevant features of running a simulation . system = structure.createSystem() # the parmed structure can create the openmm system integrator = openmm.VerletIntegrator(1.0) # create another openmm integrator . Putting it all together, we make our Simluation object. Once again, note how this is within the app layer . simulation = openmm.app.Simulation(structure.topology, system, integrator) type(simulation) . simtk.openmm.app.simulation.Simulation . After creating the Simulation object, we have access to the Context related to the System and Integrator . simulation.context . &lt;simtk.openmm.openmm.Context; proxy of &lt;Swig Object of type &#39;OpenMM::Context *&#39; at 0x1153785d0&gt; &gt; . Once again, we need to specify the positions. Fortunately, the parmed.Structure already uses simtk.Quantity for its positions. . simulation.context.setPositions(structure.positions) . Before running the simulation, we can get some State information related to this Context . simulation.context.getState().getTime() . Quantity(value=0.0, unit=picosecond) . We can now run this simulation and observe that the State changes . simulation.step(10) simulation.context.getState().getTime() . Quantity(value=10.0, unit=picosecond) . The application layer to interact with OpenMM . The OpenMM application layer is largely everything you would need to build and run a simulation with OpenMM, with some compatibility with files from other MD engines. The application layer was built on top of the base library that housed the core OpenMM classes. . Summary . OpenMM is a flexible library and API for molecular modelling. It has well-designed classes wrapped in convenience API for users, while supporting hardware/GPU acceleration with minimial user effort. This may just be me, but I found learning the &quot;vocabulary&quot; and distinction between the base classes was a little hard to understand, but this kind of issue addresses itself over time if one plays around with the API. I am interested to investigate how well one can use OpenMM to build a variety of molecular models and how OpenMM can interface with interconversion libraries such as ParmEd to facilitate engine-flexibility. The devil is always in the details, so building your molecular model in OpenMM or ParmEd is always going to require due diligence to ensure correct output. . Using this notebook . All notebooks within this webiste/repo can be found here .",
            "url": "https://ahy3nz.github.io/fastpayges/molecular%20modeling/scientific%20computing/grad%20school/2019/10/29/openmm1.html",
            "relUrl": "/molecular%20modeling/scientific%20computing/grad%20school/2019/10/29/openmm1.html",
            "date": " ‚Ä¢ Oct 29, 2019"
        }
        
    
  
    
        ,"post16": {
            "title": "Molecular Modeling Software: OpenForceField",
            "content": "The open-source molecular modelling community is a small (but growing) trend within academics. A lot of academics (professors, lab scientists, grad students) are putting together libraries and API that help fulfill a small task or purpose, and 21st century software engineering standards make them usable by others. Even in these early stages of open-source molecular modelling, these libraries are striving for interoperability, where two independently-developed API have gotten to the point where now they want to interact and communicate with each other. . This is a very interesting point in time, where molecular modellers are now tasked with the effort of making many different libraries and API work together to successfully run simulations and complete research projects. Usually, scientists work within a singular software package that was designed by some core developers, and those scientists didn&#39;t need to venture outside that single software package, the license they paid for it, and the manual. . With the release of OpenForceField 1.0, I was curious to use their SMRINOFF force field. To my understanding (don&#39;t quote me on this), the idea behind SMIRNOFF is to simplify molecular mechanics force fields, cut down on redundant atom types/parameters, and parametrize molecules based on &quot;chemical perception&quot; (chemical context and local bonding environment). Armed with these simplified, context-based force field methodologies, the frustrating, in-the-weeds obstacles associated with force field development might be ameliorated - the kinds of obstacles of which molecular modellers are painfully aware. . Beyond the SMIRNOFF force field, I was curious how parameters might compare to the older OPLS all-atom force fields. As a personal challenge, I wanted to see how much &quot;modern computational science&quot; I could use, specifically trying to exercise the interoperability between different open-source molecular modelling packages. . Building the our molecular system and model . We begin with some imports. We can already see a variety of packages being used: mBuild, Foyer, ParmEd, OpenForceField, Simtk, OpenMM, MDTraj, and NGLView. . Take note of all the different data structure interconversions happening. There are a lot. This is good that we can get these API working together this often, but maybe not-so-good that we have to do these interconversions so often . Note: OpenForceField also utilizes RDKit and OpenEyeToolkit. mBuild also utilizes OpenBabel . import mbuild from mbuild.examples import Ethane import foyer # ParmEd for interconverting data structures import parmed # Omnia suite of molecular modelling tools from openforcefield.topology import Topology, Molecule from openforcefield.typing.engines.smirnoff import ForceField from simtk import openmm, unit # For post-simulation analysis and visualization import mdtraj # Also Omnia import nglview . Warning: Unable to load toolkit &#39;OpenEye Toolkit&#39;. The Open Force Field Toolkit does not require the OpenEye Toolkits, and can use RDKit/AmberTools instead. However, if you have a valid license for the OpenEye Toolkits, consider installing them for faster performance and additional file format support: https://docs.eyesopen.com/toolkits/python/quickstart-python/linuxosx.html OpenEye offers free Toolkit licenses for academics: https://www.eyesopen.com/academic-licensing /Users/ayang41/anaconda3/envs/mosdef37/lib/python3.7/site-packages/nglview/widget.py:162: DeprecationWarning: Traits should be given as instances, not types (for example, `Int()`, not `Int`). Passing types is deprecated in traitlets 4.1. _ngl_view_id = List(Unicode).tag(sync=True) . We will use mBuild to create a generic Ethane ($C_2H_6$) molecule. While this is imported from the examples, mBuild functionality allows users to construct chemical systems in a lego-like fashion by declaring particles and bonding them. Under the hood, rigid transformations are performed to orient particles-to-be-bonded . mbuild_compound = Ethane() mbuild_compound.visualize(backend=&#39;nglview&#39;) . /Users/ayang41/Programs/mbuild/mbuild/utils/io.py:120: DeprecationWarning: openbabel 2.0 detected and will be dropped in a future release. Consider upgrading to 3.x. warnings.warn(msg, DeprecationWarning) /Users/ayang41/Programs/mbuild/mbuild/utils/io.py:120: DeprecationWarning: openbabel 2.0 detected and will be dropped in a future release. Consider upgrading to 3.x. warnings.warn(msg, DeprecationWarning) . Another operation we can do within mBuild is to take this compound, convert it to an openbabel.Molecule object, and obtain the SMILES string for it. . ethane_obmol = mbuild_compound.to_pybel() ethane_obmol.write(&quot;smi&quot;, &#39;out.smi&#39;, overwrite=True) smiles_string = open(&#39;out.smi&#39;, &#39;r&#39;).readlines() print(smiles_string) . [&#39;CC tEthane n&#39;] . /Users/ayang41/Programs/mbuild/mbuild/utils/io.py:120: DeprecationWarning: openbabel 2.0 detected and will be dropped in a future release. Consider upgrading to 3.x. warnings.warn(msg, DeprecationWarning) . Using foyer, we can convert an mbuild.Compound object to an openmm.Topology object. openmm.Topology objects don&#39;t actually know positions, they just know certain atomic and bonding information, but no coordinates/velocities/force field information. This foyer function helps recover the positions in a simple array of simtk.Quantity . omm_topology, xyz = foyer.forcefield.generate_topology(mbuild_compound, residues=&#39;Ethane&#39;) print(omm_topology) print(xyz) . &lt;Topology; 1 chains, 1 residues, 8 atoms, 7 bonds&gt; [Vec3(x=2.819666989525475e-16, y=-1.4, z=-1.4644271506889933e-16), Vec3(x=-1.0699999332427972, y=-1.4000000000000001, z=-6.273541601638111e-17), Vec3(x=0.3570000827312472, y=-2.169000053405761, z=0.6530000269412993), Vec3(x=0.3570000827312474, y=-1.5810000836849212, z=-0.9929999709129338), Vec3(x=0.0, y=0.0, z=0.0), Vec3(x=1.0699999332427979, y=0.0, z=0.0), Vec3(x=-0.35700008273124695, y=0.7690000534057617, z=0.6530000269412994), Vec3(x=-0.35700008273124695, y=0.18100008368492126, z=-0.9929999709129333)] A . To translate these objects into openforcefield.Topology objects, we need to identify the unique molecules, which helps identify the isolated subgraphs - individual molecules that don&#39;t bond to anything outside its molecular network. . Using the SMILES string, we can generate an openforcefield.Molecule object, which is this self-enclosed bonding entity (chemically speaking, this is a molecule) . ethane_molecule = Molecule.from_smiles(smiles_string[0].split()[0]) ethane_molecule . Molecule with name &#39;&#39; and SMILES &#39;[H][C]([H])([H])[C]([H])([H])[H]&#39; . Now that we have isolated the unique molecules, we can construct our openforcefield.Topology object from our openmm.Topology and openmm.Molecule objects. . off_topology = Topology.from_openmm(omm_topology, unique_molecules=[ethane_molecule]) off_topology . &lt;openforcefield.topology.topology.Topology at 0x113ad0748&gt; . Adding in a force field, evaluating energy . Next, we need to create our openforcefield.Forcefield object. These are created from offxml files, and the OpenForceField group publishes new ones fairly regularly. . In the comments is an example (but out-of-date) force field within the main openforcefield package. . The one we are using is the most-recent SMIRNOFF force field (I think this one is Parsley, or maybe just 1.0.0). The SMIRNOFF force fields are being housed in a separate repo, but utilize pythonic entry_points to help one repo into another. . off_forcefield = ForceField(&#39;smirnoff99Frosst-1.1.0.offxml&#39;) off_forcefield . &lt;openforcefield.typing.engines.smirnoff.forcefield.ForceField at 0x114dd72e8&gt; . With the openforcefield.Topology and openforcefield.Forcefield objects, we can create an openmm.System. Note the discrepancy/interplay between the objects - openforcefield for the molecular mechanics building blocks, but openmm is ultimately the workhorse for simulating and representing these systems (although you could opt to simulate with other engines via parmed). . Note the use of AM1-BCC methods to identify partial charges. . smirnoff_omm_system = off_forcefield.create_openmm_system(off_topology) smirnoff_omm_system . Warning: In AmberToolsToolkitwrapper.compute_partial_charges_am1bcc: Molecule &#39;&#39; has more than one conformer, but this function will only generate charges for the first one. . &lt;simtk.openmm.openmm.System; proxy of &lt;Swig Object of type &#39;OpenMM::System *&#39; at 0x111a4fde0&gt; &gt; . This is a utility function we will use to evaluate the energy of a molecular system. Given an openmm.system (force field, parameters, topological, atomic information) and atomic coordinates, we can get a potential energy associated with that set of coordinatess . def get_energy(system, positions): &quot;&quot;&quot; Return the potential energy. Parameters - system : simtk.openmm.System The system to check positions : simtk.unit.Quantity of dimension (natoms,3) with units of length The positions to use Returns energy Notes -- Taken from an openforcefield notebook &quot;&quot;&quot; integrator = openmm.VerletIntegrator(1.0 * unit.femtoseconds) context = openmm.Context(system, integrator) context.setPositions(positions) state = context.getState(getEnergy=True) energy = state.getPotentialEnergy().in_units_of(unit.kilocalories_per_mole) return energy . Next, we will try to calculate the potential energy of our ethane system under the SMIRNOFF force field. As a (small) obstacle to doing this, we need to change the dimensions of our simulation box because some force fields and simulations use cutoffs, and cutoffs cannot be larger than the simulation box itself. . Okay 45 kcal/mol, cool. Potential energies of single configurations are usually not helpful for any real physical analysis, but can be helpful in comparing force fields. . new_vectors = [[10*unit.nanometer, 0*unit.nanometer, 0*unit.nanometer], [0*unit.nanometer, 10*unit.nanometer, 0*unit.nanometer], [0*unit.nanometer, 0* unit.nanometer, 10*unit.nanometer]] smirnoff_omm_system.setDefaultPeriodicBoxVectors(*new_vectors) get_energy(smirnoff_omm_system, xyz) . Quantity(value=44.96860291382222, unit=kilocalorie/mole) . Tangent: Interfacing with other simulation engines . We can use parmed to convert the openmm.Topology, openmm.System, and coordinates into a parmed.Structure. From a parmed.Structure, we can spit out files appropriate for different simulation packages. Word of caution, while the developers of parmed did an excellent job building the conversion tools, please do your due diligence to make sure the output is as you expect . pmd_structure = parmed.openmm.load_topology(omm_topology, system=smirnoff_omm_system, xyz=xyz) pmd_structure . &lt;Structure 8 atoms; 1 residues; 7 bonds; PBC (orthogonal); NOT parametrized&gt; . Comparing to the OPLS-AA force field . Let&#39;s use a different force field. foyer ships with an XML of the OPLS-AA force field. We will use foyer (which utilizes some parmed and openmm api) to build our molecular model of ethane with OPLS . Create the foyer.Forcefield object | Apply it to our mbuild.Compound, get a parmed.Structure | Convert the parmed.Structure to an openmm.System | Reset the box vectors to be consistent with the SMIRNOFF example | Evaluate the energy | . foyer_ff = foyer.Forcefield(name=&#39;oplsaa&#39;) opls_pmd_structure = foyer_ff.apply(mbuild_compound) opls_omm_system = opls_pmd_structure.createSystem() opls_omm_system.setDefaultPeriodicBoxVectors(*new_vectors) get_energy(opls_omm_system, opls_pmd_structure.positions) . /Users/ayang41/Programs/foyer/foyer/validator.py:132: ValidationWarning: You have empty smart definition(s) warn(&#34;You have empty smart definition(s)&#34;, ValidationWarning) /Users/ayang41/Programs/foyer/foyer/forcefield.py:248: UserWarning: Parameters have not been assigned to all impropers. Total system impropers: 8, Parameterized impropers: 0. Note that if your system contains torsions of Ryckaert-Bellemans functional form, all of these torsions are processed as propers warnings.warn(msg) . Quantity(value=37.52734328319192, unit=kilocalorie/mole) . 37.5 kcal/mol versus 45.0 kcal/mol, for a single ethane. This is a little alarming because this energetic difference stems from how the interactions are quantified and parametrized. . However, this isn&#39;t a deal-breaker since most physically-interesting phenomena depend on changes in (free) energies. So a singular energy isn&#39;t important - how it varies when you change configurations or sample configurational space is generally more important . Cracking open the models and looking at parameters . In this whole process, we&#39;ve been dealing with data structures and API that are fairly transparent. What&#39;s great now is that we can look in-depth at these data structures. Specifically, we could look at the force field parameters, either within the force field files or molecular models themselves. . We&#39;re going to crack open these openmm.System objects, looking at how some of these forces are parametrized . Within the SMIRNOFF force field applied to ethane, we have some harmonic bonds, harmonic angles, periodic torsions, and nonbonded forces . smirnoff_omm_system.getForces() . [&lt;simtk.openmm.openmm.NonbondedForce; proxy of &lt;Swig Object of type &#39;OpenMM::NonbondedForce *&#39; at 0x116245960&gt; &gt;, &lt;simtk.openmm.openmm.PeriodicTorsionForce; proxy of &lt;Swig Object of type &#39;OpenMM::PeriodicTorsionForce *&#39; at 0x116245a50&gt; &gt;, &lt;simtk.openmm.openmm.HarmonicAngleForce; proxy of &lt;Swig Object of type &#39;OpenMM::HarmonicAngleForce *&#39; at 0x116245a80&gt; &gt;, &lt;simtk.openmm.openmm.HarmonicBondForce; proxy of &lt;Swig Object of type &#39;OpenMM::HarmonicBondForce *&#39; at 0x116245ab0&gt; &gt;] . Within the OPLS force field applied to ethane, we have some harmonic bonds, harmonic angles, Ryckaert-Belleman torsions, and nonbonded forces. Don&#39;t worry about the center of mass motion remover - that&#39;s more for running a simulation. . opls_omm_system.getForces() . [&lt;simtk.openmm.openmm.HarmonicBondForce; proxy of &lt;Swig Object of type &#39;OpenMM::HarmonicBondForce *&#39; at 0x116245930&gt; &gt;, &lt;simtk.openmm.openmm.HarmonicAngleForce; proxy of &lt;Swig Object of type &#39;OpenMM::HarmonicAngleForce *&#39; at 0x116245b40&gt; &gt;, &lt;simtk.openmm.openmm.RBTorsionForce; proxy of &lt;Swig Object of type &#39;OpenMM::RBTorsionForce *&#39; at 0x116245b70&gt; &gt;, &lt;simtk.openmm.openmm.NonbondedForce; proxy of &lt;Swig Object of type &#39;OpenMM::NonbondedForce *&#39; at 0x116245ba0&gt; &gt;, &lt;simtk.openmm.openmm.CMMotionRemover; proxy of &lt;Swig Object of type &#39;OpenMM::CMMotionRemover *&#39; at 0x116245bd0&gt; &gt;] . We are going to compare the nonbonded parameters between these openmm.System objects. For every particle in our system, we&#39;re going to look at their charges, LJ sigmas, and LJ epsilsons (both of these systems utilize Coulombic electrostatics and Lennard-Jones potentials) . Based on the charges and frequency-of-appearance, we can see which ones are carbons and which ones are hydrogens. . The OPLS-system is more-charged, carbons are more negative and hydrogens are more positive. The SMIRNOFF-system actually isn&#39;t electro-neutral, and that might be consequence of having used AM1-BCC for such a small system. . The sigmas are pretty similar between FF implementations. The hydrogen epsilsons in SMIRNOFF are about half of those in OPLS. The carbon epsilons in SMIRNOFF are almost double those in OPLS. This is kind of interesting, while SMIRNOFF-ethane has weaker electrostatics (weaker charges), the LJ might compensate with the greater carbon-epsilon. . opls_omm_nonbond_force = opls_omm_system.getForce(3) smirnoff_omm_nonbond_force = smirnoff_omm_system.getForce(0) for i in range(opls_omm_nonbond_force.getNumParticles()): opls_params = opls_omm_nonbond_force.getParticleParameters(i) smirnoff_params = smirnoff_omm_nonbond_force.getParticleParameters(i) print(opls_params) print(smirnoff_params) print(&#39;&#39;) . [Quantity(value=-0.18, unit=elementary charge), Quantity(value=0.35000000000000003, unit=nanometer), Quantity(value=0.276144, unit=kilojoule/mole)] [Quantity(value=-0.0941, unit=elementary charge), Quantity(value=0.3399669508423535, unit=nanometer), Quantity(value=0.4577296, unit=kilojoule/mole)] [Quantity(value=0.06, unit=elementary charge), Quantity(value=0.25, unit=nanometer), Quantity(value=0.12552, unit=kilojoule/mole)] [Quantity(value=0.0317, unit=elementary charge), Quantity(value=0.2649532787749369, unit=nanometer), Quantity(value=0.06568879999999999, unit=kilojoule/mole)] [Quantity(value=0.06, unit=elementary charge), Quantity(value=0.25, unit=nanometer), Quantity(value=0.12552, unit=kilojoule/mole)] [Quantity(value=0.0317, unit=elementary charge), Quantity(value=0.2649532787749369, unit=nanometer), Quantity(value=0.06568879999999999, unit=kilojoule/mole)] [Quantity(value=0.06, unit=elementary charge), Quantity(value=0.25, unit=nanometer), Quantity(value=0.12552, unit=kilojoule/mole)] [Quantity(value=0.0317, unit=elementary charge), Quantity(value=0.2649532787749369, unit=nanometer), Quantity(value=0.06568879999999999, unit=kilojoule/mole)] [Quantity(value=-0.18, unit=elementary charge), Quantity(value=0.35000000000000003, unit=nanometer), Quantity(value=0.276144, unit=kilojoule/mole)] [Quantity(value=-0.0941, unit=elementary charge), Quantity(value=0.3399669508423535, unit=nanometer), Quantity(value=0.4577296, unit=kilojoule/mole)] [Quantity(value=0.06, unit=elementary charge), Quantity(value=0.25, unit=nanometer), Quantity(value=0.12552, unit=kilojoule/mole)] [Quantity(value=0.0317, unit=elementary charge), Quantity(value=0.2649532787749369, unit=nanometer), Quantity(value=0.06568879999999999, unit=kilojoule/mole)] [Quantity(value=0.06, unit=elementary charge), Quantity(value=0.25, unit=nanometer), Quantity(value=0.12552, unit=kilojoule/mole)] [Quantity(value=0.0317, unit=elementary charge), Quantity(value=0.2649532787749369, unit=nanometer), Quantity(value=0.06568879999999999, unit=kilojoule/mole)] [Quantity(value=0.06, unit=elementary charge), Quantity(value=0.25, unit=nanometer), Quantity(value=0.12552, unit=kilojoule/mole)] [Quantity(value=0.0317, unit=elementary charge), Quantity(value=0.2649532787749369, unit=nanometer), Quantity(value=0.06568879999999999, unit=kilojoule/mole)] . Running some molecular dynamics simulations . We&#39;ve come this far in building our model with different force fields, we might as well build up the rest of the simulation. . openmm will be used to run our simulation, since we already have an openmm.System object. We need an integrator that describes our equations of motion, timestep, and temperature behavior. . As a side note, we forcibly made our simulation box really big to address cutoffs, but we can probably go with a smaller box that still fits the bill. The smaller box helps speed up the computation. . integrator = openmm.LangevinIntegrator(323 * unit.kelvin, 1.0/unit.picoseconds, 0.001 * unit.picoseconds) smallbox_vectors = [[2*unit.nanometer, 0*unit.nanometer, 0*unit.nanometer], [0*unit.nanometer, 2*unit.nanometer, 0*unit.nanometer], [0*unit.nanometer, 0* unit.nanometer, 2*unit.nanometer]] smirnoff_omm_system.setDefaultPeriodicBoxVectors(*smallbox_vectors) . We combine our openmm.Topology, openmm.System, and openmm.Integrator to make our openmm.Simulation, then set the positions . smirnoff_simulation = openmm.app.Simulation(omm_topology, smirnoff_omm_system, integrator) smirnoff_simulation.context.setPositions(xyz) . Before running the simulation, we need to report some information. Otherwise, the simulation&#39;s going to run and we won&#39;t have anything to show for it. This is handled in openmm by creating openmmm.reporters and attaching them to your openmm.Simulation We will write out the timeseries of coordinates (trajectory) in a dcd format, but also a pdb format to show a singular configuration. In this case, we&#39;re printing a pdb file that corresponds to the first configuration, before any simulation was run. . smirnoff_simulation.reporters.append(openmm.app.DCDReporter(&#39;trajectory.dcd&#39;, 10)) pdbreporter = openmm.app.PDBReporter(&#39;first_frame.pdb&#39;, 5000) pdbreporter.report(smirnoff_simulation, smirnoff_simulation.context.getState(-1)) . Now we can run our simulation! . smirnoff_simulation.step(1000) . After it&#39;s finished, we can load the trajectory files into an mdtraj.Trajectory object, and visualize in a jupyter notebook with nglview. From this mdtraj.Trajectory object, you have pythonic-access to all the coordinates over time, and also access to various analysis libraries within mdtraj. . The ethane is jumping around the boundaries of the periodic box, but you can see it wiggling. Unfortunately, markdown doesn&#39;t show NGL widgets, so I advise people to look at the notebook. Not super interesting, but simulations from open-source software are doable. If I had a more powerful computer, maybe I&#39;d try a larger system, but I&#39;ll leave it to others to build off of my notebook (you can find this in my website&#39;s git repo) . traj = mdtraj.load(&#39;trajectory.dcd&#39;, top=&#39;first_frame.pdb&#39;) nglview.show_mdtraj(traj) .",
            "url": "https://ahy3nz.github.io/fastpayges/molecular%20modeling/scientific%20computing/grad%20school/2019/10/15/openforcefield1.html",
            "relUrl": "/molecular%20modeling/scientific%20computing/grad%20school/2019/10/15/openforcefield1.html",
            "date": " ‚Ä¢ Oct 15, 2019"
        }
        
    
  
    
        ,"post17": {
            "title": "Molecular Modeling Software: Open Babel",
            "content": "&quot;Open Babel is a chemical toolbox designed to speak the many languages of chemical data. It&#39;s an open, collaborative project allowing anyone to search, convert, analyze, or store data from molecular modeling, chemistry, solid-state materials, biochemistry, or related areas.&quot; . Open Babel main page | Open Babel overview documentation | Open Babel git repo | . Open Babel is written in C++ and can be interacted with via the command line or C++ scripts, but I want to focus on the Python interface for Open Babel, pybel . Pybel documentation | Pybel in the repo | . Open Babel and pybel are available on pip and conda. . There seems to be an interesting history in which there was first Babel, open-sourced by others into OpenBabel, but shares some history with OELib/OEChem from OpenEye. . Given I&#39;m Python-centric, I like to dabble with pybel to interface with openbabel, and see where openbabel functionality could be useful (maybe from a molecular modelling perspective). . import openbabel import pybel . For starters, pybel provides functionality to read smiles strings, with some convenient 2D visualization within Jupyter. This is propane ($C_3H_8$). . Going further, openbabel supports a wide range of fileformats . molecule = pybel.readstring(&quot;smi&quot;, &#39;CCC&#39;) molecule . CH 3 H 3 C type(molecule) . pybel.Molecule . Within this pybel.Molecule object, we can access various other data. Notably, we can access the underlying openbabel.OBMol object as defined in the C source code. This is handled using the SWIG package, and this is our route to utilizing openbabel functionality within python . molecule.OBMol . &lt;openbabel.OBMol; proxy of &lt;Swig Object of type &#39;OpenBabel::OBMol *&#39; at 0x10e33e840&gt; &gt; . The openbabel.OBMol object is immense, so I refer readers to the API for an exhaustive list of getters, setters, and other functions relevant to molecular modelling and cheminformatics. . Openbabel ships with a variety of iterator objects to loop through various properties of openbabel.OBMol objects. Not as pythonic as trying to generators or enumeration, but it still works . for atom in openbabel.OBMolAtomIter(molecule.OBMol): print(atom.GetIndex(),atom.GetType(), atom.GetAtomicNum()) . 0 C3 6 1 C3 6 2 C3 6 . Since we created pybel.Molecule out of a SMILES string, the hydrogens were implicit and ignored. To fully enumerate this compound, we can add hydrogens . for atom in openbabel.OBMolAtomIter(molecule.OBMol): molecule.OBMol.AddHydrogens(atom) . molecule . H H H H H H H H for atom in openbabel.OBMolAtomIter(molecule.OBMol): print(&quot;{:&lt;4}{:&lt;4}{:&lt;4}&quot;.format(atom.GetIndex(), atom.GetType(), atom.GetAtomicNum())) . 0 C3 6 1 C3 6 2 C3 6 3 H 1 4 H 1 5 H 1 6 H 1 7 H 1 8 H 1 9 H 1 10 H 1 . We can also iterate through the bonds in our openbabel.Molecule object, and even get an idea for the equilibrium bond length . for bond in openbabel.OBMolBondIter(molecule.OBMol): print(&quot;{:&lt;5}{:&lt;5}{:&lt;5}&quot;.format( bond.GetBeginAtom().GetIndex(), bond.GetEndAtom().GetIndex(), bond.GetEquibLength())) . 0 1 1.52 1 2 1.52 0 3 1.07 0 4 1.07 0 5 1.07 1 6 1.07 1 7 1.07 2 8 1.07 2 9 1.07 2 10 1.07 . The angles . molecule.OBMol.FindAngles() for angle in openbabel.OBMolAngleIter(molecule.OBMol): print(angle) . (0, 1, 3) (0, 1, 4) (0, 1, 5) (0, 3, 4) (0, 3, 5) (0, 4, 5) (1, 0, 2) (1, 0, 6) (1, 0, 7) (1, 2, 6) (1, 2, 7) (1, 6, 7) (2, 1, 8) (2, 1, 9) (2, 1, 10) (2, 8, 9) (2, 8, 10) (2, 9, 10) . And the torsions (proper dihdrals, not improper) . molecule.OBMol.FindTorsions() for dih in openbabel.OBMolTorsionIter(molecule.OBMol): print(dih) . (3, 0, 1, 2) (3, 0, 1, 6) (3, 0, 1, 7) (4, 0, 1, 2) (4, 0, 1, 6) (4, 0, 1, 7) (5, 0, 1, 2) (5, 0, 1, 6) (5, 0, 1, 7) (0, 1, 2, 8) (0, 1, 2, 9) (0, 1, 2, 10) (6, 1, 2, 8) (6, 1, 2, 9) (6, 1, 2, 10) (7, 1, 2, 8) (7, 1, 2, 9) (7, 1, 2, 10) . SMARTS matching within openbabel . Every openbabel.OBAtom object has the function MatchesSMARTS. . Without referring to much to my earlier Foyer post, SMARTS matching helps identify atoms within a system that fit a particular SMARTS pattern (chemistry and bonding) . This next section will do some primitive atomtyping via SMARTS matching. . To start, we&#39;ll create a dictionary that relates atomtypes to their SMARTS definition (a boiled-down version of the Foyer FFXML). . smarts_definitions = { &#39;internal C&#39;:&#39;[C;X4]([C])([C])([H])[H]&#39;, &#39;external C&#39;:&#39;[C;X4]([C])([H])([H])[H]&#39;, &#39;external H&#39;:&#39;[H][C;X4]&#39;, &#39;internal H&#39;:&#39;[H]([C;X4]([C])([C]))&#39;, } . Next, we will iterate through all of our atoms. For each atom, we will iterate through each of our SMARTS definitions to see if there is a match . for atom in openbabel.OBMolAtomIter(molecule.OBMol): matched = False for label, smarts in smarts_definitions.items(): if atom.MatchesSMARTS(smarts): print(&quot;Atom {} is {}&quot;.format(atom.GetIndex(), label)) matched = True if not matched: print(&quot;Atom {} is unmatched&quot;.format(atom.GetIndex())) . Atom 0 is external C Atom 1 is internal C Atom 2 is external C Atom 3 is external H Atom 4 is external H Atom 5 is external H Atom 6 is external H Atom 6 is internal H Atom 7 is external H Atom 7 is internal H Atom 8 is external H Atom 9 is external H Atom 10 is external H . It&#39;s also worth noting that the SMARTS definitions in openbabel are slightly different from SMARTS in Foyer. This isn&#39;t so surprising since scientists are always devising their own takes on SMILES and SMARTS nowadays. Some other key differences: . Foyer can utilizes labels (when matching SMARTS patterns, we can also leverage the atoms for which we already know the atom type). The code above doesn&#39;t apply labels, nor would the SMARTS matching in openbabel know what to do with the labels. In this regard, it&#39;s helpful that Foyer has its own SMARTS matching engine | Foyer utilizes blacklists and whitelists to help identify multiple atom types while resolving to a singular atom type. | . Force fields . Openbabel has some support for molecular mechanics force fields (GAFF, UFF, MMFF94, MM2, ghemical). . Within Openbabel, these basic FFs can parametrize systems and also compute forces/energies. Some functional forms appear to include harmonic bonds, harmonic angles, periodic torsions, RB torsions, LJ nonbond interactions, harmonic impropers, and Coulombic interactions. These various computes are defined as classes within each FF, so it&#39;s a little different from OpenMM where there are broadly classes of different force objects for different FFs to utilizes when studying a system. For each force class&#39;s compute method, you pass the relevant parameters and measurables, and out pops the gradient or energy. . From these force fields, you can generate conformers, perform energy mimization, and even run molecular dynamics (with constraints and cutoffs) . Summary . Open babel is an extensive molecular modelling, cheminformatics, and bioinformatics package, and I&#39;m sure many computational chemists/biologists would find something useful within this library. As the name &quot;Babel&quot; suggests, it&#39;s related to all the many various languages that split from one, original language. The Open babel library has a lot of functionality and API that a C++ or Python (or CLI) developer could utilize in their own work. .",
            "url": "https://ahy3nz.github.io/fastpayges/molecular%20modeling/scientific%20computing/grad%20school/2019/10/03/openbabel1.html",
            "relUrl": "/molecular%20modeling/scientific%20computing/grad%20school/2019/10/03/openbabel1.html",
            "date": " ‚Ä¢ Oct 3, 2019"
        }
        
    
  
    
        ,"post18": {
            "title": "Molecular Modeling Software: Foyer",
            "content": "Foyer . Foyer is an open-source Python package, part of the MoSDeF suite of tools for molecular modeling. In the description: ‚Äúa package for atom-typing as well as applying and disseminating forcefields.‚Äù . Reading Check . If you don‚Äôt know what atom types or force field parameters are, I suggest reading this post to catch up. If you‚Äôre new to molecular modeling, try this or that . Overview . The basic idea behind Foyer is to assign atom types and force field (FF) parameters given an arbitrary chemical topology (bonding network). Anyone who has worked in modeling knows this is a daily frustration. Further, the process is aimed at being transparent - how each atomtype gets chosen, what the various force field parameters are - and easily share-able - a Python script that constructs your system and utilizes Foyer to atomtype and parametrize accroding to a share-able, human-readable force field file (XML-style). . For most molecular modelers, we rely on other software like antechamber, ligpargen, charmm-gui, cgenff, etc. to atomtype and parametrize. In goes the structure, out come the FF parameters, and pray it works. Fairly black box, no flexible API to modify the process. For as complicated as we will see the Foyer API to be, the underlying FF parameters and atom types are easy to debug and modify. As it stands (September 2019), there is good support for the OPLS-AA force field provided with Foyer, and more in development. Contributions from others are always welcome, especially trying to get the more general force fields codified and shared. . This post is going to start from general Foyer API and work flow to finer-detailed explanations of particular routines. So hop on the train and feel free to hop off as you see fit. For users, you might not need to read the whole thing. For debuggers, developers, and maintainers, it might help to stay to the end. If you wanted code-examples and tutorials, I recommend you look through the MoSDeF organization and Foyer repos. There are lots. If you wanted the big ideas and concepts, I refer you to the Credits at the end of this post for the publication and related manuscript. My aim is to help elucidate the code design and each routine. . The Foyer force field XML . A molecular mechanics force field defines the atoms and atom types within a chemical system. The Foyer force field XML derives and builds upon the OpenMM force field XML Furthermore, the interaction parameters are also specified (bonds, angles, dihedrals, nonbonded interactions). The available functional forms and units are based on OpenMM style. Foyer-supported parameters are here . Below is a small example of an XML file . The heading &lt;AtomTypes&gt; defines the different atom types within the FF. The attribute name defines the name of the atom type, and it is unique. No other atom types can have the same name. The attribute class defines a broad category that can encompass multiple atom types (notice how there are multiple class=&quot;CT&quot;). Multiple atom types can belong to the same class, and this can help down the line if you want to assign parameters to an entire class, not just a specific, unique atom type. Mass is self-explanatory, amu. desc is a human-readable description to help understand the atomtype. There is also an additional doi attribute, which is useful for citing the relevant papers where those FF parameters were published. . def and overrides are where Foyer really makes its money. Foyer-documentation here. def refers to a SMARTS string that defines the bonding and chemical context of the atom of interest. At its most basic, you could use SMARTS strings to uniquely define sp3, sp2, and sp hybridized carbons (4, 3, and 2 bonded partners, respectively). SMARTS is not developed by the MoSDeF group, but rather utilized by us. SMARTS is similar to SMILES in that they are strings that define chemistry, but SMILES defines molecules; SMARTS defines patterns. SMARTS theory here. There is a lot of chemistry/bonding that can be defined via SMARTS, only a subset is supported in Foyer so far. In the image posted above, many definitions are missing, but the ones that are filled-in will describe alkane CH3, alkane H, alkene C, and alkene H. Based on SMARTS definitions, Foyer detects all the possible SMARTS pattern matches for every atom. . SMARTS-strings can be as general or specific as you want. This is helpful for implementing some force fields, or developing a new atom type that should only be used in a very particular molecular pattern. If a particular atom satisfies multiple SMARTS definitions, and thus could be multiple atom types, Foyer helps narrow down to a single atom type via overrides. No easy examples to show, but here you can find oplsaa.xml with examples of overrides. Here‚Äôs a use-case, you have a carbon that fits atomtypes C1, C2, C3, and C4. But you also have overrides definied, such that C2 overrides C1, C3 overrides C2, and C4 overrides C3. Alternatively, you could have C4 override C3, C2, and C1. Because of this priority/precedence, your carbon will get defined as C4. . The remaining sections within the FFXML file outline the different forces that this FF implements. Again, the Foyer-supported parameters are here, and we derive these objects/forces from OpenMM-style here. If you wanted to incorporate a new functional term, it would have to abide by the OpenMM conventions prior to being implemented within Foyer. . Foyer Forcefield class . We‚Äôve outlined the FFXML, but now we need to create an object from this text. This is where the Forcefield class comes into play. The class is defined in here. The Foyer FF class inherits from the OpenMM FF Class, so some functionality gets re-used (one example is the parameter assignment is taken from the OpenMM FF). Within the Foyer FF class, there is the Foyer-specific information like definitions, overrides, descriptions, dois, etc. Within the OpenMM FF class (the superclass), there is the OpenMM information like the atom types and forces, among other fields that don‚Äôt get utilized in Foyer. . In addition to creating digitizing the FFXML into a FF object, the forces also described in the FFXML also get turned into their own Force objects. . In actuality, the forces described in the FFXML get turned into ForceGenerators like this. Further down the pipeline/workflow, these generators will later create their respective Force objects, which are found elsewhere in the OpenMM C code. . In summary, the Foyer FF class turns the FFXML into a data structure that contains information about the atomtypes, classes, forces, and everything else that went into the FFXML. . Applying a Foyer FF . We now have our FF object with which we can interact using Foyer‚Äôs API. The next thing is to take this FF and apply it to our chemical system of choice, identifying the atom types and interaction parameters according to the FF, thus building our molecular model. Correspondingly, the method is Forcefield.apply . The apply step can be broken into 4 steps . FF Applying, step 1: Preparing a topology . To apply a force field, you need to provide a chemical topology. This topology needs to know the elements of all the atoms/particles in your system, and all the bonds between them. There are MANY flavors of a chemical topology, but they all end up converted to an OpenMM.Topology. Additional information in the OpenMM.Topology include residues and chains. Thus far, only residue functionality is encoded; while residues are technically associated with proteins and amino acids, residues can also generically refer to a molecule type in our system (if you have a box of ethanes and methanes, then all the ethanes would be of the ethane residue, even if you have 10 distinct ethane molecules) . FF Applying, step 2: Atomtyping . After consolidating any chemical topology input into an OpenMM.Topology, we now iterate through the atoms to identify each atom‚Äôs atom type. This is the majority of the Foyer API. The gist is that we try to find all the SMARTS patterns and atom types that fit each atom in our topology. Using overrides functionality, we help narrow down a list of possible atom types to a single one. This is often a pain point in the Foyer workflow, as sometimes overrides or SMARTS definitions are poorly defined. If overrides are not sufficiently defined, narrowing down the possible atom types to just one will not work. If SMARTS definitions are not sufficiently defined, some atoms in your topology will not fit any atom type as defined in your FFXML (or maybe something was wrong with your chemical topology). . If you are interested in digging deeper into the Foyer atomtyping API, there will be sections later in this post. . FF Applying, step 3: Parametrize the forces . Once all atom types have been detected within our topology, we use some functionality from OpenMM to parametrize all the interactions within our chemical system. . Remembering that the FFXML -&gt; FF object actually conatined ForceGenerators, not the actual Forces, this step now creates the Forces and identifies all the relevant atoms, bonds, angles, dihedrals, etc. to which that Force applies. . This step yields an OpenMM.System object. Within the Foyer workflow, this gets converted to a ParmEd.Structure. . FF Applying, (optional) step 4: Validation . When building a molecular model, sanity checks are necessary. Do all bonds have parameters defined? What about angles and dihedrals? Foyer includes functionality to sanity check the resultant parametrized ParmEd.Structure. As a note, most (if not all) FFs define bond interactions (two-atom partners directly bonded). Angles and dihedrals may not always be defined or parametrized within that FF, and that‚Äôs okay if that‚Äôs what the FF correctly states. . Additionally, if the FFXML had DOIs associated with it, the DOIs get written to a bibtex file, helpful for properly citing and crediting authors of the FF. . Done applying a Foyer FF . At this point, we have a parametrized ParmEd.Structure object, in which all atoms have atom types, and all physical interactions are defined with parameters (provided they were specified in the FFXML). From this ParmEd.Structure we perform a variety of different data structure conversions or I/O to different file formats suitable to the engine of choice. . If you are using or interested in using Foyer, hopefully this is enough design and implementation detail to get you comfortable with the API. The goal is to provide a tool that helps atomtype and parametrize arbitrary chemical systems in a manner that users can walk through, debug, and fine-tune a force field as necessary. . An in-depth dive into the Foyer API . If you are a maintainer or developer of Foyer, this will hopefully help you get a better sense of how the code works, as a lot of the internal API is hidden from users. We will, once again, walk through the Foyer API and workflow, but with a lot more detail. . I want to make something very clear: I hope this serves as a helpful reference for people getting familiar with the entire API, but a well-trained developer should eventually outgrow this post and hopefully be able to write their own walkthroughs, as this will demonstrate, exercise, and force the developer‚Äôs own fluency with the Foyer API. . More about the Foyer FFXML and Forcefield . The FFXML does follow the XML structure, and can be manipulated using lxml or xml Python libaries. . If you are writing your own FFXML file, creating a Forcefield object includes a ‚Äúvalidation‚Äù step, checking to see if your own FFXML fits the style laid out in Foyer. Programatically, this is done as an XML schema document here. Most of time, you will not have to interact with this document unless you are incorporating a new functional form within Foyer and your Foyer FFXML. The general idea is for every XML element, you specify possible attributes and their data types. If there are nested XML elements, you can specify those too. . After the validation step, there are some initializing operations. These initializing operations include processing the FFXML files using some regex operations, and calling the superclass (OpenMM FF) __init__ function. This is an interesting use of Python and its super/subclass functionality, on this line, we call the superclass‚Äôs __init__, which calls OpenMM.Forcefield.loadFile, which calls OpenMM.Forcefield.registerAtomType. All the OpenMM FF stuff can be found here. BUT, we are NOT actually using OpenMM.Forcefield.registerAtomType, we have overridden this function with Foyer.Forcefield.registerAtomType, which is written here. So there‚Äôs a lot of super/subclass interaction and function overriding. . I bring this up because this is where Foyer identifies the non_element_types (names of particles that don‚Äôt fit within the periodic table of elements). This is useful for systems that introduce new particles, like coarse-grained methods (‚Äú_myBead‚Äù vs ‚ÄúmyAtom‚Äù). The difference between elemental and non-elemental is denoted by the use of an underscore before the actual name of your particle. In reality, this is mainly done because a lot of elemental detection is done by inferring from the particle name and elemental symbol. . Lastly when creating the Foyer.FF object, we initialize the SMARTS Parser, and pass the non_element_types we detected when reading in the FFXML. . The SMARTS Parser . SMARTS as a human-readable language designed to encode chemical and stereochemical bonding patterns in text. Now that it‚Äôs human-readable, we need to develop a ‚Äúgrammar system‚Äù for a computer program to decipher SMARTS strings. This is handled within our SMARTS class here. The file is short, but there‚Äôs some key pieces of information here. First, we create a really long string GRAMMAR that encodes all the different symbols and characters that could yield a valid SMARTS string. Some symbols are operators that denote relationships or properties, others are terminals that are the objects that comprise these relationships. In mathematical examples, parentheses, additions, subtractions, etc. are relationships or properties, but the numbers or variables themselves are the terminals. To read more about this, I direct you to the Wikipedia article about LALR parsing. Computer science grammar parses are a wicked rabbit hole. Within GRAMMAR, we also have some string formatting - notice the last line in the GRAMMAR string is a long string of symbols. These are atomic symbols, just disguised. C[laroudsemf] will refer to the symbol C, Cl, Ca, Cr, etc. But there‚Äôs also the curly braces {optional}, which is where we will do some string formatting and insert the non_element_types. . Second, there is the actual SMARTS object whose property is this GRAMMAR string, but now we‚Äôve formatted the string to fill in the {optional} symbols with the non_element_types we found earlier. If you‚Äôre going to perform SMARTS-matching, you need to make sure all your symbols (elemental or not) get captured within the SMARTS object, which is created when we read in the FFXML and registerAtomTypes. . Lastly, we have a property for the self.PARSER, which utilizes an external library, to take the defined grammar rules and create abstract syntax trees which are used to digest and process the SMARTS strings within the FFXML. . FF Applying, step 1: Preparing a topology . Thus far in the advanced section, we have discused what happens when we write an FFXML, read an FFXML, and create the FF object from the FFXML, including a discussion on the SMARTS object and the grammar used in analyzing SMARTS patterns. At this point, we have a FF object. . Now, we move onto the apply step. First, we prepare the topology, where we aim to create an OpenMM.Topology object out of whatever input we were initially given. Within the apply function, we call _prepare_topology: if we have an OpenMM.Topology, we are already done. In addition, we erase the positions only to re-add them later. If we do not have an OpenMM.Topology, we call generate_topology, which converts parmed.Structure or mbuild.Compound objects into OpenMM.Topology objects by extracting the bonding and particle information from each. Editorializing, but there are a million and one different chemical topology objects that encapsulate a lot of the same core information about bonding and particle information, it‚Äôs just a matter of getting the API to work with each other and writing the converters. Once this idea is well-ingrained, then it‚Äôs somewhat easy to interconvert between all the different chemical topology objects, provided you learn the two API involved. . At this point, we have created an OpenMM.Topology object by parsing and converting bonding and particle information. Now, we move to the atomtyping step. . FF Applying, step 2: Atomtyping . At a glance, it seems like all we call is self.run_atomtyping, but there are many calls underneath that. The gist is that we can map each particle in our system to an atom type. . In run_atomtyping, we call a couple methods. One method involves residue mapping, which is largely template matching. If you already found the atom types for one ethane, you can quickly copy that atomtyping information to the other ethanes in your system without having to re-run all the SMARTS matching. The other method is find_atomtypes, whose code is found in another file here, atomtyper. . Within the entire atomtyping process, we pass along a typemap dictionary that relates the atom‚Äôs index to its blacklist and whitelist. For a given atom (its index), we keep track of the overrides in the blacklist, and the possible atom types in the whitelist. This is used at the end of the atomtyping process, but we need to pass this dictionary around the various methods and sub-methods within Foyer. . find_atomtypes starts by calling _load_rules. Before I explain the function, I will describe the change in terminology. . A rule is a general umbrella term that refers to the atom type and its associated SMARTS pattern. | A rule_name is the name of atom type. | A SMARTSGraph is the digitized graph of the SMARTS pattern | SMARTS is the SMARTS pattern in our FFXML within the def attribute, also referred to as the SMARTS string or SMARTS definition | . Within _load_rules, we iterate through our FF object. Specifically, for every atomTypeDefinition, we get the rule_name and SMARTS. We convert the SMARTS into a SMARTSGraph, then store this information in a dictionary called rules, which relates the rule_name to the SMARTSGraph . Sidenote: The SMARTSGraph . The SMARTSGraph is a subclass of the NetworkX.Graph, the class is defined here, in smarts_graph.py. A SMARTSGraph object contains information about the atom type and SMARTS string it encodes - the rule_name, overrides, and the graphical representation of bonding topology (nodes are particles, edges are bonds). Additionally, the SMARTSGraph contains the abstract syntax tree that is generated when our FF.parser (the LALR parser with the GRAMMAR from earlier) parses the SMARTS pattern‚Äôs grammar into the abstract syntax tree. For each atom type, we will have a SMARTSGraph object. . Within this module and class are all the subgraph matching operations used to identify the possible atom types. At this point in the post, we do not need to cover the different operations here, just the properties and significance of the SMARTSGraph. . Back to atomtyping . Before this tangent, we were talking about the load_rules method, which is called from find_atomtypes. When we load rules, we are creating a dictionary that relates the rule_name (atom type string) to its SMARTSGraph (graph-ified SMARTS pattern). Now we have this dictionary, we return to find_atomtypes. . Even though we have created a rule for every atom type outlined in the FFXML, we can do some simple acceleration to the atomtyping, which involves the simplest SMARTS patterns whose only pattern defines a singular atom type with no bonding context. This is the ‚Äúlow hanging fruit‚Äù to match within a FF, so this can be handled quickly. Furthermore, if there are elements in the FFXML, but those elements are not present within our chemical topology, we can toss those elements and their associated rules out the window - that element is not in our topology, so why should we even try to match something that definitely isn‚Äôt present? This pruning process helps narrow down large FFXMLs into smaller, faster-to-match FFs. . Now, we call _iterate_rules, whose code is outlined here. In this function, we iterate through each rule (SMARTS definition) and find the atoms the fit this rule, and iterate through those fitted-atoms. For the fitted-atoms (atoms that fit this SMARTS definition), we add the rule_name (atom type) to the whitelist, and add the overrides to the blacklist. . We will focus on rule.find_matches, because this goes into the graph matching. Remember, a rule is a SMARTSGraph object, and find_matches can be found here . First, _prepare_atoms involves initialzing typemap properties and looking at ring sizes. _prepare_atoms calls _find_chordless_cycles We have a bonding network, but part of SMARTS involves identifying the size of a ring. Within _find_chordless_cycles, we look at each node within our graph (atom within our system), and aim to build out a cycle by looking at the atom-in-question‚Äôs neighbors, and then look at those neighbors. We repeat this process until we can actually close the ring. Side note: chordless means a ring without any bridges within the cyclic strucure. I think chordless poses some issues with ring finding, as a ring with a chord could actually yield multiple rings (the rings that use the chord to close the loop, or the rings that skip the chord) . After _prepare_atoms, we create the topological graph, a NetworkX.Graph, from our OpenMM.Topology. . For each rule (a graph), we create a SMARTSMatcher object as a property. In the workflow of graph matching, NetworkX defines various graph matching algorithms as Python classes, and our SMARTSMatcher class inherits from the NetworkX VF2 algorithm. The NetworkX documentation is here. What this means is that our SMARTSMatcher class takes some of the functions from the parent class, but now we override them to fit our chemical matching problem. It‚Äôs important to pair these rules and SMARTSMatcher objects together such that the SMARTSMatcher knows what it is trying to match. . While we create the SMARTSMatcher object, we have to define the node_match, saying when we‚Äôve actually found matching nodes in corresponding graphs. . Defining a node match via atom_expression and atom_id matches . node_match is a method within the SMARTSGraph class that gets passed into the SMARTSMatcher.__init__, and can be found here. Compare a host and pattern node, check if their expressions match (atom_expr_matches). . atom_expr_matches is another method within the SMARTSGraph class that is found here. At this point, we are utilizing some strings that were outlined and parsed from our SMARTS grammar. This is a lot of if-statements that analyze the expressions as defined within our abstract syntax trees (the AST that gets created when we parsed the grammar for each SMARTS definition). The expressions refer to some sort of operation or relationship between nodes. Similar to mathematical order of operations, we are trying to ‚Äúunwrap‚Äù these different operations and operations-within-operations to get to the terminal point of comparison. If our AST is very complex with a lot of nested expressions (in math, this would be lots of parenthesis within parentheses), we will end up calling atom_expr_matches a bunch to dig through the layers of our AST onion. Some possible AST nested expressions could involve chemical branching in compounds or and/or logical expressions. . Eventually, we reach the terminal end point, where we call self._atom_id_matches, a method defined here. Within this routine, we are looking at the specifics of our atom-in-question: . What is the label? Symbol? Atomic Number? | How many neighbors does it have? | Is it in a ring? If so, how big should the ring be? | . atom_id_matches doesn‚Äôt just return the output of another routine, atom_id_matches is the final routine (like base case in recursion) that will finally return you a True or False. Once this method returns a boolean, this boolean propagates back up through all the different atom_expr_matches routines, which FINALLY gives you something that _node_match can finally return True or False. . After defining the match criteria, we have to iterate . Back to the _find_matches method, we have now reached this loop. The technical jargon is we are performing subgraph isomorphisms. For our chemical system, we have a huge topological graph. For each SMARTS definition or rule, we have a smaller subgraph. The aim of the subgraph isomoprhism is to identify where, within the topological graph, the subgraph may fit. This is the technical jargon of finding the atom types in our topology, just cast into a graph problem. The logic and code of finding a match was outlined in the section above. Most of the subgraph matching is handled within the line that actually declares the loop self._graph_matcher.subgraph_isomorphisms_iter. Some of code may not be found within Foyer, because we are inheriting code and routines from the API in NetworkX, though some do get overridden like how we define node_matches or how we iterate in the VF2 algorithm. . Remember that the purpose of this subgraph isomorphism iteration, which is within find_matches, which is within atomtyper._iterate_rules, is to return or yield an atom index for which we can modify the white and blacklists. . Returning back out to iterating rules . Summarizing, we are iterating through every rule, trying to find matches via subgraph isomorphisms. The previous sections laid out how we utilize NetworkX functionality to utilize the VF2 algorithm for subgraph matching, which is all utilized in iterate_rules when we call rule.find_matches, where the subgraph matching code is used for the rules (SMARTSGraph objects). . Once we HAVE found matching atoms, we add to their white and blacklists. The possible atomtypes go in the white list for that atom, and the overrides associated with those atomtypes go in the black list. . All this information gets stored in the typemap, and then we return back out to find_atomtypes . Resolving atomtypes . After iterating through all our rules, finding atom type matches, updating white lists, and upating blacklists, we now have to resolve the atom types to specifyc a single atom type for each atom. . At this point in the code, we are here. In resolve_atomtypes, we iterate through every atom in our typemap. For each atom, we subtract the blacklist from the whitelist. Our whitelist lists all the possible atom types, and the blacklist lists all the overriden atom types. Doing this (set) subtraction, there should only be one remaining atom type. For example, our atomtyping process could yield a whitelist of C1, C2, C3, and C4. Also in this process, we have identified the overrides to be C3, C2, C1. Thus, the remaining C4 takes priority over all the other atom types, and this atom gets atomtyped as C4. . If the set difference doesn‚Äôt yield one remaining atom type, it means . There were multiple atom types that fit this atom. There were were insufficient overrides to narrow down the whitelist to just one atom type. As a FF developer, you might have just stumbled across an atom that you didn‚Äôt expect to satisfy multiple atom types, and now you need to update your FF for a new override. | There were insufficient atom types and none fit the atom. Your chemical system in question has a particular chemical context not yet accounted for within the FF and SMARTS definitions. If you KNOW which atom type it should have been, you need to go back and update the SMARTS definition in your FFXML for that atom type. If you did NOT know which atom type it should have been, you need to make a new atom type and enumerate FF parameters for it within your FFXML. | . After resolving within our typemap, we have to apply this mapping scheme back to our OpenMM.Topology, where we are finally assigning the atom types. . Parametrizing the system . If the code made it this far, we know the atom types for each system. We now need to parametrize the bonded and nonbonded interactions within our system, according to the FF. . This is done here, where we are adopting a lot of code from OpenMM.Forcefield.createSystem. Within Foyer.Forcefield.createSystem, we identify all the possible bonds, angles, and dihedrals in our system, which updates data and self._systemData . After identifying the bonds, etc. to parametrize, then we actually parametrize them. Back when we were initializing our FF and reading the FFXML, we didn‚Äôt actually create the Force objects, but rather ForceGenerators. The ForceGenerators contained the parameter information, but never said which bonded entity had that parameter information. This is done here, where we call force.createForce, where we are unpacking all the parameters and assigning them to each bond, angle, and dihedral within our system. force.createForce is a method that all OpenMM.ForceGenerator objects know (like here). Within this code, you can see where the ForceGenerator is now adding particular information, specifying the atoms involved, the reference length, and the force constant. . At this point, we have a fully parametrized OpenMM.System - all atoms have atom types, all bonded identities have been found, and all bonded (and nonbonded) entities have parameters associated with them. This would actually be sufficient to run a simulation with OpenMM, but our code seeks flexibility across many engines. . As a side note for Urey-Bradleys (angle terms with a 1-3 bond), the OpenMM code appears to create separate angle forces and separate bonds. There ends up being a singular HarmonicBondForce that encapsulates both 1-2 and 1-3 bonds, but this messes up some logic later in the code. So we create a second HarmonicBondForce that removes the 1-3 from the first HarmonicBondForce, and puts the 1-3 bonds in the second HarmonicBondForce. . Returning to the OpenMM.System, we want to make this flexible across different engines, so we use Parmed to convert the OpenMM.System into a Parmed.Structure here. From the parmed.Structure, we can utilize the ParmEd API to write to different simulation engines, or a user can easily dump out the contents of the parmed.Structure in a format for a simulation engine of choice - all the parametrized, topological, atomic, and position information is there, find how to access it and then find out how you should probably write to a file format. . FF Applying, (optional) step 4: Validation . Historically, modelers had to be extremely meticulous to hand-define every bond, every angle, and every parameter in their system, each time they wanted to perform a simulation. Because this is automated by the Foyer API, it helps to incorporate sanity checks to make sure everything is defined. There are some validation checks here, back when we enumerated all bonds and angles in systemData, we now compare them to the bonds and angles in the parmed.Structure. If it was parametrized, it would carry over into the parmed.Structure. If it wasn‚Äôt, then there would be something in systemData not in the parmed.Structure. . This is the bare-bones sanity check for building a model, actual validation of the parameters and numbers themselves is harder to do without human eyes scrutinizing everything. . After the Foyer workflow . We have a fully-parametrized parmed.Structure. If we don‚Äôt, we should hopefully know where in the process we missed things - ill-defined SMARTS patterns, missing atomtypes or definitions, missing parameters, poorly-defined bonding, or poorly-defined particle names, among others. . Now we have the model and need to run a simulation, which will require: . Writing/converting the model (parmed.Structure) to a form that fits your simulation engine | Writing the simulation parameters (integrators, timesteps, thermostats, barostats, nonbonded instructions, constraints, cutoffs), if not already specified in the model. | . In addition to the various formats supported in ParmEd, mBuild also has additional formats that extract information from the ParmEd structure here . Conclusion . To varying levels of detail, I have described the Foyer API, the routines, hidden routines, classes, and how the routines interact or call each other. There is a wealth of functionality derived from mBuild, ParmEd, OpenMM, and NetworkX, so it may help to understand those API as well. . Many Foyer modules and functions are hidden away for users, but the routines are sufficiently modular and well-defined for someone to hopefully understand issues and submit pull requests to build the API into an even more robust tool for molecular modelers. . As Foyer changes, this guide may become out-dated, but until then, I hope this serves as a useful tool for new users, maintainers, and developers. . Credits . I am not the original developer of Foyer. I help maintain and develop the package (as of September 2019 at least). Christoph Klein and Janos Sallai are the original developers, and I advise others who want to truly understand the package to read Christoph Klein‚Äôs Ph.D. Thesis at Vanderbilt University. The relevant publication can be found at DOI:&lt;10.1016/j.commatsci.2019.05.026&gt;. Further, this package was largely built upon the contributions of others at Vanderbilt University and other institutions. .",
            "url": "https://ahy3nz.github.io/fastpayges/molecular%20modeling/scientific%20computing/grad%20school/2019/09/27/foyer.html",
            "relUrl": "/molecular%20modeling/scientific%20computing/grad%20school/2019/09/27/foyer.html",
            "date": " ‚Ä¢ Sep 27, 2019"
        }
        
    
  
    
        ,"post19": {
            "title": "Fantasy NBA 2",
            "content": "from pprint import pprint import numpy as np import pandas as pd import matplotlib %matplotlib inline import matplotlib.pyplot as plt import scipy from scipy.stats import expon, skewnorm, norm import nba_api from nba_api.stats.static import teams, players from nba_api.stats.endpoints import shotchartdetail, playercareerstats, playergamelog import ballDontLie from ballDontLie.util.api_nba import find_player_id from ballDontLie.util.fantasy import compute_fantasy_points . seasons_range = [&#39;2018-19&#39;, &#39;2017-18&#39;, &#39;2016-17&#39;, &#39;2015-16&#39;] players_range = [&#39;Anthony Davis&#39;, &#39;James Harden&#39;, &#39;Stephen Curry&#39;, &#39;Giannis Antetokounmpo&#39;, &#39;Karl-Anthony Towns&#39;, &#39;Nikola Jokic&#39;, &#39;Joel Embiid&#39;, &#39;Paul George&#39;, &#39;Kawhi Leonard&#39;, &#39;Damian Lillard&#39;, &#39;Jimmy Butler&#39;, &#39;LeBron James&#39;, &quot;Bradley Beal&quot;] player_id_map = {a: find_player_id(a) for a in players_range} . For the various players and the various seasons, let&#39;s look at the distributions of some of their box stats . for player, player_id in player_id_map.items(): fig, ax = plt.subplots(1,1) df = pd.read_csv(&#39;data/{}.csv&#39;.format(player.replace(&quot; &quot;,&quot;&quot;))) df.hist(column=[&#39;FGM&#39;, &#39;FGA&#39;, &#39;FTM&#39;, &#39;FTA&#39;, &quot;REB&quot;, &#39;AST&#39;, &#39;STL&#39;, &#39;BLK&#39;, &quot;PTS&quot;], ax=ax) fig.suptitle(player) . /Users/ayang41/anaconda3/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3296: UserWarning: To output multiple subplots, the figure containing the passed axes is being cleared exec(code_obj, self.user_global_ns, self.user_ns) . I&#39;m going off by what these distributions sort of look like over all the players: . AST: Skewed normal | BLK: Exponential | FGA: Normal | FGM: Normal | FTA: Skewed normal | FTM: Skewed normal | PTS: Normal | REB: Skewed normal | STL: Skewed normal | . For all players, I&#39;m going to model each box stat as such. Given the gamelog data (blue), fit the model to that data, generate some values with that model (orange), and compare to the actual gamelog data. . Some comments: . For the &quot;bigger&quot; numbers like PTS, FGA, FGM, REB, the model distributions fit pretty well. . For the &quot;smaller&quot; numbers like BLK or STL (a player will usually have 0, 1, 2, 3, or maybe 4 of that stat) - these numbers are more discrete than the &quot;bigger numbers&quot;. If you can score points between 0 and 40, each actually reported points behaves more continuously since there is more variety. . From earlier work with PyMC for Bayesian probability modeling, I could have tried using PyMC to sample parameters for each stat-distribution, rather than just do a singular fitting. While that could help report a variety of parameters for each stat-distribution in addition to a sense of variation or uncertainty, I don&#39;t think it&#39;s super necessary to really venture into exploring the different distributions and their parameters that could fit each box stat; the fitting schemes via scipy seem to work well. . It&#39;s possible there are better models to fit some of the data - I can&#39;t say my brain-database of statistical models is extensive, so I just kinda perused through scipy.stats. . Fitting a distribution helps formalize how much a player&#39;s game can vary (is he consistently a 20ppg player? Or are is he hot and cold between 10 and 30 ppg?) Furthermore, if a player is out (injured or some other reason), that implicitly gets captured by a gamelog of 0pts, 0reb, etc. This is definitely important in fantasy because some may value a more reliable/consistent player who will show up to 80/82 games rather than a glass weapon who could drop 50 points, but will only play 40-50/82 games . These distributions assume we can ignore: coaching changes, team roster changes, and maybe player development. For player development, a younger player between 2015-2019 will demonstrate huge variance in two ways - young players are inconsistent game-to-game, but young players can also develop rapidly season-by-season. At the very least, these distributions try to describe variance, which shows room where a young player could go off or bust on a given night. Factoring season-by-season improvement will be hard - one would need to try to forecast a player&#39;s future stats rather than draw samples from a &quot;fixed&quot; distribution based on previous stats . stat_model_map = {&quot;AST&quot;: skewnorm, &quot;BLK&quot;: expon, &quot;FGA&quot;: norm, &quot;FGM&quot;: norm, &quot;FTA&quot;: skewnorm, &quot;FTM&quot;: skewnorm, &quot;PTS&quot;: norm, &quot;REB&quot;: skewnorm, &quot;STL&quot;: skewnorm} . for player, player_id in player_id_map.items(): fig, axarray = plt.subplots(3,3) df = pd.read_csv(&#39;data/{}.csv&#39;.format(player.replace(&quot; &quot;,&quot;&quot;))) for i, (stat, model) in enumerate(stat_model_map.items()): row = i // 3 col = i % 3 axarray[row, col].hist(df[stat], alpha=0.3) axarray[row, col].set_title(stat) params = model.fit(df[stat]) axarray[row, col].hist(model.rvs(*params, size=len(df[stat])), alpha=0.3) fig.suptitle(player) fig.tight_layout() . At this point, for each player and box stat, we have a distribution that can describe their game-by-game performance. Maybe we can sample from this distribution 82 times (82 games per season) to get an idea of the fantasy points they&#39;ll yield (the fantasy points will depend on the league settings and how each league weights the box stats). . To simulate a season for a player, we will model the distribution for each box stat, and sample from it 82 times. This is our simulated season. . simulated_season = pd.DataFrame() for player, player_id in player_id_map.items(): df = pd.read_csv(&#39;data/{}.csv&#39;.format(player.replace(&quot; &quot;,&quot;&quot;))) simulated_player_log = {} for stat, model in stat_model_map.items(): params = model.fit(df[stat]) sample = model.rvs(*params, size=82) simulated_player_log[stat] = sample simulated_player_log_series = pd.Series(data=simulated_player_log, name=player) simulated_season = simulated_season.append(simulated_player_log_series) . In addition to getting an 82-list of ast, blk, fga, etc. We can compute an 82-list of fantasy points (point values will depend on the league, but the default args for compute_fantasy_points are pulled from ESPN head-to-head points league default categories . simulated_season = compute_fantasy_points(simulated_season) simulated_season . AST BLK FGA FGM FTA FTM PTS REB STL FP . Anthony Davis [5.9597699533362665, 3.218551371084111, 1.1022... | [8.578394640988105, 0.026198088049728254, 1.84... | [20.21599540596214, 19.100646432695086, 26.528... | [13.872698337473729, 7.918853515554012, 9.5048... | [14.873567185676178, 9.685624594656804, 10.241... | [1.0995020882840587, 7.294623211426735, 7.2877... | [30.67865219506473, 15.260082139070928, 17.769... | [13.498196895282199, 5.04028872591627, 16.2989... | [1.1347148361527453, 1.7312487922029391, 1.068... | [39.732366354943515, 11.703574815952832, 18.10... | . James Harden [6.725125502991835, 16.337635303737578, 8.6225... | [0.05211498826964095, 0.5114333703840529, 0.08... | [31.451667457570714, 15.992361996925819, 19.37... | [9.307080381863589, 8.033940758354536, 11.1817... | [12.808604943217352, 12.428544803014987, 11.82... | [-0.06498946030802255, 8.515864278527303, 3.23... | [20.97714144699512, 32.02286792578204, 22.5610... | [6.9093167207067765, 7.988699867679405, 5.5192... | [0.32632320180968244, 0.6531429770923043, 2.89... | [-0.0281596184594477, 45.64267768161641, 22.89... | . Stephen Curry [12.279924280581394, 7.879981071011731, 5.9023... | [0.06292945230558766, 0.09033987679766506, 0.1... | [28.850986709915357, 24.43691542042258, 17.400... | [12.472086572331825, 9.555215666190444, 8.2599... | [12.977887030692326, 3.1789263709957845, -0.86... | [-0.010198935847337554, 3.516792829956679, 6.6... | [32.67407066270164, 24.97462588841426, 29.3427... | [3.9327364866510104, 7.6111446721103615, 4.726... | [3.590539393208442, 1.7898067047882282, 2.1206... | [23.173214171324872, 27.802064917851006, 40.60... | . Giannis Antetokounmpo [0.028588769334945585, 3.262013551923701, 3.40... | [1.9174837837589362, 6.695156990323455, 0.2378... | [7.172387181137829, 15.12244530666314, 20.4210... | [10.4727740339432, 16.131669525478472, 9.71417... | [11.746787857935951, 5.523726797994956, 15.646... | [8.191304596222707, 1.8995774627238957, 9.6062... | [38.321060006893674, 17.906593672693283, 7.885... | [19.219901699676136, 9.067580346050558, 9.3800... | [0.19042746270131458, 0.8734312501637576, 1.68... | [59.422365313457135, 35.189850694699025, 5.842... | . Karl-Anthony Towns [4.2665586948098095, 2.239352382015603, 2.8486... | [1.5142034413304508, 0.5455258265042157, 0.238... | [16.282437483033767, 16.64803799271481, 15.635... | [6.438364846075592, 9.772816680970095, 12.7305... | [-0.7312367428381779, 8.855766357633657, 7.839... | [12.274313450703058, 3.407774443197603, 1.1792... | [39.024821580722396, 18.827116373850785, 6.703... | [13.494959487083559, 6.618974662978514, 13.515... | [1.1278422771833472, 0.969599487392581, 1.1214... | [62.589863037712625, 16.87735550656093, 14.862... | . Nikola Jokic [2.2495283329958697, 4.313187987247313, 13.591... | [0.12814150793734586, 1.0322534945133215, 0.22... | [15.985739566133859, 3.729397368467904, 13.122... | [2.2695563355573927, 4.6162142202699465, 12.37... | [1.140250956051838, 0.7396592852362885, 1.0218... | [1.728722802038216, 4.521343806420914, 2.80787... | [18.321586322668246, 14.595347285061017, 36.97... | [12.219234056488448, 6.129650356878898, 10.120... | [0.46503541814884397, 2.3095400747975847, 4.42... | [20.255814253648666, 33.0484805714848, 66.3739... | . Joel Embiid [-0.5626100721097174, 6.31861993131192, 2.4004... | [3.745530342633801, 1.2329342353392367, 0.6703... | [23.858670698530716, 14.171117481078998, 22.38... | [8.531705694178644, 6.592226230945353, 5.78683... | [3.636279316429121, 10.099346265562726, 5.7510... | [5.614085976697099, 10.835254446544083, 10.530... | [21.316896006823455, 37.63500403364662, 15.459... | [11.773784732117047, 12.398188556975562, 7.601... | [0.5668458333098912, 0.5312890917217526, 0.310... | [23.491288498690384, 51.2730527798428, 14.6226... | . Paul George [3.747746441549796, 1.4181358595970992, 5.9393... | [0.1381904316955113, 1.0488620581494552, 0.056... | [11.961150191031587, 21.844008115116335, 28.35... | [8.270970012797578, 10.471683200512018, 8.6575... | [7.461807193390171, 4.502386500618187, 3.01446... | [2.330402296133858, 7.49604672023695, 4.815137... | [24.937871476385926, 24.159147380580418, 16.18... | [8.39563811879939, 10.92290776474599, 6.403018... | [0.09488761613910184, 2.6952464170520942, 5.14... | [28.492749009079404, 31.865634785139502, 15.83... | . Kawhi Leonard [1.9111587130339496, 3.861952310135943, 5.2338... | [1.2732246842101056, 0.3281317197316312, 0.826... | [11.360972973682683, 9.585929657228244, 21.373... | [12.958518216160943, 6.903991550195954, 7.6377... | [16.432022800206976, 2.660269738496524, 7.0656... | [7.797850653453996, 4.33192367872911, 8.529788... | [10.856905074154483, 20.15734528602171, 24.835... | [2.7909724123148116, 5.850300462819424, 10.213... | [0.42294253595485454, 1.7846431134663236, 1.04... | [10.218576515393485, 30.97208872537533, 29.882... | . Damian Lillard [10.677268995445747, 4.180050225294141, 9.8551... | [0.15727721227445404, 0.9580314076148106, 0.00... | [19.284257429878746, 17.31204360854523, 15.118... | [5.286410181032442, 6.34510375863316, 12.39764... | [14.714450025225243, 17.748107775039408, 3.093... | [0.8363791899940345, 3.588830309255546, 6.4196... | [37.76255224153543, 22.075858810385594, 27.925... | [4.773627696731243, 4.621992467769962, 7.50805... | [1.3504753152495277, 0.08428523535351211, 1.64... | [26.845283377158893, 6.794000830722091, 47.545... | . Jimmy Butler [4.209873691051277, 14.197422029412843, 5.9746... | [0.21814011560469518, 0.30447250932287995, 0.9... | [7.725284038601737, 10.845475416658621, 13.048... | [8.892692053613146, 7.059212392324561, 10.9128... | [3.190729877814753, 7.330081264685362, 12.8000... | [5.125910874417352, 12.424270820405049, 6.3800... | [36.69969739697747, 11.83426127688215, 25.6908... | [7.963523560116641, 7.136457070141159, 7.17463... | [0.9233590447096361, 1.9610296115104289, 0.402... | [53.11718282007372, 36.74156902865509, 31.6143... | . LeBron James [4.383784518789513, 11.422636546717118, 1.8432... | [0.49669613448309097, 0.22661116243441118, 0.8... | [16.273532104553603, 18.89206066775093, 20.779... | [11.305028513244693, 16.344920059482988, 9.013... | [11.481284754437317, 7.998971744828838, 7.7999... | [1.894279478363282, 8.134466938477937, 5.16229... | [22.984748887841146, 29.481915659477433, 34.58... | [10.903870207177395, 6.4820198722838445, 8.954... | [1.42399775829285, 3.7503728208149565, 0.23432... | [25.637588639201052, 48.951910647108924, 32.10... | . Bradley Beal [2.0746630930670698, 6.754432736863302, 8.1447... | [0.14781341009400328, 0.1456013540011799, 0.25... | [14.470299375421758, 17.38438420787558, 8.2879... | [12.937693524852126, 7.286039336111235, 3.5386... | [8.81124846003997, 5.690489995458582, 8.479262... | [1.582589186451698, 1.4114941956513458, 1.5218... | [9.943219061144877, 31.48297458824507, 12.0875... | [6.252901453057193, 3.3518717200947448, 6.4345... | [1.2849268221556598, 1.965109099251958, 1.7432... | [10.942258715360898, 29.322648826884677, 16.96... | . To make things simpler to read, we will compress the dataframe into totals for the entire season, including the total fantasy points for that season . simulated_totals = simulated_season.copy() for col in simulated_totals.columns: simulated_totals[col] = [sum(a) for a in simulated_totals[col]] simulated_totals.sort_values(&#39;FP&#39;, ascending=False) . AST BLK FGA FGM FTA FTM PTS REB STL FP . James Harden 747.830702 | 50.407450 | 1762.218494 | 767.004764 | 864.891220 | 728.555527 | 2578.629298 | 554.455532 | 138.349447 | 2938.123005 | . LeBron James 644.902311 | 53.848741 | 1528.397672 | 867.848033 | 535.034764 | 397.555538 | 2299.442187 | 621.522299 | 111.886953 | 2933.573626 | . Giannis Antetokounmpo 411.042856 | 113.488274 | 1307.959350 | 724.625241 | 567.077934 | 473.292758 | 2117.891129 | 777.568640 | 99.125458 | 2841.997073 | . Stephen Curry 506.145110 | 24.894564 | 1409.398642 | 773.939875 | 380.741190 | 329.111538 | 2362.073947 | 420.829387 | 137.321349 | 2764.175938 | . Karl-Anthony Towns 205.572031 | 131.434058 | 1226.963594 | 778.868996 | 426.204892 | 330.018614 | 1801.931600 | 946.156019 | 79.285543 | 2620.098373 | . Anthony Davis 196.578860 | 175.150824 | 1622.873347 | 816.723251 | 696.171419 | 456.367971 | 2260.519240 | 900.291765 | 124.250201 | 2610.837347 | . Joel Embiid 234.186200 | 163.287594 | 1435.194243 | 660.241425 | 734.381531 | 582.971875 | 2023.816505 | 1017.135889 | 65.729671 | 2577.793384 | . Kawhi Leonard 254.481192 | 58.379376 | 1429.088809 | 694.413700 | 508.884724 | 464.766005 | 1988.136387 | 495.686852 | 164.975375 | 2182.865355 | . Damian Lillard 517.704615 | 22.305801 | 1532.940857 | 714.316068 | 628.611949 | 519.754053 | 2133.813013 | 339.602730 | 96.813527 | 2182.757000 | . Jimmy Butler 413.035473 | 38.031837 | 1281.235832 | 606.988700 | 589.880982 | 492.461756 | 1824.943569 | 467.421917 | 151.978400 | 2123.744838 | . Nikola Jokic 366.181073 | 64.952508 | 953.642637 | 511.123020 | 260.587381 | 218.712031 | 1287.338234 | 741.543901 | 101.564244 | 2077.184995 | . Paul George 311.926094 | 33.618702 | 1485.702959 | 679.384305 | 533.333046 | 431.228670 | 1877.436210 | 558.674019 | 164.005182 | 2037.237177 | . Bradley Beal 347.030138 | 39.924951 | 1495.795748 | 668.972669 | 400.151491 | 350.540788 | 1754.362308 | 325.133155 | 93.936173 | 1683.952942 | . Generally speaking, this method is in-line with many other fantasy predictions. James Harden, Anthony Davis, LeBron James, Karl-Anthony Towns, Steph Curry, Giannis, and Joel Embiid all top the list. . In this &quot;simulation&quot; our sample size was 82 to match a season. We could repeat this simulation multiple times (so 82 * n times). That effectively increases our sample size from 82 to much larger. . Sampling enough is always a question, so we&#39;ll address that by simulating multiple seasons. Discussion of the approach will follow later . def simulate_n_seasons(player_id_map, stat_model_map, n=5): # For a season, we just want the player, FP, and the rank # Initialize dictionary of dictionary of lists to store this information across &quot;epochs&quot; epoch_results = {} for player in player_id_map: epoch_results[player] = {&#39;FP&#39;:[], &#39;rank&#39;:[]} for i in range(n): # Just copy-pasted code for convenience in a notebook # If this were a python script, I would probably put these functions in a module/library somewhere # Model the distribution of a player&#39;s box stats, simulate 82 times, compute fantasy points simulated_season = pd.DataFrame() for player, player_id in player_id_map.items(): df = pd.read_csv(&#39;data/{}.csv&#39;.format(player.replace(&quot; &quot;,&quot;&quot;))) simulated_player_log = {} for stat, model in stat_model_map.items(): params = model.fit(df[stat]) sample = model.rvs(*params, size=82) simulated_player_log[stat] = sample simulated_player_log_series = pd.Series(data=simulated_player_log, name=player) simulated_season = simulated_season.append(simulated_player_log_series) simulated_season = compute_fantasy_points(simulated_season) simulated_totals = simulated_season.copy() for col in simulated_totals.columns: simulated_totals[col] = [sum(a) for a in simulated_totals[col]] simulated_totals = simulated_totals.sort_values(&#39;FP&#39;, ascending=False) # Store the fantasy points and player rank for that simulated season for player in player_id_map: epoch_results[player][&#39;FP&#39;].append(simulated_totals[simulated_totals.index==player][&#39;FP&#39;].values[0]) epoch_results[player][&#39;rank&#39;].append(simulated_totals.index.get_loc(player)) return epoch_results epoch_results = simulate_n_seasons(player_id_map, stat_model_map, n=10) . pprint(epoch_results) . {&#39;Anthony Davis&#39;: {&#39;FP&#39;: [2600.2718925173745, 2845.699762026843, 2732.8142372134657, 2841.1189014237507, 2971.6018500231144, 2513.4197141216027, 2671.907808833641, 2771.33344794354, 2642.4401320506413, 2668.8391100382087], &#39;rank&#39;: [3, 0, 2, 1, 0, 5, 2, 2, 2, 3]}, &#39;Bradley Beal&#39;: {&#39;FP&#39;: [2017.9614666080413, 1898.010517178849, 1804.7941022033058, 1763.042431335895, 1730.5132495765397, 1715.734094440131, 1838.502289868321, 1867.0678145772936, 1718.2489707303305, 1669.67185265485], &#39;rank&#39;: [11, 12, 12, 12, 12, 12, 12, 12, 12, 12]}, &#39;Damian Lillard&#39;: {&#39;FP&#39;: [2382.8646505353163, 2048.068798160208, 2477.586060738951, 2175.7697065125562, 2118.5140365357565, 2247.094879780937, 2072.756774030209, 2139.1192107972674, 2370.629467489499, 2257.211660962338], &#39;rank&#39;: [7, 10, 6, 9, 9, 8, 10, 10, 7, 7]}, &#39;Giannis Antetokounmpo&#39;: {&#39;FP&#39;: [2576.219466934943, 2686.76587859472, 2493.6482163098117, 2702.8549155043497, 2569.2853027544083, 2551.6813152859013, 2414.5128757456737, 2561.3702273681874, 2499.1103752312756, 2510.8640790442428], &#39;rank&#39;: [4, 3, 5, 2, 4, 3, 6, 3, 5, 6]}, &#39;James Harden&#39;: {&#39;FP&#39;: [2887.670929514508, 2843.389071489731, 3116.7281411190593, 2857.6232728678133, 2804.0017687844643, 2737.302937823686, 3007.392134417434, 2919.5661859360953, 2967.3026370340576, 2964.2404529023775], &#39;rank&#39;: [0, 1, 0, 0, 1, 1, 0, 0, 0, 0]}, &#39;Jimmy Butler&#39;: {&#39;FP&#39;: [2161.113361146545, 1909.6945703788665, 1986.2703953730904, 2136.973720154527, 2232.480783438934, 2166.1726145299494, 1962.9186078450955, 2004.7241270670554, 2039.3267955068004, 1973.2565162804035], &#39;rank&#39;: [9, 11, 11, 11, 8, 11, 11, 11, 11, 11]}, &#39;Joel Embiid&#39;: {&#39;FP&#39;: [2800.8172929600287, 2558.645668264587, 2517.7715107689955, 2382.7918864392273, 2575.564309480633, 2513.594633760708, 2639.2018786988106, 2536.501391112381, 2502.800851773036, 2546.098737416667], &#39;rank&#39;: [1, 5, 4, 6, 3, 4, 3, 4, 4, 5]}, &#39;Karl-Anthony Towns&#39;: {&#39;FP&#39;: [2438.6012883732774, 2597.9460772777898, 2437.4608371185327, 2559.1161865080608, 2562.0978448642904, 2655.419027730967, 2479.0296092681992, 2469.6417151916476, 2552.6318141655534, 2706.9979324697306], &#39;rank&#39;: [6, 4, 7, 4, 5, 2, 4, 5, 3, 2]}, &#39;Kawhi Leonard&#39;: {&#39;FP&#39;: [2192.2405107386744, 2416.815987294494, 2274.141300566951, 2137.2178749787718, 2234.4505712447212, 2213.2129013592594, 2249.1630270595037, 2255.1592921722886, 2220.8331127315014, 2193.058252470087], &#39;rank&#39;: [8, 6, 8, 10, 7, 9, 7, 8, 9, 8]}, &#39;LeBron James&#39;: {&#39;FP&#39;: [2718.7719659019, 2830.2185612255066, 2796.158077485558, 2679.1235682729816, 2793.4255223009245, 2876.54356690619, 2712.108129400297, 2785.7145304012624, 2789.5298777236117, 2929.5895036201873], &#39;rank&#39;: [2, 2, 1, 3, 2, 0, 1, 1, 1, 1]}, &#39;Nikola Jokic&#39;: {&#39;FP&#39;: [2065.5848606919335, 2211.6735032023817, 2211.9020404775197, 2257.4205116181893, 2103.9926989504497, 2279.1497412203903, 2204.2763112313123, 2424.980161680876, 2294.3977539747652, 2097.2016861034704], &#39;rank&#39;: [10, 8, 9, 7, 10, 7, 8, 7, 8, 10]}, &#39;Paul George&#39;: {&#39;FP&#39;: [1954.6580000665872, 2067.468713136873, 2048.682238913504, 2201.483917859578, 1949.90816918642, 2169.3138128730848, 2140.273072194092, 2242.3376266501905, 2052.403480766016, 2142.499394706088], &#39;rank&#39;: [12, 9, 10, 8, 11, 10, 9, 9, 10, 9]}, &#39;Stephen Curry&#39;: {&#39;FP&#39;: [2505.2751490181836, 2316.5751250140206, 2567.5576627793876, 2546.082326528174, 2560.3301729832456, 2391.130817270027, 2446.09455104389, 2428.6777503086655, 2412.173845766393, 2577.680897739675], &#39;rank&#39;: [5, 7, 3, 5, 6, 6, 5, 6, 6, 4]}} . To make things prettier, we can just summarize the player ranks over all the simulated seasons, providing us an estimated average rank and error . def summarize_epoch_results(epoch_results): summary_stats = {} for player in epoch_results: summary_stats[player] = {} avg_rank = np.mean(epoch_results[player][&#39;rank&#39;]) std_rank = np.std(epoch_results[player][&#39;rank&#39;]) summary_stats[player][&#39;rank&#39;] = avg_rank summary_stats[player][&#39;err&#39;] = std_rank return summary_stats summary_stats = summarize_epoch_results(epoch_results) sorted(summary_stats.items(), key=lambda v: v[1][&#39;rank&#39;]) . [(&#39;James Harden&#39;, {&#39;rank&#39;: 0.3, &#39;err&#39;: 0.45825756949558394}), (&#39;LeBron James&#39;, {&#39;rank&#39;: 1.4, &#39;err&#39;: 0.8}), (&#39;Anthony Davis&#39;, {&#39;rank&#39;: 2.0, &#39;err&#39;: 1.4142135623730951}), (&#39;Joel Embiid&#39;, {&#39;rank&#39;: 3.9, &#39;err&#39;: 1.3}), (&#39;Giannis Antetokounmpo&#39;, {&#39;rank&#39;: 4.1, &#39;err&#39;: 1.3}), (&#39;Karl-Anthony Towns&#39;, {&#39;rank&#39;: 4.2, &#39;err&#39;: 1.5362291495737217}), (&#39;Stephen Curry&#39;, {&#39;rank&#39;: 5.3, &#39;err&#39;: 1.1}), (&#39;Kawhi Leonard&#39;, {&#39;rank&#39;: 8.0, &#39;err&#39;: 1.0954451150103321}), (&#39;Damian Lillard&#39;, {&#39;rank&#39;: 8.3, &#39;err&#39;: 1.4177446878757827}), (&#39;Nikola Jokic&#39;, {&#39;rank&#39;: 8.4, &#39;err&#39;: 1.2}), (&#39;Paul George&#39;, {&#39;rank&#39;: 9.7, &#39;err&#39;: 1.1}), (&#39;Jimmy Butler&#39;, {&#39;rank&#39;: 10.5, &#39;err&#39;: 1.02469507659596}), (&#39;Bradley Beal&#39;, {&#39;rank&#39;: 11.9, &#39;err&#39;: 0.3})] . Observations (based on this approach) . Harden, LeBron, and AD are a cut above the rest. Beal is not looking too hot . Room for improvement . Is building a distribution from year 2015-onward a good idea? | Pick better models to represent the distribution of a player&#39;s box stats? | How do we account for player development? Forecasting player stats, not just modeling | How do we account for roster/team changes? | Can we account for hot streaks for a player? | Is there a more robust way to deal with player injury rather than hoping for 0/0/0 in the gamelogs? | Correlation between stats? If a player is on, they might end up playing better overall | Can we try to time schedules? I.e. some NBA players will have 4-game weeks, can a corresponding fantasy player use that based on the fantasy schedule and truly trying to beat your fantasy opponent? | Is there a need to draft a player in reaction to other fantasy player draftpicks? This may depend on how specific your team roles have to be. If team roles are lax, then choose the best fantasy option. If you need to fill out a roster, then you have to start weighing your roster choices vs what opponents may end up drafting | .",
            "url": "https://ahy3nz.github.io/fastpayges/basketball/data%20science/2019/08/15/nbafantasy2.html",
            "relUrl": "/basketball/data%20science/2019/08/15/nbafantasy2.html",
            "date": " ‚Ä¢ Aug 15, 2019"
        }
        
    
  
    
        ,"post20": {
            "title": "Fantasy NBA 1",
            "content": "I&#39;m using nba_api to parse the nba stats website. While it&#39;d be nice to put everything in one notebook, I&#39;ve had to split the scraping step into a separate notebook. Too many and too frequent URL requests lead to connection errors and data limits/throttles (even with a sleep call). So, we&#39;ll parse the information we want and save it to a csv for future recall . import time import numpy as np import pandas as pd import matplotlib %matplotlib inline import matplotlib.pyplot as plt import nba_api from nba_api.stats.static import teams, players from nba_api.stats.endpoints import shotchartdetail, playercareerstats, playergamelog import ballDontLie from ballDontLie.util.api_nba import * . seasons_range = [&#39;2018-19&#39;, &#39;2017-18&#39;, &#39;2016-17&#39;, &#39;2015-16&#39;] players_range = [&#39;Anthony Davis&#39;, &#39;James Harden&#39;, &#39;Stephen Curry&#39;, &#39;Giannis Antetokounmpo&#39;, &#39;Karl-Anthony Towns&#39;, &#39;Nikola Jokic&#39;, &#39;Joel Embiid&#39;, &#39;Paul George&#39;, &#39;Kawhi Leonard&#39;, &#39;Damian Lillard&#39;, &#39;Jimmy Butler&#39;, &#39;LeBron James&#39;, &quot;Bradley Beal&quot;] . player_id_map = {a: find_player_id(a) for a in players_range} . player_id_map . {&#39;Anthony Davis&#39;: [203076], &#39;James Harden&#39;: [201935], &#39;Stephen Curry&#39;: [201939], &#39;Giannis Antetokounmpo&#39;: [203507], &#39;Karl-Anthony Towns&#39;: [1626157], &#39;Nikola Jokic&#39;: [203999], &#39;Joel Embiid&#39;: [203954], &#39;Paul George&#39;: [202331], &#39;Kawhi Leonard&#39;: [202695], &#39;Damian Lillard&#39;: [203081], &#39;Jimmy Butler&#39;: [202710], &#39;LeBron James&#39;: [2544], &#39;Bradley Beal&#39;: [203078]} . for player, player_id in player_id_map.items(): compiled_log = compile_player_gamelog(player_id, seasons_range) compiled_log.to_csv(&quot;data/{}.csv&quot;.format(player.replace(&quot; &quot;,&quot;&quot;))) time.sleep(10) .",
            "url": "https://ahy3nz.github.io/fastpayges/basketball/data%20science/2019/08/14/nbafantasy1.html",
            "relUrl": "/basketball/data%20science/2019/08/14/nbafantasy1.html",
            "date": " ‚Ä¢ Aug 14, 2019"
        }
        
    
  
    
        ,"post21": {
            "title": "First-attempt at using PyMC3 for Bayesian parameter estimation",
            "content": "Applying some principles from earlier mcmc posts/notebooks to estimate the parameters of a linear model . import numpy as np from numpy.random import random,rand %matplotlib inline import matplotlib import matplotlib.pyplot as plt . Generate some data, including some random noise . xvals = np.arange(1,100) noise = rand(*xvals.shape) yvals = 13*xvals + 50 + 50*noise . fig, ax = plt.subplots(1,1) ax.plot(xvals, yvals) . [&lt;matplotlib.lines.Line2D at 0x11989f518&gt;] . from pymc3 import Normal, Uniform, Model, HalfCauchy . First pass at constructing a probability model. Our model is a line, but we need to describe the line&#39;s parameters with distributions, and the parameters of the lines&#39; parameters&#39; distributions with distributions . For values that can be both negative and positive, we will use a Normal distribution, making up some wide-spread distributions. The centers of our slope and intercept distributions can be positive or negative . For values that can only be positive (like standard deviations), we will start out with a Uniform distribution . indices = [i for i,_ in enumerate(xvals)] with Model() as my_model: # We have two parameters of interested, slope and intercept # We need to specify how we think the slope and intercept are distributed # Nested within that, there are parameters of those slope/intercept distributions # For those, we also need to specify how those parameters are distributed # Claims: # Our model is a line, y = ax + b # The slope follows a normal distribution # The center of this distribution is also normally distributed (m_a) # The sigma of this distribution is also normally distributed (s_a) # The intercept follows a normal distribution # The center of this distribution is also normally distributed (m_b) # The sigma of this distribution is also normally distributed (s_b) # The y-values (observed data) follow a normal distribution # The center of this distribution is based on our linear model guess # The sigma of this distribution is also normally distributed # We start with the leaves/root of the model, which is looking at the distributions # of the parameters that make up the slope/intercept distribution m_a = Normal(&#39;m_a&#39;, 10, sigma=10) # The slope&#39;s center is normally distributed m_b = Normal(&#39;m_b&#39;, 20, sigma=10) # The intercept&#39;s center is normally distributed # Second looking at the stdevs of the slope/center distributions s_a = Uniform(&#39;s_a&#39;, 10, 30) # The slope&#39;s stdev is uniformly distributed s_b = Uniform(&#39;s_b&#39;, 5, 15) # The intercept&#39;s stdev is uniformly distributed # With these parameters&#39; distributions specified, we now build the distributions for # the model&#39;s parameters a = Normal(&#39;a&#39;, m_a, sigma=s_a, shape=len(xvals)) # The slope&#39;s normal distribution b = Normal(&#39;b&#39;, m_b, sigma=s_b, shape=len(xvals)) # The intercept&#39;s normal distribution exp = a[indices]*xvals + b[indices] # Now let&#39;s look at our likelihood function (observed data distribution) s_y = Uniform(&#39;s_y&#39;, 30,50) #s_y = HalfCauchy(&#39;s_y&#39;, 1) y = Normal(&#39;y&#39;, exp, sigma=s_y, observed=yvals) . PyMC has some nice functionality to visualize which distributions and parameters feed into which other distributions and parameters. This was really helpful for me to understand the differences in the models, parameters, and distributions . from pymc3 import model_to_graphviz model_to_graphviz(my_model) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 cluster99 99 s_a s_a ~ Uniform a a ~ Normal s_a&#45;&gt;a m_a m_a ~ Normal m_a&#45;&gt;a s_b s_b ~ Uniform b b ~ Normal s_b&#45;&gt;b m_b m_b ~ Normal m_b&#45;&gt;b s_y s_y ~ Uniform y y ~ Normal s_y&#45;&gt;y b&#45;&gt;y a&#45;&gt;y Actually conduct the posterior sampling. Default uses a NUTS sampler. We use 2 different cores to independently run 2 different chains, with some guess starting values. 3000 samples was enough to get decent convergence, see note later . from pymc3 import sample with my_model: my_trace = sample(3000, cores=2, start={&#39;m_a&#39;:10, &#39;m_b&#39;:30}) . Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [s_y, b, a, s_b, s_a, m_b, m_a] Sampling 2 chains: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7000/7000 [00:32&lt;00:00, 109.64draws/s] There were 68 divergences after tuning. Increase `target_accept` or reparameterize. There were 78 divergences after tuning. Increase `target_accept` or reparameterize. The acceptance probability does not match the target. It is 0.6827112736926122, but should be close to 0.8. Try to increase the number of tuning steps. The estimated number of effective samples is smaller than 200 for some parameters. . After sampling, we can visualize the trace which is the statistics equivalent of a molecular simulation trajectory. The top row plots the trace of the center of the slope distribution. There are a range of possibles slopes, here are their probabilities (on the left) and how the slope value was changed over the sampling. The bottom row plots the trace of the center of the intercept distribution. . The two 2 chains are both visualized for both parameters, and they converge decently, saying the chains ended up circulating through very similar values. . from arviz import from_pymc3, plot_trace my_output = from_pymc3(my_trace) plot_trace(my_output.posterior, var_names=[&#39;m_a&#39;, &#39;m_b&#39;]); . We can also plot the posterior distribution of the various parameters we sampled. In this case we will be looking at m_a which is the center of the normal distribution that models our slope. . We can also use these distributions to calculate probabilities of observing certain slopes. In this case, we can look at the true slope (that we specified), and see it&#39;s 89.7% more likely to observe a slope greater than 13. This doesn&#39;t seem very promising for our probability model, if it says a slope of 13 isn&#39;t very likely . from arviz import plot_posterior plot_posterior(my_trace, var_names=[&#39;m_a&#39;], ref_val=13) . array([&lt;matplotlib.axes._subplots.AxesSubplot object at 0x1c2e53f898&gt;], dtype=object) . We can apply the same function to look at the center of the distribution that represents the intercept of our line. Originally, we specified it as 50 + 50 *noise, which means the intercept would vary from [50, 100) . Interestingly, it is VERY likely to observe an intercept less than 75. Given our random noise, you&#39;d expect the intercept to be ~75, but the bayesian probability model would suggest otherwise. . If we look at both slope and intercept, the slope is over-predicted and intercept under-predicted, where each correction compensates for the other parameter . plot_posterior(my_trace, var_names=[&#39;m_b&#39;], ref_val=75) . array([&lt;matplotlib.axes._subplots.AxesSubplot object at 0x1c2f8d3c18&gt;], dtype=object) . So our slope is overestimated and intercept underestimated - how does the line actually look? . We can plot the observed data as the empty circles, and pick a couple parameters from our trace to make a new line. We&#39;re pulling randomly from the last 200-ish of the sampling, some sort of steady-state sampling region, and pulling from chain 1. . So this visually shows that our slope is overestimated and intercept underestimated - this is not good . fig, ax = plt.subplots(1,1) ax.plot(xvals, yvals, color=&#39;b&#39;, marker=&#39;o&#39;, markerfacecolor=&#39;white&#39;) for i in range(50): random_idx = np.random.randint(800,1000) slope = my_output.posterior.m_a[1][random_idx].values intercept = my_output.posterior.m_b[1][random_idx].values ax.plot(xvals, slope*xvals + intercept, color=&#39;red&#39;, alpha=0.2) . For comparison, we can do a plain linear regression to fit the data. . With this method, it&#39;s not as simple to look at various probabilities of slopes or distributions of parameters. The number is the number you get from fitting the data, a singular value. . We can report correlation coefficients to get an idea of how good of a model it is, but we don&#39;t get a sense of the distribution of the slopes/intercepts. . But with the linear regression, the fit is much better. The slopes and intercepts are more in line with what we&#39;d have expected from making up the data. . from sklearn.linear_model import LinearRegression xvals = xvals.reshape(-1,1) yvals = yvals.reshape(-1,1) reg = LinearRegression().fit(xvals, yvals) print(reg.score(xvals, yvals)) print((reg.coef_, reg.intercept_)) . 0.998515144087584 (array([[13.01389575]]), array([73.01098003])) . fig, ax = plt.subplots(1,1) ax.plot(xvals, yvals, color=&#39;b&#39;, marker=&#39;o&#39;, markerfacecolor=&#39;white&#39;) ax.plot(xvals, xvals*reg.coef_ + reg.intercept_, color=&#39;r&#39;) . [&lt;matplotlib.lines.Line2D at 0x1c2f715358&gt;] . Can we do better with the Bayesian model? Let&#39;s simplify a little. Before we were modeling slope and intercept with a normal distribution, but the normal distribution&#39;s parameters were unknown - we were sampling parameters of those normal distribution, which will propagate up to the actual slopes. . Instead, let&#39;s just have &quot;fixed&quot; parameters to represent the normal distributions. This sort of &quot;constrains&quot; what we have since we&#39;ve already made a claim on the distributions of a and b, without having a distribution on the distribution of a and b. On the bright side, this reduces the degrees of freedom or room for error in the sampling. . As in the examples from the mcmc notebook 4, they chose to use a Half Cauchy distribution for their standard deviations (this distribution enforces only positive values, so that&#39;s good for standard deivation). Looking at some stack exchange responses, half-Cauchy can be weakly-informative, which helps if the posterior distribution is the more-dominant factor. For generally choosing priors, it depends on what the prior beliefs are and if you can fit them to some of the more common distributions or your own intuitive understanding. . indices = [i for i,_ in enumerate(xvals)] with Model() as half_cauchy_model: # Claims: # Our model is a line, y = ax + b # The slope follows a normal distribution (a) # The intercept follows a normal distribution (b) # The y-values (observed data) follow a normal distribution # The center of this distribution is based on our linear model guess # The sigma of this distribution is also distributed by half-cauchy # We now build the distributions for # the model&#39;s parameters a = Normal(&#39;a&#39;, 13, sigma=20) b = Normal(&#39;b&#39;, 60, sigma=20) exp = a*xvals + b # Now let&#39;s look at our likelihood function (observed data distribution) s_y = HalfCauchy(&#39;s_y&#39;, beta=10, testval=1) y = Normal(&#39;y&#39;, exp, sigma=s_y, observed=yvals) . Visualizing the simplified model, with a smaller hierarchy . from pymc3 import model_to_graphviz model_to_graphviz(half_cauchy_model) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 cluster99 x 1 99 x 1 b b ~ Normal y y ~ Normal b&#45;&gt;y s_y s_y ~ HalfCauchy s_y&#45;&gt;y a a ~ Normal a&#45;&gt;y from pymc3 import sample with half_cauchy_model: half_cauchy_trace = sample(3000, cores=2, start={&#39;a&#39;:10, &#39;b&#39;:50}) . Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [s_y, b, a] Sampling 2 chains: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7000/7000 [00:08&lt;00:00, 787.34draws/s] The acceptance probability does not match the target. It is 0.8871159685668408, but should be close to 0.8. Try to increase the number of tuning steps. . from arviz import from_pymc3, plot_trace half_cauchy_output = from_pymc3(half_cauchy_trace) plot_trace(half_cauchy_output.posterior, var_names=[&#39;a&#39;, &#39;b&#39;]); . plot_posterior(half_cauchy_trace, var_names=[&#39;a&#39;], ref_val=13) . array([&lt;matplotlib.axes._subplots.AxesSubplot object at 0x1c2b871828&gt;], dtype=object) . plot_posterior(half_cauchy_trace, var_names=[&#39;b&#39;], ref_val=75) . array([&lt;matplotlib.axes._subplots.AxesSubplot object at 0x1c345d8940&gt;], dtype=object) . Visualizing all the traces (which converge well with each other) and posteriors, the distributions all aggregate around correct centers/locations. Actually visualizing some of the resultant lines, the lines fit much more nicely than before. . fig, ax = plt.subplots(1,1) ax.plot(xvals, yvals, color=&#39;b&#39;, marker=&#39;o&#39;, markerfacecolor=&#39;white&#39;) for i in range(50): random_idx = np.random.randint(800,1000) slope = half_cauchy_output.posterior.a[1][random_idx].values intercept = half_cauchy_output.posterior.b[1][random_idx].values ax.plot(xvals, slope*xvals + intercept, color=&#39;red&#39;, alpha=0.2) . Summary . This was my first-hand pass of using Bayesian parameter estimation to estimate the slope and intercept of an analytically-specified line (with some noise). I tried to &#39;chain priors&#39; together by using distributions to esimate the parameters that described the distributions of the slope/intercept, but the estimated paramters were poor. I simplified the model to &quot;hard code&quot; the distribution underlying the slope and intercept and got much better results. Still, the sklearn linear regression still had pretty good slope/intercept estimates, but the Bayesian methods were able to provide insight on the distribution/range of parameters that could model the line - even if some were worse than others. . As in MD, sampling is always important (did you sample enough space, did you sample long enough, did doing multiple trials converge on the same trace/trajectory?) I think, when chaining priors together, we were doing a more thorough sampling through phase space, which led to worse convergence and a wilder range of parameter estimates (compare the x-scales of some of these plots for the parameter estimates). Without chaining priors, we were searching through space less and converged a litle more easily. As with most computational methods, it&#39;s important to choose the approach that is most appropriate for your task - there is never a silver bullet. .",
            "url": "https://ahy3nz.github.io/fastpayges/scientific%20computing/2019/08/08/bayes2.html",
            "relUrl": "/scientific%20computing/2019/08/08/bayes2.html",
            "date": " ‚Ä¢ Aug 8, 2019"
        }
        
    
  
    
        ,"post22": {
            "title": "Molecular Modeling Software: ParmEd",
            "content": "ParmEd is an open-source python package for molecular modelling applications. In the description: &quot;Cross-program parameter and topology file editor and molecular mechanical simulator engine.&quot; . The description doesn&#39;t quite do this package justice - ParmEd provides a data structure that represents a molecular model. Atomic names, coordinates, bonding neighbors, and ALL the interaction parameters you would use to model a chemical system and its underlying physics (at a molecular mechanics level, not quantum). . For a molecular modeller, this extremely important if we want to consistently and reliably build a model for a chemical system. Building these models ends up being a lot of bookkeeping (okay we&#39;ll use this value for the A-B bond but this other value for the A-B non-bonded interaction), which becomes exponentially laborious as the sorts of chemical systems diversify from hundreds of copies of the same type of atom and parameters, to thousands of copies of a dozen different types and hundreds of different parameters, to beyond. . For a scientific software developer, this is a great package in its object-oriented design, extensive documentation, use of unit tests, open-source nature, and active user community; active user communities are hard in academia when everyone does everything slightly differently - if you can find a common ground that everyone will use and know, then you might hit a &quot;critical threshold&quot; for userbase that the open-source community may yield some incredible contributions. . For both groups, this is a great package because it facilitates interoperability between simulation engines/applications. The world of molecular modelling has developed many simulation packages designed for a particular purpose, focused more on depth than breadth of application. The algorithms and simulation implementations may differ, but the fundamental molecular model is the same. . The interconversion and interoperability issue . Here&#39;s a gripe within the simulation community: everyone has a different file format . One simulation engine might write bond parameters as (bondtype, r0 in Angstrom, k in kcal/mol/A^2) . 1, 10, 200 . Another simulation engine might go with (atomtype1, atomtype2, k in kJ/mol/nm^2, r0 in nm) . 1, 2, 41840, 1 . For thousands of parameter lines similar to this, keeping track of units and proper syntax and overall formatting is a huge headache, and you&#39;d think this sort of thing should be automated. . Another example, to express a bond as a harmonic potential, there are 2 ways to write the function . $V(r) = k * (r-r_0)^2$ . $V(r) = (1/2) * k * (r-r_0)^2$ . So your force constant, $k$, can be off by a factor 2 if you&#39;re not bookkeeping properly. . In the grand scheme of interoperability, ParmEd processes these nuances between implementations upon translating to or from the parmed.Structure object . Let&#39;s say there are N simulation engines. Converting between each engine could be as expensive as O($N^2$) convertes. You would have to read in engine A&#39;s file format and convert it to the proper format for engine B. And then you&#39;d need one for A-C and B-C, etc. . What if there were a middleman? Converting between each engine would be O($2N$). This middleman is parmed.Structure, this massive class that contains all these parameters and coordinates and everything you need in a molecular model. The conversion procedure would be A-Structure-B or C-Structure-B. . If you can store your model information into the parmed.Structure, you just need to write a routine that parses through the Structure and spits the information out according to the engine. If you have the information stored in an engine&#39;s file format, you just need to write a routine that processes that information into parmed.Structure . Examining pmd.Structure . One popular use-case of parmed involves reading files to get a pmd.Structure. It&#39;s fairly simple to go from some GROMACS files to pmd.Structure. In this case, we&#39;ll be looking at ethane (C2H8) . import parmed as pmd my_struc = pmd.load_file(&#39;ethane_periodic.top&#39;, xyz=&#39;ethane_periodic.gro&#39;) my_struc . &lt;GromacsTopologyFile 8 atoms; 1 residues; 7 bonds; PBC (orthogonal); parametrized&gt; . In this one line, all the molecular model information about bond parameters, angle parameters, dihedral parameters, non-bonded parameters, etc. are all loaded in to my_struc. . Now we can start digging through the various properties within my_struc . my_struc is just a massive container for smaller containers that eventually hold numerical values. We can examine the bonds within my_struc . print(my_struc.bonds) . TrackedList([ &lt;Bond &lt;Atom C [0]; In RES 0&gt;--&lt;Atom H [3]; In RES 0&gt;; type=&lt;BondType; k=340.000, req=1.090&gt;&gt; &lt;Bond &lt;Atom C [0]; In RES 0&gt;--&lt;Atom H [1]; In RES 0&gt;; type=&lt;BondType; k=340.000, req=1.090&gt;&gt; &lt;Bond &lt;Atom C [0]; In RES 0&gt;--&lt;Atom C [4]; In RES 0&gt;; type=&lt;BondType; k=268.000, req=1.529&gt;&gt; &lt;Bond &lt;Atom H [2]; In RES 0&gt;--&lt;Atom C [0]; In RES 0&gt;; type=&lt;BondType; k=340.000, req=1.090&gt;&gt; &lt;Bond &lt;Atom C [4]; In RES 0&gt;--&lt;Atom H [5]; In RES 0&gt;; type=&lt;BondType; k=340.000, req=1.090&gt;&gt; &lt;Bond &lt;Atom H [6]; In RES 0&gt;--&lt;Atom C [4]; In RES 0&gt;; type=&lt;BondType; k=340.000, req=1.090&gt;&gt; &lt;Bond &lt;Atom H [7]; In RES 0&gt;--&lt;Atom C [4]; In RES 0&gt;; type=&lt;BondType; k=340.000, req=1.090&gt;&gt; ]) . The string representations of these bonds is pretty helpful at a glance. We can see the two atoms participating in the bond, and the nature of their interactions (bondtypes) There are two different bondtypes, one for C-C and another for C-H. . Next, we can select out an individual bond and sift through the various information inside it . print(my_struc.bonds[0]) print(my_struc.bonds[0].type) print(my_struc.bonds[0].type.k) . &lt;Bond &lt;Atom C [0]; In RES 0&gt;--&lt;Atom H [3]; In RES 0&gt;; type=&lt;BondType; k=340.000, req=1.090&gt;&gt; &lt;BondType; k=340.000, req=1.090&gt; 339.99999999999994 . We can look at the atoms similarly - we&#39;ll get the coordinates of the first atom . print(my_struc.atoms[0]) print(my_struc.atoms[0].xx, my_struc.atoms[0].xy, my_struc.atoms[0].xz) . &lt;Atom C [0]; In RES 0&gt; 0.0 -1.4000000000000001 -0.0 . Before going forward, I want to introduce the term atom type, which describes the identity of an atom. . If you had a system of just argon atoms, they are all chemically identical. There might be a bunch of argon atoms, but there would only be one atom type . In a more complicated example, let&#39;s say you had a box of propane . Looking at the carbons, there might be 3 carbons, but 2 different atom types. The &quot;outside&quot; carbons (each with 3 hydrogens) moieties, are chemically different from the &quot;middle&quot; carbon (with 2 hydrogens). . When you look at bonds, the external C-H bond might be chemically different from the inside C-H bond (they probably aren&#39;t very chemically different, but let&#39;s assume they are). As a result, while they are both C-H bonds, they will have different bond types. This sort of delination appears for angles (3 atoms bonded linearly) and dihedrals (4 atoms bonded together either linearly) and improper dihedrals (3 atoms bonded to a central atom). You might have a lot of bonds, but many fewer bond types . Let&#39;s look deeper at the atoms within my_struc, the atom type . print(my_struc.atoms[0].type) . opls_135 . Not the most interesting or intuitive name, but it&#39;s still unique enough for differenting atom types . Before we go further, I have to introduce another equation that models the non-bonded interactions in our system. For $N$ atoms, there are $N^2$ of these. And usually non-bonded interactions are broken into coulombic (think physics 2) interactions, but also your dispersion forces (which arise from temporary dipole interactions that may arise between two atoms). I will focus on the dispersion forces - most models for this force utilize the Lennard-Jones potential (though some people use Buckingham, Morse, Mie, etc). . $V(r) = 4 epsilon[( frac{ sigma}{r})^{12} - ( frac{ sigma}{r})^6]$ . $ sigma$ and $ epsilon$ are parameters of the model, $r$ is the interatomic distance between the two atoms in question . For the atom type opls_135, we can examine the associated sigma and epsilon parameters . print(my_struc.atoms[0].sigma, my_struc.atoms[0].epsilon) . 35.0 0.066 . For fun, we can visualize this potential over distance . import matplotlib %matplotlib inline import matplotlib.pyplot as plt import numpy as np r = np.linspace(0.00001, 120, num=1000) v = 4 * my_struc.atoms[0].epsilon * ((my_struc.atoms[0].sigma/r)**12 - (my_struc.atoms[0].sigma/r)**6) fig, ax = plt.subplots(1,1) ax.plot(r, v) ax.set_ylim([-0.1,0.1]) ax.set_ylabel(&quot;V(r), kcal/mol&quot;) ax.set_xlabel(&quot;r, $ AA$&quot;) . Text(0.5, 0, &#39;r, $ AA$&#39;) . That is an introduction to the pmd.Structure - it is a MASSIVE container for all things related to a molecular model. This works well for molecular models with a lot of things in common. Now we will tackle the components of a molecular model are different. . Dihedrals . A dihedral refers to the interaction between 4 linearly connected atoms. This isn&#39;t exactly chemistry, but rather an approximation to chemistry and things like steric interactions (think Neumann projections back from organic chemistry). . There a variety of ways to model this interaction. For example: . Periodic torsions . | Ryckaert-Bellemans (RB) torsions . | . They are both trigonometric series, but there&#39;s not much commonality between them - there&#39;s not a simple underlying set of parameters/function between the two that you can easily relate. You&#39;d have to generate some energies for one function, and then numerically fit the other. Since there&#39;s no easy way for one function/set of parameters to express both of these functions, we have to handle them separately within pmd.Structure . periodic_torsions = pmd.load_file(&#39;ethane_periodic.top&#39;, xyz=&#39;ethane_periodic.gro&#39;) rb = pmd.load_file(&#39;ethane_rbtorsion.top&#39;, xyz=&#39;ethane_rbtorsion.gro&#39;) . periodic_torsions is a molecular model for ethane using periodic torsions. rb is a molecular model for ethane using RB torsions. . All the different interaction models/classes are found in parmed/topologyobjects.py . But some dihedral bookkeeping in ParmEd: periodic torsions are stored in structure.dihedrals while RBtorsions are stored in structure.rb_torsions. Handling improper dihedrals is a bigger mess, but I want to emphasize how ParmEd handles these different functional forms: they are kept as separate objects and handled separately. . The next two snippets will look at periodic torsions within rb and periodic_torsions (or lack thereof) . periodic_torsions.dihedrals . TrackedList([ &lt;Dihedral; &lt;Atom H [1]; In RES 0&gt;--&lt;Atom C [0]; In RES 0&gt;--&lt;Atom C [4]; In RES 0&gt;--&lt;Atom H [5]; In RES 0&gt;; type=&lt;DihedralType; phi_k=0.741, per=1, phase=179.909, scee=2.000, scnb=1.000&gt;&gt; &lt;Dihedral; &lt;Atom H [1]; In RES 0&gt;--&lt;Atom C [0]; In RES 0&gt;--&lt;Atom C [4]; In RES 0&gt;--&lt;Atom H [6]; In RES 0&gt;; type=&lt;DihedralType; phi_k=0.741, per=1, phase=179.909, scee=2.000, scnb=1.000&gt;&gt; &lt;Dihedral; &lt;Atom H [1]; In RES 0&gt;--&lt;Atom C [0]; In RES 0&gt;--&lt;Atom C [4]; In RES 0&gt;--&lt;Atom H [7]; In RES 0&gt;; type=&lt;DihedralType; phi_k=0.741, per=1, phase=179.909, scee=2.000, scnb=1.000&gt;&gt; &lt;Dihedral; &lt;Atom H [2]; In RES 0&gt;--&lt;Atom C [0]; In RES 0&gt;--&lt;Atom C [4]; In RES 0&gt;--&lt;Atom H [5]; In RES 0&gt;; type=&lt;DihedralType; phi_k=0.741, per=1, phase=179.909, scee=2.000, scnb=1.000&gt;&gt; &lt;Dihedral; &lt;Atom H [2]; In RES 0&gt;--&lt;Atom C [0]; In RES 0&gt;--&lt;Atom C [4]; In RES 0&gt;--&lt;Atom H [6]; In RES 0&gt;; type=&lt;DihedralType; phi_k=0.741, per=1, phase=179.909, scee=2.000, scnb=1.000&gt;&gt; &lt;Dihedral; &lt;Atom H [2]; In RES 0&gt;--&lt;Atom C [0]; In RES 0&gt;--&lt;Atom C [4]; In RES 0&gt;--&lt;Atom H [7]; In RES 0&gt;; type=&lt;DihedralType; phi_k=0.741, per=1, phase=179.909, scee=2.000, scnb=1.000&gt;&gt; &lt;Dihedral; &lt;Atom H [3]; In RES 0&gt;--&lt;Atom C [0]; In RES 0&gt;--&lt;Atom C [4]; In RES 0&gt;--&lt;Atom H [5]; In RES 0&gt;; type=&lt;DihedralType; phi_k=0.741, per=1, phase=179.909, scee=2.000, scnb=1.000&gt;&gt; &lt;Dihedral; &lt;Atom H [3]; In RES 0&gt;--&lt;Atom C [0]; In RES 0&gt;--&lt;Atom C [4]; In RES 0&gt;--&lt;Atom H [6]; In RES 0&gt;; type=&lt;DihedralType; phi_k=0.741, per=1, phase=179.909, scee=2.000, scnb=1.000&gt;&gt; &lt;Dihedral; &lt;Atom H [3]; In RES 0&gt;--&lt;Atom C [0]; In RES 0&gt;--&lt;Atom C [4]; In RES 0&gt;--&lt;Atom H [7]; In RES 0&gt;; type=&lt;DihedralType; phi_k=0.741, per=1, phase=179.909, scee=2.000, scnb=1.000&gt;&gt; ]) . rb.dihedrals . TrackedList([ ]) . The next two snippets will look at RB torsions within rb and periodic_torsions (or lack thereof) . periodic_torsions.rb_torsions . TrackedList([ ]) . rb.rb_torsions . TrackedList([ &lt;Dihedral; &lt;Atom H [1]; In RES 0&gt;--&lt;Atom C [0]; In RES 0&gt;--&lt;Atom C [4]; In RES 0&gt;--&lt;Atom H [5]; In RES 0&gt;; type=&lt;RBTorsionType; c0=0.150; c1=0.450; c2=0.000; c3=-0.600; c4=0.000; c5=0.000; scee=2.0; scnb=1.0&gt;&gt; &lt;Dihedral; &lt;Atom H [1]; In RES 0&gt;--&lt;Atom C [0]; In RES 0&gt;--&lt;Atom C [4]; In RES 0&gt;--&lt;Atom H [6]; In RES 0&gt;; type=&lt;RBTorsionType; c0=0.150; c1=0.450; c2=0.000; c3=-0.600; c4=0.000; c5=0.000; scee=2.0; scnb=1.0&gt;&gt; &lt;Dihedral; &lt;Atom H [1]; In RES 0&gt;--&lt;Atom C [0]; In RES 0&gt;--&lt;Atom C [4]; In RES 0&gt;--&lt;Atom H [7]; In RES 0&gt;; type=&lt;RBTorsionType; c0=0.150; c1=0.450; c2=0.000; c3=-0.600; c4=0.000; c5=0.000; scee=2.0; scnb=1.0&gt;&gt; &lt;Dihedral; &lt;Atom H [2]; In RES 0&gt;--&lt;Atom C [0]; In RES 0&gt;--&lt;Atom C [4]; In RES 0&gt;--&lt;Atom H [5]; In RES 0&gt;; type=&lt;RBTorsionType; c0=0.150; c1=0.450; c2=0.000; c3=-0.600; c4=0.000; c5=0.000; scee=2.0; scnb=1.0&gt;&gt; &lt;Dihedral; &lt;Atom H [2]; In RES 0&gt;--&lt;Atom C [0]; In RES 0&gt;--&lt;Atom C [4]; In RES 0&gt;--&lt;Atom H [6]; In RES 0&gt;; type=&lt;RBTorsionType; c0=0.150; c1=0.450; c2=0.000; c3=-0.600; c4=0.000; c5=0.000; scee=2.0; scnb=1.0&gt;&gt; &lt;Dihedral; &lt;Atom H [2]; In RES 0&gt;--&lt;Atom C [0]; In RES 0&gt;--&lt;Atom C [4]; In RES 0&gt;--&lt;Atom H [7]; In RES 0&gt;; type=&lt;RBTorsionType; c0=0.150; c1=0.450; c2=0.000; c3=-0.600; c4=0.000; c5=0.000; scee=2.0; scnb=1.0&gt;&gt; &lt;Dihedral; &lt;Atom H [3]; In RES 0&gt;--&lt;Atom C [0]; In RES 0&gt;--&lt;Atom C [4]; In RES 0&gt;--&lt;Atom H [5]; In RES 0&gt;; type=&lt;RBTorsionType; c0=0.150; c1=0.450; c2=0.000; c3=-0.600; c4=0.000; c5=0.000; scee=2.0; scnb=1.0&gt;&gt; &lt;Dihedral; &lt;Atom H [3]; In RES 0&gt;--&lt;Atom C [0]; In RES 0&gt;--&lt;Atom C [4]; In RES 0&gt;--&lt;Atom H [6]; In RES 0&gt;; type=&lt;RBTorsionType; c0=0.150; c1=0.450; c2=0.000; c3=-0.600; c4=0.000; c5=0.000; scee=2.0; scnb=1.0&gt;&gt; &lt;Dihedral; &lt;Atom H [3]; In RES 0&gt;--&lt;Atom C [0]; In RES 0&gt;--&lt;Atom C [4]; In RES 0&gt;--&lt;Atom H [7]; In RES 0&gt;; type=&lt;RBTorsionType; c0=0.150; c1=0.450; c2=0.000; c3=-0.600; c4=0.000; c5=0.000; scee=2.0; scnb=1.0&gt;&gt; ]) . In the parmed.Structure object, the different dihedral forms are held in different containers. Regardless, the parameters relevant to each dihedral object are still tracked . parmed.Structure conversion . Many like to use parmed as the middleman to convert from one engine&#39;s file format to another engine&#39;s file format. In this notebook, we&#39;ve been using GROMACS files to model ethane, and now we&#39;ll mess around with converting to files suitable for other engines. . my_struc = pmd.load_file(&#39;ethane_periodic.top&#39;, xyz=&#39;ethane_periodic.gro&#39;) my_struc.save(&#39;ethane_periodic.prmtop&#39;) my_struc.save(&#39;ethane_periodic.crd&#39;) . Another molecular simulation engine, AMBER, specifies the molecular model in prmtop (parametrized topology) and crd (coordinate) files. I confess I haven&#39;t actually used AMBER (it&#39;s very popular), but parmed has made it incredibly easy for a GROMACS-user to switch over (and use functionality that AMBER might provide) . !head ethane_periodic.prmtop . %VERSION VERSION_STAMP = V0001.000 DATE = 08/01/19 17:37:44 %FLAG TITLE %FORMAT(20a4) %FLAG POINTERS %FORMAT(10I8) 8 2 6 1 12 0 9 0 0 0 29 1 1 0 0 2 2 9 1 0 0 0 0 0 0 0 0 1 8 0 0 . !head ethane_periodic.crd . * GENERATED BY PARMED (HTTPS://GITHUB.COM/PARMED/PARMED) * 8 EXT 1 1 RES C 0.0000000000 -1.4000000000 -0.0000000000 SYS 0 0.0000000000 2 1 RES H -1.0700000000 -1.4000000000 -0.0000000000 SYS 0 0.0000000000 3 1 RES H 0.3600000000 -2.1700000000 0.6500000000 SYS 0 0.0000000000 4 1 RES H 0.3600000000 -1.5800000000 -0.9900000000 SYS 0 0.0000000000 5 1 RES C 0.0000000000 0.0000000000 0.0000000000 SYS 0 0.0000000000 6 1 RES H 1.0700000000 0.0000000000 0.0000000000 SYS 0 0.0000000000 7 1 RES H -0.3600000000 0.7700000000 0.6500000000 SYS 0 0.0000000000 . What we did with GROMACS files to pmd.Structure to AMBER files involed an IOstep to read GROMACS files into an in-memory representation, then an IO step to write AMBER files from the in-memory representation. These 2 simulation engines require these files to perform a simulation - what about a simulation engine that doesn&#39;t require this IO step, and we can purely handle this all in-memory? . OpenMM is another simulation engine rooted in CUDA and C++ with wrappers for python implementations. We can &quot;interface&quot; more tightly with the OpenMM codebase by doing all these molecular model conversions within python . my_omm_system = my_struc.createSystem() print(type(my_omm_system)) my_omm_system . &lt;class &#39;simtk.openmm.openmm.System&#39;&gt; . &lt;simtk.openmm.openmm.System; proxy of &lt;Swig Object of type &#39;OpenMM::System *&#39; at 0x110c0ef00&gt; &gt; . In that one line, we&#39;ve converted pmd.Structure into openmm.System, which gets us very close to performing a simulation within OpenMM without writing out an input file. . In my opinion, this adds some elegance to performing molecular simulations in that they can be well-encompassed in a python script (and not a python script that just wraps a bunch of shell commands) . Does this relate to other fields of technology? . Are there other applications of STEM where objects/entities need to be able to convert, communicate, or inter-operate between one another? ParmEd is a nice intermediate that allows different simulation engines to communicate information. . I recently set up a Google Home Mini in my apartment - how does the Google product know how to turn off all the different smart bulbs out there (Philips, Element, Sengled, Wyze, FluxSmart)? I&#39;m sure there are engineers who have to work on this GoogleHome-bulb interoperability to make sure a command to Google Home will perform an action to another device. . What about parallel computing? The MPI standard defines routines and protocols to which any MPI library must adhere (OpenMPI, MPICH, etc). This defines a common ground so that an external program can call the same MPI functions and expect the same operation, but under the hood it&#39;s a different MPI library performing the gather/recieve commands. . Summary . ParmEd is a digital, in-memory data structure that represents a molecular model. It often serves an intermediary and great bookkeeper to facilitate translating molecular model (parameters, topology, coordinates) information from one simulation&#39;s file format (or data structure) to another simulation&#39;s file format (or data structure). It supports a variety of functional forms for the molecular modellers out there, and it is well-designed for the scientific software developers out there. Any molecular modeller knows how difficult and laborious this sort of package is, so the contributors to ParmEd deserve a big thank you. .",
            "url": "https://ahy3nz.github.io/fastpayges/grad%20school/molecular%20modeling/scientific%20computing/2019/08/01/pmd.html",
            "relUrl": "/grad%20school/molecular%20modeling/scientific%20computing/2019/08/01/pmd.html",
            "date": " ‚Ä¢ Aug 1, 2019"
        }
        
    
  
    
        ,"post23": {
            "title": "Bayesian Methods and Molecular Modeling 1",
            "content": "(Last updated: 2019-07-31). This is an ongoing post as I work through a tutorial I found. . Introduction . Within the world of molecular modeling, a very exciting group is doing some interesting work. They are the OpenForceField consortium. Looking through one of their workshops, they draw some interesting analogies between developing force fields and bayesian inference. As someone transitioning from molecular modeling to data science, this is certainly an interesting intersection. . A big question . Given some thermodynamic/QM data, how can we best parametrize force fields? . Let‚Äôs step back a bit and make it sound more statistics-y. . Given some energetic data, how can we best model the energy of a system? What is the best distribution/equation to model the energy (aka force field)? And what are the best distributions to describe the parameters of the model? From these fitted/parametrized models, can we interpret anything from them? Can we sample/simulate anything from them? . Learning some Bayesian methods . Here‚Äôs where I stumble through miscellaneous talks, lectures, publications, and tutorials to try to piece together bits of this field. I found a particularly interesting tutorial by Chris Fonnesbeck . Important: I am really hoping someone out there is reading this and will correct me where I am wrong. I‚Äôve learned solo-learning can be trial-and-error, and having external input can be very invaluable for guiding education. . Notebook 1: Introduction to PyMC3 . Their example is studying radon levels (actually, log(radon)) in households, developing a model, and trying to identify how many households are greather than a threshold. . We are interested in the posterior distribution given the data | To develop a sampling distribution for the data, they use a normal distribution, which requires parameters for $ mu$ and $ sigma$, and also is based on an observed distribution (the gathered data) | To develop a model or distribution for $ mu$, they first approximate/guess with a normal distribution (it‚Äôs okay if the log radon is negative) | To develop a model or distribution for $ sigma$, they first approximate/guess with a uniform distribution (standard deviations and variances shouldn‚Äôt be negative) | . In actuality, first the models for $ mu$ and $ sigma$ came first, THEN the sampling distribution for the data was built off the $ mu$ and $ sigma$ distributions in addition to the observed data . The overarching model was fit using a Markov chain Monte Carlo (MCMC) method. . After fitting the model we can observe some probabilities about a point estimate, $ mu$: . Probability that the mean level of radon is above a threshold | Probability that the mean level of radon is below a threshold | . Translation to molecular modeling . Our posterior distribution is the force field (potential energy function) given some observable. Analogously, this was the posterior distribution of radon levels given the observed data. . | In our molecular model, let‚Äôs say we just model the force field as harmonic springs . Here, the two parameters are $k$ and $r_0$ | In the radon model, we modeled with a normal distribution. There, the two parameters were $ mu$ and $ sigma$ | We used normal distributions for $ mu$ and $ sigma$. For $k$ and $r_0$, we know they should be non-negative (force constants and equilibrium lengths, respectively, are physically and generally non-negative), but we could use some sort of distribution that won‚Äôt get us negatives | . At this point in both cases: . We have an equation that expresses either our energy or the radon level (force field, posterior distribution). | We have also identified the associated parameters in these equations ($k$ &amp; $r_0$, $ mu$ &amp; $ sigma$). | For these parameters, we are estimating/approximating them via some other equation or distribution (normal or uniform) | . The next step is to fit the model using some sort of sampling method. Molecular modelers would call this parametrization. In layman‚Äôs terms, I think this means some sophisticated guess-and-check or numerical optimization method to get the parameters that well-describe the observed data. . Now we have our parameters, so we can sample from this model or distribution (simulating, sampling), and see how well the simulated/sampled data recreates the generated data. . We can interpret these models: make some statements about the mean radon model or make some statements about the equilibrium bond length . Lastly, sensitivity analysis means messing with the underlying assumptions - what if we didn‚Äôt use normal/uniform distributions to describe [$k$, $r_0$] or [$ mu$, $ sigma$]? I think you could go one step further and mess with the underlying model, what if the radon level wasn‚Äôt modeled as a normal distribution? What if the force field wasn‚Äôt modeled as harmonic spring bonds? . I would hazard a guess and say not to try radically different models if they intuitively don‚Äôt really describe the data we are interested in modeling - modeling a force field (energy) as a purely random distribution would probably be a physically-unrealistic model and would not describe your data well. . Notebook 2: Markov chain Monte Carlo . (Update 7/31: Some of these early bits (from 7/30) are incorrect, and then I later correct myself - Jump to the 7/31 Update if you don‚Äôt care to see me be wrong). . This was a lot more than notebook 1. On a completely different note, there‚Äôs so many parallels to molecular modeling here, and I keep thinking of everything in terms of molecular modeling and I‚Äôm not sure if that‚Äôs hindering or facilitating my education. I‚Äôm going to unwind some thoughts here. . Once again, Bayesian methods are used to estimate parameters of a model. If your model is a normal distribution, your parameters are $ mu$ and $ sigma$. If your model is a line, your parameters are slope and intercept If your model is a harmonic spring, your parameters are force constant and equilibrium bond length. . Bayes‚Äô formula is used to estimate the probability of certain parameters given some observed data. . . The denominator, $P(y)$ is called a normalization constant or marginal likelihood. Okay, well how do you calculate that? . . To compute the marginal likelihood (probability of one variable), we have to compute this integral of the joint probability distribution (probabilities of two variables) along the other variable. . Here‚Äôs the first analogy to statistical mechanics: . . Where $ epsilon$ is the energy of state, $k_B$ is Boltzmann‚Äôs constant, $T$ is temperature, and $Q$ is our normalization constant (in fancy stat mech terms, this is the canonical partition function). Except this probably isn‚Äôt as noteworthy as it seems, since probabilities, in general, are just numbers of desired outcomes divided by all possible outcomes. . ## (Ignore this is wrong) Back to Bayesian, this marginal likelihood integral is analytically hard to compute, so we have to use some numerical sampling methods. In molecular modeling, we are also trying to compute an integral over phase space (the 6N distribution over positions and velocities, but really just 3N positions because the velocity component is a priori known based on temperature). In both methods, we can employ Monte Carlo methods. In Bayesian, this is Markov chain Monte Carlo. . Side note: Monte Carlo integration . Let‚Äôs say you wanted to integrate $f(x)dx$. Assuming $f(x)$ is complicated and can‚Äôt be solved analytically, you use a numerical method like Riemann sums, Simpson‚Äôs Rule, Trapezoidal Rule, etc. The summary of these numerical methods is you (uniformly) pick a bunch of $x$, compute $f(x)$, and draw a bunch of rectangles whose height is $f(x)$ and length $x_{i+1} -x_i$ (otherwise known as $dx$), then sum up all those areas. . What if you didn‚Äôt pick $x$ uniformly? If you randomly picked $x$ (and adjusted the area summation to account for the non-uniform $dx$), you would be implementing a crude Monte Carlo integration. . To sum up Monte Carlo, pick random inputs, evaluate the function at those inputs, compute area, and sum. . Back to the notebook . Markov chains essentially mean that the probability of visiting a future state is dependent on what your current state, but not any other state in history. In the Monte Carlo scheme, this means, while you are ‚Äúrandomly‚Äù generating inputs, your next input is somewhat dependent on your current one. . Some of the underlying theory says you can use Bayes‚Äô theorem (again, but applied to the Markov chain, not how we‚Äôre obtaining model parameters), to demonstrate a concept known as detailed balance that means you have balanced movement through phase space (this is a princple that holds for broader statistics in addition to molecular modeling Monte Carlo methods). . Okay, so if our next input depends on our current input, what is the critieria or algorithm we use to pick the next input in this integration scheme? . One algorithm is Metropolis-Hastings where you accept or reject based on the ratio of probabilities of new-input to current-input. This means comparing $P( theta‚Äô)$ to $P( theta)$, which I think means referencing our posterior distribution ($P theta|y$). . In molecular modeling, we relate ratios to exponentials of energies so that our acceptance ratio simplifies to: . (7/30) Checkpoint: maybe summarizing things so far . We want to model some sort of observable, which requires parameters | We want to get the observable‚Äôs posterior distribution, which tells the probability distribution of certain parameters given the observable data | We have Bayes‚Äô theorem that formalizes some relationships | We have the likelihood $P(y| theta)$, which is an equation/distribution that depends on our parameters. From notebook1, this is a model/distribution we specify beforehand, with parameters we are trying to find. | We have the prior $P( theta)$, which is an equation/distribution of our parameters. From notebook1, this is a model/distribution we specify beforehand, with parameters we guessed. | (## ignore this is wrong )We have our marginal likelihood $P(y)$, which is re-written as an integral of $P(y| theta)P( theta)d theta$, which is an integral of our likelihood and prior we just outlined. | (## ignore this is wrong) To evaluate this integral, we use a Markov chain Monte Carlo method to pick different values of $ theta$, evaluate the product of likelihood and prior, then compute the summation. This integration method requires lots of iterations/sampling (this sampling is NOT sampling the observed data, but sampling different $ theta$ parameters). (## ignore)This integral is really difficult to comprehend in my opinion. | (## ignore)You are computing an integral to end up with a posterior distribution | (## ignore)But to compute the integral iteratively and numerically, you need to continuously re-evaluate the posterior distribution to choose moves based on Metropolis-Hastings. | . | (## ignore) How we efficiently calculate this integral is where the Bayesian inference methods get interesting from a computational perspective. | . I think writing some of this out maybe helped my own understanding. Again, if someone out there is reading this and deems me incorrect, PLEASE contact me - I‚Äôd love to learn this properly. For now, I‚Äôm going to take a break and return to this notebook 2 later - we‚Äôre going to be hitting some crazy similarities to molecular simulation. . (7/31) Update: I was pretty wrong yesterday . Let me at least re-iterate what I still think is correct: . We want to model some sort of observable, which requires parameters | We want to get the observable‚Äôs posterior distribution, which tells the probability distribution of certain parameters given the observable data | We have Bayes‚Äô theorem that formalizes some relationships | We have the likelihood $P(y| theta)$, which is an equation/distribution that depends on our parameters. From notebook1, this is a model/distribution we specify beforehand, with parameters we are trying to find. | We have the prior $P( theta)$, which is an equation/distribution of our parameters. From notebook1, this is a model/distribution we specify beforehand, with parameters we guessed. | We have our marginal likelihood $P(y)$, which is re-written as an integral of $P(y| theta)P( theta)d theta$, which is an integral of our likelihood and prior we just outlined. It‚Äôs important to remember this is just a (normalizing) constant, as in, this number doesn‚Äôt change. | . | . What is the point of the Markov chain Monte Carlo then? . We are trying to sample a bunch of different parameters $ theta$, and a Monte Carlo approach works to sample parameters, with a Markov chain being an efficient way to sample randomly while keeping track of your current state. . I had to work backwards, reverse-engineer, dissect the code (whatever you want to call it). . calc_posterior . calc_posterior is a method to calculate the joint posterior, given your data: measured output (recorded prices) + measured input (recorded ages) and your sampled parameters (slope, intercept, stdev of the posterior distribution assuming it obeys a normal distribution). It‚Äôs worth mentioning that we are working in log terms, which means products of values become summations of log(values). . So let‚Äôs pull up Bayes‚Äô theorem again, but focus on the numerator . We are only interested in the likelihood and the prior. To calculate the log posterior, let‚Äôs first compute the prior $P( theta)$. Because we have 3 parameters, we have 3 prior distributions to look at. Remember, a priori we make some claims about the distributions of these parameters. We said the slope and intercept obey a normal distribution (in the example, the center was on 0 with standard deviation 10000). Within this function, we have an estimate for the slope and intercept. Given the specified (normal) distributions, we can compute the probability of observing the slope, and we can compute the probability of observing the intercept. Multiplying probabilities is adding the log probabilities. The third parameter, t or $ tau$, pertains to a standard deviation (actually I think it‚Äôs the variance but they allude to the same thing). Given that $ tau$ has to be positive, we claimed its distribution obeyed a gamma distribution. So given the specified (gamma) distribution, we can compute the probability of observing this $ tau$, and add this to the log posterior. At this point, we have accounted for all 3 priors. . Next, we look at the likelihood, $P(y| theta)$. This component is different because now we have to look at our observed data, which is 39 data points of (age, price). It‚Äôs wrapped in nice vectorized calculations, which means a single line of code acting on a variable actually acts on a bunch of values. In this scope where we are calculating the posterior, I reiterate: we have estimates for 3 parameters. We can use these parameters and our model to try to predict some output (price) given the actual input (age). In this example, we are modeling the relationship between price and age with a linear model. We have a guess for the slope and a guess for the intercept. For all of our 39 ages, let‚Äôs use these 2 parameters and model (slope, intercept, linear equation) to predict 39 prices. In the code, this is mu = a + b*x, where x is our array of ages. Now we have 39 predicted prices and 39 actual prices. Let‚Äôs return to the likelihood $P(y| theta)$, the probability of the observed data given your parameters. The hidden middle step is the underlying model that converts parameters + input into output, which is why we compute mu. Examine the next line of code logp += sum(dnorm(y, mu, t**-0.5)). In particular dnorm(y, mu, t**-0.5), in which we compute the probability of observing the actual, observed price under/given a normal distribution centered around mu with standard deviation $ tau$ (this is where the 3rd parameter comes in!). And then we sum up all these log probabilities for all 39 data. . Compare and contrast the code/function calls, i boiled them down for simplicity: . Priors : logp = dnorm(a,0,10000) | Likelihood: logp = dnorm(y, mu, t**-0.5) | . With the prior, we knew the underlying (normal or gamma) distribution and parameters of that distribution (center and stdev), and those were hard-coded/fixed/constant. The only variable/thing that changes is our estimate for the parameter, and then we compute the probability of observing that parameter under a (fixed) distribution. . With the likelihood, we have many more variables that will change throughout the sampling. Each iteration, we are coming up with a new distribution (based on mu and t), and computing the probability of observing the price (whicch is our observed data) under this new distribution. To come up with this new distribution, we still made a claim that it was a normal distribution, but the center (mu) and variance (t) of the normal distribution are subject to change. t or $ tau$ is one of the 3 parameters we are continuously changing/sampling, so that one comes sort of ‚Äúfree‚Äù in this iterative process. mu, however, is our price, thing we are trying to predict. mu isn‚Äôt free, we have to compute mu based on our model and other parameters (slope and intercept). But at least y is free, it‚Äôs the observed data (price) that we measured. . Let‚Äôs rephrase this another way . You want a model to relate input to output. It is some sort of mathematical function/equation that adds/multiplies/exponentiates/logs your input to get an output. You have observed data for (input, output) | You need parameters for this model. These are not observed and you have to come up with them. We make a claim about the distribution of these parameters (they seem normally distributed about 0 with standard deviation 10000). In the parentheses, we have just specified 3 pieces of information - the type of distribution, and the 2 parameters of that distribution. These distributions compose your prior | Using Bayes‚Äô theorem, we are trying to identify the parameters for the observed data according to our model. This manifests as the posterior distribution, the probability of a particular value of a parameters given the observed data. This means we say ‚Äúhey here‚Äôs a range of possible parameters of the model, and here‚Äôs how likely each of those parameters seems, given the data we have gathered‚Äù. We aren‚Äôt making a definitive claim on what the parameters are, just providing some possible parameters and likelihoods. | We have just discussed the posterior distribution and prior, now we have to address the likelihood. This is the probability of the observed data given your parameters. We make a claim about the distribution of the data (it seems normally distributed). Unlike the prior, we have only specified one piece of information, the type of distribution. What‚Äôs important to note is that the properties of the distribution (center and stdev) are computed over the course of the MCMC simulation. | The nature of this likelihood distribution changes over MCMC, whereas the nature of the prior distribution does not change. Over the MCMC, we get new parameters. | In the case of one observed datapoint: For the new parameters, we re-predict the price from the age. The predicted price forms one property of our normal distribution (center), and this predicted price arises from our underlying model (the linear function)! But the other property of our normal distribution (stdev), is another parameter we are continually updating (like the slope and intercept). | Within this new, updated distribution (which depends on our model and parameters), we look at the probability of observing the datapoint. Do this for all datapoints. | . | I find the likelihood component complicated (I mean, I got it wrong the first time around, and probably got it wrong in this explanation), so I put a lot of words there. | . What is the role of MCMC? What is the role of the marginal likelihood? . The whole point of MCMC is to try a bunch of different $ theta$ and get the probability of that $ theta$ given our data - sample different $ theta$ to build our posterior distribution. . One element of MCMC is the use of an acceptance criteria to accept or reject proposed, new parameters (Metropolis-Hastings). This notebook2 used some different notation, but I think the code explains more. alpha = proposed_log_prob - current_log_prob, we are looking at the log-difference of the posterior distributions (which is the non-log-quotient of the posterior distributions). . Here‚Äôs some math proof, where $ theta$ is the current $ theta$, and $ theta‚Äô$ is the proposed $ theta$ that we accept/reject. . . . . The marginal likelihood $P(y)$, being a constant, cancels out in the division! . To summarize the MCMC loop and sub-routines: . Propose a model to relate price and age (linear model) | For the model, propose distributions (and properties) for the parameters we are trying to find (prior) | For the model, propose a distribution (and properties) for the resultant output/data (likelihood). Note that THIS distribution will be centered on the predicted output, with variance as a parameter we are trying to find. | . | Propose an initial guess for the parameters (slope and intercept of model, variance of the likelihood) | Now we begin the MCMC loops | Propose new $ theta$ values (slope, intercept, variance) according to the prior distributions we had already specified | With the new $ theta$, compute the new posterior Find the probability of observing these $ theta$ according to their prior distributions | With the model and new $ theta$, predict some data (prices) given our measured input (ages) | With our likelihood distribution centered around each predicted value (and variance being one component of the new $ theta$), find the probability of observing the actual, observed value. | Sum up the logs (of the priors, of the likelihood of each actual observed value) , this is your new posterior | . | Calculate the acceptance ratio: compare the new posterior and the old posterior. Marginal likelihood $P(y)$ is cancelled out, so we can ignore it. | Compare the acceptance ratio to a value from a random, uniform distribution. If the acceptance ratio is greater than the random number, accept the proposed parameters and move onto next iteration with proposed parameters | If not, reject the proposed parameters and move onto the next iteration with the same/old parameters | . | Do this a bunch of times: you are trying different $ theta$ and checking how well it fits/models the data | At the end of the day, you will have just a huge collection of attempted $ theta$. | If well-sampled, the commonly-visited-$ theta$ are the $ theta$ most likely to explain your observed data (histogram $ theta$ for a good visualization). | . I think that summarizes my learning of Bayesian methods up until now. This next section, still in notebook2, deals with the sampling method (how we choose $ theta$. In this 7/31 section, I didn‚Äôt really mention molecular modeling, but now I probably will. . Hamiltonian Monte Carlo . Some $ theta$ are more typical and likely (we want to sample these), where as other $ theta$ are less likely (less important to sample these). In molecular simulation, some configurations (sets of coordinates) are more likely than others. We are generally interested in the more probable configurations. . To include Hamiltonian properties, we are characterizing $ theta$ using potential and kinetic energies - the former depends on ‚Äúposition‚Äù and the latter depends on ‚Äúvelocity‚Äù. Our $ theta$ can be considered a particle who can be characterized using its position and velocity - potential and kinetic energy. Some say velocity, others say momentum, but they are very tightly related via mass, except mass isn‚Äôt super important for these HMC methods. . In a canonical distribution, probabilities are proportional to the exponential of their energy (Hamiltonian), the Hamiltonian is expressed a sum of potential and kinetic energies . . Where $s$ is our position, $ phi$ is our velocity. Physicists and molecular modelers will probably cringe that we aren‚Äôt using $r$ and $v$ or $r$ and $p$. Remember that kinetic energy is $1/2 * mv ** 2$, but mass is 1 in these Bayesian methods. . And the associated probabilities of each state (a state is a collection positions and velocities): . . Now this very well matches canonical distribution as applied to statistical mechanics: . . Physics says we can relate position, energy, and the Hamiltonian via some differential equations (with respect to time), and we apply them to Hamiltonian dynamics for this Bayesian sampling. . . The top line, the derivative of position WRT time is the velocity (fundamental), which is also the derivative of the Hamiltonian WRT velocity (analytically differentiate the Hamiltonian yourself, observing that potential energy is a constant WRT velocity). . The bottom line, the derivative of velocity WRT time is acceleration (fundamental), which is also the negative derivative of the Hamiltonian (energy) WRT position - in molecular modeling the derivativative of energy WRT position is our force. . At this point, we have some coupled, partial differential equations. All molecular modelers are familiar with this - we just need to numerically integrate these PDEs over time to get a trajectory of positions and velocities! There are a whole slew of integrators in Hamiltonian Dynamics as there are in Molecular Dynamics, and the same considerations for these integrators both hold. The notebook2 uses leap-frog integration, which is actually a popular integration method in MD. I‚Äôll just note that coding up some of these integrators is a pain because you are taking a mixture of full, half, etc. steps and evaluating velocities/forces at different steps, until you actually take a full step to move to the next iteration/timestep. . In this Bayesian sampling, now that we have a Hamiltonian, the MCMC loop changes a bit - do some Hamiltonian Dynamics steps to get a new state (set of positions and velocities), then choose whether or not you want to accept these steps to get to a new state, via the same Metropolis-Hastings algorithm. While the velocity is artificial in the sampling, the position is analogous to your $ theta$ . In molecular simulation, people call this MCMD, where you do a bunch of MD steps to get a new state, and then you use MC methods to choose to accept this new state or not. . Now here‚Äôs where the molecular modelers might finally have a leg-up, and that‚Äôs in enhanced sampling (some call this non-Boltzman sampling because you are no longer sampling according to the Boltzmann distribution). Basically you introduce extra forces/energy and bias your trajectory/system/ positions away from already-visited states. . Bayesian-ists have a variety of enhancing sampling, but it seems they also echo similarly to molecular simulation enhanced sampling. . The thing with these methods, you are artificially changing the frequency/histogram with which you visit states, so you need a way to re-weight the biased/artificial histogram to recover the true histogram/distribution (I‚Äôm speaking from molecular modeling experience, but I‚Äôm guessing Bayesian stats requires something similar) . This concludes my study of Notebook2. Call me out if I got anything wrong, please. .",
            "url": "https://ahy3nz.github.io/fastpayges/molecular%20modeling/scientific%20computing/2019/07/29/bayesian1.html",
            "relUrl": "/molecular%20modeling/scientific%20computing/2019/07/29/bayesian1.html",
            "date": " ‚Ä¢ Jul 29, 2019"
        }
        
    
  
    
        ,"post24": {
            "title": "Conducting a simulation",
            "content": "Running a simulation means taking a model and sampling sort of distribution with it. . Recapping molecular modelling . Remember, our model from a molecular modelling perspective is the potential energy, which depends on the coordinates of every atom or particle in the system. We can either model the system energy using QM or MM methods. QM methods are more accurate, but more expensive. MM methods simplify away some of the less-relevant details (this depends on your system), make some approximations, and allow us to study larger and slower systems. . The Boltzmann distribution . The Boltzmann distribution describes the probability of observing states as a function of its energy and other thermodynamic variables (like the temperature). Delving into the thermodynamic theory, the Boltzmann distribution is the distribution that maximizes a system&#39;s entropy, so this is a physically-rooted distribution. Concisely put into an equation: . $ Huge p_i ; alpha ; e^{E_i/k_BT}$ . where $p_i$ is the probability of a state, $E$ is the energy of the system, $k_b$ is Boltzmann&#39;s constant, and $T$ is the temperature . What is a &#39;state&#39;? . In the Boltmzann distribution, a state refers to an energetic state (which can be associated to a chemical structure&#39;s 3D coordinates. Going further, depending on our thermodynamic conditions, we have macrostates that desribe a system&#39;s macroscopic properties (like temperature, pressure, volume, energy, number of particles). There are a set of microstates that can satisfy or achieve a particular macrostate. . For example, if you had 3 coins, you could have a macrostate consisting of 2 Tails and 1 Head. The corresopnding micorstates might be HTT, THT, TTH . Application to molecular simulation . One often overlooked fact is that all molecules move around, a lot or a little (unless you&#39;re at absolute zero but that&#39;s not the point). Thermal motion means that every atom vibrates a little bit - every molecule can wiggle ever so slightly or fly around. However, the physical phenomenon that atoms move around is the whole reason we have a distribution of configurations (coordinates) . Under the Boltzmann distribution, the probability of witnessing a chemical microstate (a particular set of coordinates that a chemical configuration occupies) is related to the energy of that state. . If a particular configuration is high-energy, we probably won&#39;t witness it. If it low-energy, there is a good chance we will . Monte Carlo methods . Monte carlo (MC) sampling is not unique to molecular simulation, but molecular modellers do like to implement MC methods. . Briefly, MC methods involve a trial where you try to change/alter some part of your system. In molecular modeling, your MC trial moves involve altering your configuration (rotating a molecule, displacing an atom, stretching a bond, etc.) . The choice to accept this move depends on the energy before and after the trial move. If the energy is lower, we accept the move and proceed with the simulation. If the energy is higher, we calculate the relative probabilities (according to the Boltzmann distribution), and compare that to a randomly-generated number; we either reject the move and propose a new one or accept and proceed. . There are lots of different algorithms, but a common one in the molecular modelling field is the Metropolis-Hastings algorithm . If you sample a lot of configurations, you can eventually get a good idea of the distribution of various configurations of your system. From this resultant sample or trajectory, we can start computing various (static) properties. By nature of the sampling, the configurations are somewhat independent and uncorrelated compared to other sampling methods . Molecular dynamics methods . $ Huge F ; = ; ma$ . In molecular dynamics (MD) sampling, we utilize kinetic energy and momentum to actually simulate the motion of these atoms. This is where we bring Newton&#39;s laws of motion in order to physically capture these motions - the acceleration on an object is related to the forces acting upon it . To compute the forces acting upon each atom, we look back to another physical relationship - force is the negative derivative of energy with respect to distance. This works well because now we we can relate motion to our molecular model; given the energy of our system, compute the gradient to get the forces, and these forces dictate the acceleration . $ Huge F( vec r) = - nabla U( vec r)$ . There a variety of other formalisms that have been used in MD like Hamiltonian or Lagrangian mechanics, but the idea is to relate potential and kinetic energy to the motion of a system . We also know that . $ huge a = frac{d^2 x}{d t^2}$ . Which means we can relate acceleration to position via a second order ordinary differential equation. If we integrate this, we can get a system&#39;s position over time. . This is very hard to do analytically, so we often resort to various numerical methods to integrate a second order ODE (compute the gradient and take a small step in that direction). In MD, we call this an integrator, and the field is very interested in all the different integration algorithms, their computational complexity, and overall stability (energy conservation versus time step, time-reversibility, among others). Don&#39;t forget, this integration means we now also account for things like velocity and kinetic energy (which follow the Maxwell-Boltzmann distribution) . To summarize molecular dynamics, we are integrating Newton&#39;s equations of motion over time according to a potential energy function. . After integrating for a finite number of steps, we have sampled a number of configurations that are more correlated to each other compared to MC methods. . Statistical side note . As a molecular modeller venturing into broader areas of statistics and data science, I find myself trying to relate concepts like Markov chain Monte Carlo or Hamiltonian dynamics back to these molecular modelling notions of MC and MD. I think there are similarities in that the MC analogs are drawing random samples, but the Hamiltonian and MD methods are accounting for some sort of kinetics or momentum. Even the notion of some steepest descent gradient algorithms reminds me that we essentially compute a gradient (force) of our objecive function (energy). . The law of large numbers, ergodicity, and phase space . As in statistics, the only way we can reliably trust our sample is if we draw enough samples. If we sample enough, the sample statistics and population statistics relate well. . In simulation, before we can even begin to think about drawing enough samples, we have to draw physically correct samples. We call this ergodicity - when our the probability distributions from our simulations don&#39;t change much. This means we need to run a simulation long enough such that our sampled configurations results replicate the underlying physical distributions. . Here&#39;s a more involved discussion. For N atoms, we have 6 N variables (for the 3 dimensions we have a velocity/momentum and a position). This results in a 6N phase space. Over the course of the simulation, we are effectively traversing through 6N phase space, with some regions being more &quot;popular&quot; or favorable than others. When this probability density no longer changes with respect to time, our system is ergodic and we just need to generate a lot of samples from this probability distribution. . The formulation (Liouville&#39;s theorem) is as follows . $ large frac{ partial rho}{ partial t} = -iL = 0$ . A simpler way of thinking about this: you can start a simulation from some very unrealistic coordinates (like water in a crystalline configuration even though you&#39;re at room temperature), but if you simulate long enough, eventually you begin visiting only the physically-realistic and probabilistic configurations. At this point, your system is equilibrated and then you begin the task of sampling from this distribution. So if you run a 100 ns simulation, you might discard the first 20 ns as &quot;burn-in&quot; or &quot;equilibration&quot; when you were trying to hit equilibration. The other 80 ns you actually care about and analyze - this is your &quot;production&quot; run where you are reliably sampling from the correct distribution. . MC vs MD . There are a variety of things to think about here: computational complexity, equilibration, and the physical properties you want to measure. But at the end of each simulation, you end up with a series of configurations (coordinates). . Computational complexity . In most force fields (potential energy functions), bonded interactions are cheap because each atom participates in maybe a dozen different bonded interactions. Nonbonded interactions are much harder becuase each atom participates in a nonbonded interaction with every other atom in your system, this is $O(n^2)$, and these nonbonded, pairwise interactions are the most expensive calculations in a simulation code. In reality, there are some simulation tricks to speed up this pairwise computation to only look at the relevant/nearby atoms (neighbor lists) or use reciprocal space to rapidly compute long-distance interactions (Ewald sums) . In MC, you don&#39;t move EVERY atom, you move a few or just one. To evaluate a trial move, you need to compute how the energy changes. Fortunately, for the 99% of atoms that didn&#39;t move, that saves you some energy calculations. You only need to calculate the energy for the part of the system that changed. . In MD, you are moving EVERY atom, so you have to do this $O(n^2)$ calculation every, single time. . So comparing each iteration, a single MC iteration is faster than a single MD iteration. Actually, for various reasons, MD algorithms have found success being implemented as GPU kernels, so MD is really accelerated by GPUs. The complexity of MC has inhibited MC packages from really harnessing the computational power of a GPU. Don&#39;t get me wrong, there are some MC packages that utilize the GPU fantastically well, but you can find more MD packages that use the GPU. . Equilibration . MC means we take &quot;random&quot; moves - we could twist a long polymer, move an atom halfway across the simulation box, or something creative. Because MD aims to simulate the motion of atoms, our moves are somewhat constrained to local displacements. . With a wider variety, and more &quot;radical&quot; moves, MC can reach equilibration faster than MD, whose moves are very dependent on small displacements . Physical properties . It&#39;s 2-0, so we have to find something in favor of MD. Some physical properties depend on the time-evolved-dynamics of a system - we care about how the coordinates relate to each other over time. MC cannot do this because each configuration is fairly uncorrelated from the previous one. In MD, these configurational correlations help us calculate transport properties like viscosity and diffusion. MC has a hard time computing these properties due to the lack of correlation between configurations . A grad student confession . Honestly, most comptuational grad students don&#39;t think about these underlying theories or formulations that often. We&#39;re more concerned with applying them to do our research. We often take coursework that covers these concepts, but more often than not, we shrug off simulation techniques as just calculating energy/forces and moving atoms. . In terms of implementing these algorithms, they are already well-implemented in existing software packages. We don&#39;t have to write our Metropolis-Hastings algorithms, MC moves, or integrators - other generations of academics, scientists, and engineers have constructed and tested these tools and made sure they work. They made way for newer generations of students to spend their time applying these tools to research. . Usually, a particular lab or field gravtitates to either MC or MD, and then that becomes the learning environment and code infrastructure for new students. Occasionally we move into another method, but only if the scientific problem truly necessitates using another method. . Should the (unfortunate) time come when we have to find bugs in these packages, then we dust off the textbooks and re-re-re-re-learn these algorithms and techniques. . Conclusion . There are a variety of simulation/sampling techniques (MD or MC), each with its own perks and drawbacks. Fundamentally, there is a lot of derivation and proof that validates these methods in sampling the Boltzmann distribution. The tools of other scientists and engineers have allowed us to study interesting scientific problems without being &quot;caught in the weeds&quot;. . In broader statistical/data science perspectives, we use simulation methods to sample from a distribution and compute various properties (some dependent on time-correlations), and we have to ensure that we have correctly sampled enough to draw reliable conclusions. Some build the model and simulation cornerstones, others apply these tools as they see fit. .",
            "url": "https://ahy3nz.github.io/fastpayges/grad%20school/molecular%20modeling/scientific%20computing/2019/07/25/mm2.html",
            "relUrl": "/grad%20school/molecular%20modeling/scientific%20computing/2019/07/25/mm2.html",
            "date": " ‚Ä¢ Jul 25, 2019"
        }
        
    
  
    
        ,"post25": {
            "title": "Technical Debt",
            "content": "Technical debt is an ongoing, ever-pressing issue to any large, collaborative code base. If left unaddressed, technical debt can seriously cripple productivity. . credit . Contextualizing the issue in academia . You‚Äôre a STEM graduate student or young professor in the 70s or 80s. Coding standards, software practices, continuous integration, unit tests, and modularity haven‚Äôt been popularized. The codes you wrote were very long scripts tailored to your particular problem. Your code was bad, and the only person who could use your code was you. . You‚Äôve probably tried to share your code with a collaborator, mentee, or student, but it‚Äôs incredibly hard for him/her to navigate and use your code. Maybe you could spend some time to re-design and re-factor for extensibility, but time is publications is money in the life of an academic. You‚Äôll do it later. . Years pass and your messy code has grown from 100 lines of hard-to-understand, poorly-tested software to thousands of lines of mess. At this point, if you wanted to re-factor, it would be a significant overhaul that would require considerable amounts of time, energy, maybe money that would detract from publishing. Uh-oh. On top of that, you‚Äôre far enough in your career that you‚Äôre not the one writing the code anymore, so you‚Äôll need some help. . Problems that arise . For starters, messy code is harder to learn than well-organized, documented code. If you‚Äôve ever been thrown in the deep-end of a codebase without a guide, you‚Äôre left floundering for quite a long time. If it was a better codebase, you‚Äôd still struggle, but way less. . In the grand scheme of ‚Äúgood science‚Äù, your methods and techniques need to be reproducible by others. This could be as simple as re-running the exact same scripts in the exact same environment. Or it could be more complex as giving someone a broad experiment/simulation, and hoping that person can figure it out without copy-pasting your code. . In the grand scheme of ‚Äúgood science‚Äù, your methods have to be correct. Before you take your technique to study some obscure, complex system, how about some sanity checks that your results work for something simple, like water? In general software engineering, unit tests are easier in that the inputs and outputs are simpler and more direct. In scientific software, it‚Äôs a little more complicated and groups can take a variety of approaches (checking input, doing energy calculations, checking if a simulation doens‚Äôt crash, etc.), but a systematic testing framework is necessary to ensure correctness and accuracy. . If you‚Äôre trying to popularize a method or software (AKA citations), it helps if external audiences can understand the codebase before they get flustered, give up, and go to the easier-to-implement method. . How do we start addressing technical debt? . There are bigger brains who have thought longer about the issue and have considered many external factors. I‚Äôve had some chances to talk and pick their brains - it‚Äôs complicated. Broadly, this could be split into two categories: the small, growing code base vs. the large, archaic, software monolith . I should mention that the landscape of government funding is acknowledging the need for sustainable software and helping develop centers that can organize and facilitate sustainable efforts, like URSSI, MolSSI, Nanohub, among others . Easier: the small, growing code base . Like cleaning your room, it‚Äôs easier to clean your room daily or weekly before the mess is incomprehensibly large after a year of not cleaning. Refactoring often is very important. As the codebase grows, functions, routines, classes, modules, files may need to be re-organized into a cohesive package rather than just of a jumble of files that don‚Äôt seem to fit well together. . Write tests as you go. It feels less laborious if the tests are written as a new routine/module is put together. Depending on the scope of the project, unit tests, integration tests, regression testing, stress testing, etc. are all different sorts of tests to sanity-check and expose bugs. . MolSSI has a good cookiecutter for any package getting started, and it‚Äôs much better to start off on the right foot to set a pattern or set of practices before things get out of hand. . It‚Äôs important to establish best practices as early as possible, so good design and infrastructure can be in place for the future. . Harder: large software monoliths . In my experience, this is where a good portion of academic software stands. &gt;10,000 lines of code with poor testing, organization, or modularity. . One approach involves a dedicated application support engineer on hand to refactor and maintain. (this job has many names, but essentially a somewhat permanent position for someone who is familiar with the code). Obviously this costs money, and the academic position is significantly less enticing than a corresponding role in industry (salary, culture, career growth). Some of the most successful software sells academic and commercial licenses, but that becomes a whole other beast to tackle in the open source community. Otherwise, you can hope you have a very charitable graduate student that spent his/her PhD with this package and you hope will donate time post-graduation to supporting the package. This is voluntary, so large code overhauls are unlikely. Honestly, this is voluntary, so expecting anything is optimistic, especially since PhD graduates can be burnt out. . Another approach involves accepting the monolith as it is, but trying to build around that with good practice. For example, if someone wanted to contribute a module/routine, make sure that routine is testable and sufficiently modular while still interacting well with the overarching package. This could take extra work if this involves making different programming languages work seamlessly well together. Another example, if you have an old simulation package, the simulation package itself might be hard to re-factor, but the mechanisms by which you generate inputs (coordinate files, simulation parameters) or analyze outputs (trajectories, energies) might be a more tractable project to raise to modern code standards. . Given the labs have sort of rooted traditions and work environments, instilling an ethos of sustainable software definitely takes a lot of initial work to foster these practices, but the open source community (and the molecular sciences sub-community) is friendly and willing to help. . Overall, I think this approach is a sort of a compromise between the hefty resources required for sustainable software and the long-term benefit of sustainable practices. . If there‚Äôs anything to take away from this . Sustainble software is essential to long-term productivity. Technical debt is going to hinder education, progress, and collaboration. If you‚Äôre a manager, boss, advisor, etc., it‚Äôs important to recognize these issues and try to develop a plan of action with your team and the ones who are heavily involved with the codebase. .",
            "url": "https://ahy3nz.github.io/fastpayges/grad%20school/scientific%20computing/2019/07/21/techdebt.html",
            "relUrl": "/grad%20school/scientific%20computing/2019/07/21/techdebt.html",
            "date": " ‚Ä¢ Jul 21, 2019"
        }
        
    
  
    
        ,"post26": {
            "title": "Analyzing Simulation Trajectories",
            "content": "Let&#39;s say you&#39;ve conducted a simulation. Everything up to that point (parametrization, initialization, actually running the simulation) will be assumed and probably discussed another day. What you have from a simulation is a trajectory (timeseries of coordinates), and now we have to derive some meaningful properties from this trajectory. . Many meaningful properties can be derived from these coordinates, be it how atomic coordinates are related to each other, the sorts of geometries or larger structures we see, or how these coordinates are correlated over time. Whatever it is you&#39;re interested in, it all starts with the coordinates . There are many analysis packages: . [MDTraj] (http://mdtraj.org/1.9.3/) | [MDAnalysis] (https://www.mdanalysis.org/docs/) | [Freud] (https://freud.readthedocs.io/en/stable/) | [Pytraj] (https://amber-md.github.io/pytraj/latest/index.html) | [Cpptraj] (https://amber-md.github.io/cpptraj/CPPTRAJ.xhtml) | and many, many others (this is what happens when a open-source software goes rampant with different desired functionality and starts from independent research groups) | . While each has a variety of different built-in/common analysis routines, some are more common (like radial distribution functions). What EVERY modeller will use, though, is the coordinates. The most important function in these analysis packages is the ability to turn a large trajectory file, written to disk, and read it into memory as a data structure whose XYZ coordinates we can access. . Every simulation engine has different file formats and data encodings, but many of these analysis packages can support a wide range of file formats and pump out the same Trajectory data structure core to each package. . For example, we can use MDtraj to read in some simulation files from GROMACS. We obtain information about the XYZ coordinates and molecular topology (atoms, elements, atom names/types, residues, chemical bonding) . In general, there&#39;s a sort of hierarchy/classification to groups of atoms. At the base, you have an atom, which is as it sounds, or a coarse-grained particle depending on your simulation. Groups of atoms can form a chain, which is pretty much just a bonded network of atoms. Groups of atoms and chains form a residue. This derives from protein amino acid residues, where each monomer was a residue. In other applications, this can also refer to a closed-loop bonded network of atoms (a singular molecule). All of these different entities/groupings form your topology . import mdtraj traj = mdtraj.load(&#39;trajectory.xtc&#39;, top=&#39;em.gro&#39;) traj . &lt;mdtraj.Trajectory with 1501 frames, 18546 atoms, 2688 residues, and unitcells at 0x10dab9ba8&gt; . Most analysis packages have some way to access each atom in your topology . traj.topology.atom(0) . DSPC1-N . If designed well, you can access residue information from each atom . traj.topology.atom(0).residue . DSPC1 . Or, you could acess each residue in your topology . traj.topology.residue(0) . DSPC1 . And then access each atom from within that residue . traj.topology.residue(0).atom(2) . DSPC1-H13A . Every atom has an index, which is often used for accessing the different arrays . traj.topology.atom(100).index . 100 . Some analysis packages also have an atom-selection language, which returns various atom indices . traj.topology.select(&quot;element N&quot;) . array([ 0, 142, 284, 426, 568, 710, 852, 994, 1136, 1278, 1420, 1562, 1704, 1846, 1988, 2130, 2272, 2414, 2556, 2698, 2840, 9273, 9415, 9557, 9699, 9841, 9983, 10125, 10267, 10409, 10551, 10693, 10835, 10977, 11119, 11261, 11403, 11545, 11687, 11829, 11971, 12113]) . Now we can get to the important numbers, the coordinates . traj.xyz . array([[[3.75500011e+00, 2.16800022e+00, 6.84000015e+00], [3.64100027e+00, 2.17200017e+00, 6.94400024e+00], [3.68000007e+00, 2.21600008e+00, 7.03500032e+00], ..., [1.38600004e+00, 2.20000014e-01, 8.43700027e+00], [1.31000006e+00, 2.01000005e-01, 8.49200058e+00], [1.39500010e+00, 1.43000007e-01, 8.38100052e+00]], [[3.92900014e+00, 2.18300009e+00, 6.83200026e+00], [3.85500026e+00, 2.24800014e+00, 6.94500017e+00], [3.92200017e+00, 2.28600001e+00, 7.02000046e+00], ..., [7.00000003e-02, 3.23000014e-01, 1.30000010e-01], [6.00000005e-03, 3.45000029e-01, 6.30000010e-02], [8.50000009e-02, 4.05000031e-01, 1.77000001e-01]], [[3.79200029e+00, 2.13300014e+00, 6.90600014e+00], [3.75500011e+00, 2.17000008e+00, 7.04500055e+00], [3.69800019e+00, 2.26100016e+00, 7.03900051e+00], ..., [3.31000030e-01, 8.42000067e-01, 8.24400043e+00], [2.39000008e-01, 8.64000022e-01, 8.26000023e+00], [3.51000011e-01, 7.74000049e-01, 8.30900002e+00]], ..., [[5.35700035e+00, 3.21400023e+00, 6.66500044e+00], [9.00000054e-03, 3.30200005e+00, 6.77700043e+00], [5.32400036e+00, 3.37800026e+00, 6.80000019e+00], ..., [5.89000046e-01, 2.46400023e+00, 6.28700018e+00], [5.42000055e-01, 2.38100004e+00, 6.27500010e+00], [6.04000032e-01, 2.49500012e+00, 6.19800043e+00]], [[9.20000076e-02, 3.15200019e+00, 7.07700014e+00], [1.93000004e-01, 3.26800013e+00, 7.08700037e+00], [1.33000001e-01, 3.35700011e+00, 7.09800053e+00], ..., [8.20000052e-01, 2.19400001e+00, 6.70000029e+00], [8.31000030e-01, 2.10200000e+00, 6.67600012e+00], [7.78000057e-01, 2.23400021e+00, 6.62400055e+00]], [[1.24000005e-01, 2.99600005e+00, 6.71500015e+00], [9.60000008e-02, 3.05700016e+00, 6.84500027e+00], [4.00000019e-03, 3.11200023e+00, 6.83600044e+00], ..., [5.77000022e-01, 2.19900012e+00, 6.82900047e+00], [6.62000060e-01, 2.23500013e+00, 6.80300045e+00], [5.80000043e-01, 2.10800004e+00, 6.80000019e+00]]], dtype=float32) . This is a multi-dimensional array, but off the bat you can start seeing these 3-tuples for XYZ. . This is a numpy array, though, so we can use some numpy functions . traj.xyz.shape . (1501, 18546, 3) . 1501 frames, 18546 atoms, 3 spatial coordinates. . We can also snip out a frame to get all of the coordinates for all the atoms in that one frame . traj.xyz[0].shape . (18546, 3) . Snip out an atom (or collection of atoms) - based on index - to get all frames and all the coordinates of that collection of atoms . traj.xyz[:, [1,2,3],:].shape . (1501, 3, 3) . Snip out just one dimension to get all frames and all atoms and just one dimension . traj.xyz[:,:,0].shape . (1501, 18546) . Since a trajectory is just a collection of frames, one after another, you can also snip out frames from a trajectory . traj[0] . &lt;mdtraj.Trajectory with 1 frames, 18546 atoms, 2688 residues, and unitcells at 0x1257af908&gt; . This is still a Trajectory object, just 1 frame. XYZ coordinates are still accessible as earlier . All simulations occur within a unitcell to define the boundaries of the simulation. . traj.unitcell_vectors . array([[[5.11195 , 0. , 0. ], [0. , 3.74324 , 0. ], [0. , 0. , 8.80772 ]], [[5.116633 , 0. , 0. ], [0. , 3.7466693, 0. ], [0. , 0. , 8.806476 ]], [[5.0943184, 0. , 0. ], [0. , 3.7303293, 0. ], [0. , 0. , 8.894873 ]], ..., [[5.3887806, 0. , 0. ], [0. , 3.9459503, 0. ], [0. , 0. , 8.189841 ]], [[5.3347273, 0. , 0. ], [0. , 3.9063694, 0. ], [0. , 0. , 8.352207 ]], [[5.3583217, 0. , 0. ], [0. , 3.9236465, 0. ], [0. , 0. , 8.363432 ]]], dtype=float32) . traj.unitcell_vectors.shape . (1501, 3, 3) . For each frame, there is a 3x3 array to describe the simulation box vectors . I won&#39;t go into how you should analyze a trajectory, but every molecular modeller should be familiar with what analysis routines exist in which packages, and which analysis routines you should design yourself . Comments . There is a whole zoo of trajectory file formats that simulation engines produce - each analysis package can accommodate a subset of those file formats, each analysis package has different built-in analysis routines. Sometimes it&#39;s a mix-and-match game where you need to use package A to read a trajectory, and convert to package B representation because it has some particular analysis routine you need. . You could use an analysis package to read in one file format but write in another file format, or use an analysis package to manipulate coordinates/toplogy. Because these packages are designed intuitively and very similar to other structures in the SciPy ecosystem, there is a lot of room for creativity . Recent developments in the SciPy ecosystem look at out-of-memory or GPU representations of numpy array or pandas DataFrame, and this is a growing issue in our field - sometimes loading an entire Trajectory into memory is just not possible, so chunking is necessary to break the whole Trajectory into memory-manageable data . Summary . There are a variety of analysis packages out there, but they all start out the same way: read a simulation trajectory file and create an in-memory data representation that contains trajectory (coordinates) and topology (atoms, bonds) information. .",
            "url": "https://ahy3nz.github.io/fastpayges/grad%20school/molecular%20modeling/scientific%20computing/2019/07/17/trajanalysis1.html",
            "relUrl": "/grad%20school/molecular%20modeling/scientific%20computing/2019/07/17/trajanalysis1.html",
            "date": " ‚Ä¢ Jul 17, 2019"
        }
        
    
  
    
        ,"post27": {
            "title": "Molecular Modeling 1",
            "content": "How do you model something? . Let‚Äôs talk about molecular modeling from both the chemistry and mathematic standpoints. When you want to model something, what do you need? . An equation or objective function that describes the system of interest | Parameters and variables that quantify the relevant inputs Consequently, you need to determine what are the relevant properties | A robust, informed way to develop these parameters | . | Assumptions are always built into a model This is always important if you‚Äôre going to understand the model‚Äôs limitations | . | A method to sample, simulate, or gather data via this model | Some sort of output, measurable, or result that can be computed from the model | . How do you model a molecule? . In molecular modeling, the equations or objective functions of interest are those that describe the potential energy of a chemical system. This is known as a potential function or force field, and it is a function of a system‚Äôs coordinates. . Of what are you measuring the energy? . Back to general chemistry, molecules are composed of atoms. Atoms are composed of electrons, protons, and neutrons. Atoms have bonded interactions and nonbonded interactions. It is these interactions whose energy we try to quantify and describe. There are two approaches to quantifying the energy and describing a chemical system . Quantum mechanics (QM) descriptions . image from https://www.chemicool.com/images/schrodinger-equation-time-ind-annotated.png . The underlying equation in QM is Schrodinger‚Äôs equation. Molecules are modeled using wavefunctions, which are computed via the atomic orbitals in which each electron participates. For a modeler, the inputs are the ways we describe the orbitals. This level of detail ranges from highly-detailed ab initio methods, to semi-empirical methods, to lower-detailed density functional theory (DFT). From our description of atomic orbitals, we can obtain molecular orbitals and wavefunctions. . After modeling these wave functions, we try to identify the molecular coordinates and geometries that minimize the energy of our system. Minimizing the energy or optimizing the geometry of a system is VERY computationally expensive. On current supercomputers, we can maybe study up to 100s of atoms. With a detailed ab initio method, you‚Äôre very accurate, but the calculations are extremely expensive. With a less-detailed DFT method, you‚Äôre less accurate but the calculations are less expensive. As in all modeling, there is a tradeoff between computational expense and accuracy. . Once optimized, we can make observations about electronic and thermodynamic properties of our system. Depending on the method, we can also study some reaction mechanisms. . In general, quantum mechanical models are accurate and desirable because the only necessary inputs are how you want to describe the electrons, and the resultant chemistry and physics fall out via simulation. However, the fundamental limitation is the computational expense that restricts these methods to small systems. . Molecular mechanics (MM) descriptions . Molecular mechanics utilizes Newtonian mechanics to describe chemistry, sacrificing the detail of quantum-level descriptions in favor computational efficiency. This allows us to study upwards of one million atoms (your mileage may vary, but at time of writing, specialized hardware and algorithms have just begun to hit this astounding feat). More typically, system sizes are on the order of 10,000 to 100,000 atoms, with length scales on the order of 10s to 100s of nanometers, with time scales on the order of nanoseconds to microseconds (and milliseconds if you have the specialized hardware). . MM models are notably different from QM models in that MM models are more involved in describing bonded and nonbonded interactions. In MM, you specify a lot of the chemical behavior via many simpler equations. See below for an example potential energy function or force field (the Amber FF). . image from https://www.researchgate.net/profile/Ling-Hong_Hung/publication/5773728/figure/fig1/AS:340704250351617@1458241631445/The-physical-models-for-the-AMBER-molecular-mechanics-force-field-Atoms-and-bonds-are.png . For every bond, angle, dihedral, or non-bonded pair in your system, there is an equation that describes the energy as a function of distance or angle. As you can see, the equations might be something you learned from intro physics courses - harmonic springs, cosine series, coulomb‚Äôs law. It‚Äôs actually quite remarkable that these very basic equations not only do a pretty good job describing our system, but they actually fall out of the theory and approximations (see below for some example energetic profiles). . image from https://live.staticflickr.com/2/1608614_5ae09db3ec_z.jpg . Armed with these force fields, we can perform simulations and obtain a plethora of properties: . Transport properties (diffusion, viscosity, heat transfer, permeability, conductivity, friction) | Thermodynamic properties (free energies, heat capacities, density, activity coefficients, heats of vaporization, vapor pressure, contact angles, surface tensions) | Structural properties (crystallographic, packing, phase behavior) | Protein folding, membrane signaling, membrane transport | Metal-organic frameworks and zeolites and their loading capacity | Relative free energies to compare chemical states and drug binding affinities | . Even though MM methods broaden the scope of things we can computationally study, there are still many, many natural phenomena MM methods still cannot attain. . Many biological phenomena occur on length and time scales just out of reach, but the computational resources are always growing. | Bulk material properties and crack propagation are things amenable to more finite element methods. | . Furthermore, the quality of the computational result is dependent on the quality of the model. Garbage in, garbage out. . Concluding remarks . Molecular modeling is much like any other sort of modeling, you have a system of interest and you want to describe the parameters, properties, features of interest via some sort of equation. The choice of model (QM to MM) will depend on the problem you are trying to solve. Accuracy vs computational expense is a neverending battle. . The bread and butter of my PhD research utilizes molecular modeling via molecular mechanics methods. I‚Äôll be back again to discuss the computational techniques within molecular mechanics, sampling/simulation with these models, and some of the analysis/quantities we like to report. .",
            "url": "https://ahy3nz.github.io/fastpayges/molecular%20modeling/grad%20school/scientific%20computing/2019/07/14/mm1.html",
            "relUrl": "/molecular%20modeling/grad%20school/scientific%20computing/2019/07/14/mm1.html",
            "date": " ‚Ä¢ Jul 14, 2019"
        }
        
    
  
    
        ,"post28": {
            "title": "Introduction",
            "content": "Hello world . This is my first post. I‚Äôm Alex. I‚Äôm from the Northern Virginia area. I like chemical engineering, chemistry, computer science, and scientific computing/data science. . Undergraduate: University of Virginia (2016), high honors Major: Chemical engineering and economics | Minor: Computer science | Activities: Rowing, lion dance, attempted case competitions, attempted apps | Research: Molecular dynamics simulation and metadynamics to study protein folding and structure. GROMACS, PLUMED | . | . | Graduate: Vanderbilt University Chemical Engineering | Research: Molecular dynamics simulation of lipid bilayers (membranes) and membrane permeability. Coarse-grained model development | Many, many open source packages (as broad as the SciPy ecosystem, as niche as molecular modeling) | . | . What to expect . I want to talk about grad school, molecular modeling, and other academic/personal thoughts worth sharing with the internet. I‚Äôm going to avoid intense technical detail, but hopefully have just enough detail to understand the bigger picture and purpose of some topics and packages. More application than theory. . More importantly, in the molecular modeling community, I‚Äôve found there‚Äôs a degree of craftsmanship or art in working with the field. Explaining some of these software packages or ideas is like trying to explain conspiracy theories . For molecular modeling, the theories are well-established, and the popular packages have enough documentation to understand the input and output arguments. There are plenty of examples and tutorials for people to copy code and refactor to fit their needs, but there isn‚Äôt good discussion on the design and engineering behind these packages. . In my experience, to get the most out of these packages, you could spend years coming up to speed about a package‚Äôs core data structures before you feel comfortable actually running with it (beyond the examples) or contributing a good pull request. . Hopefully I can do a good job introducing this material to the budding molecular modeler. .",
            "url": "https://ahy3nz.github.io/fastpayges/molecular%20modeling/grad%20school/scientific%20computing/2019/07/13/intro.html",
            "relUrl": "/molecular%20modeling/grad%20school/scientific%20computing/2019/07/13/intro.html",
            "date": " ‚Ä¢ Jul 13, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "",
          "url": "https://ahy3nz.github.io/fastpayges/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  

  
  

  

  
  

  

  
  

  
      ,"page9": {
          "title": "",
          "content": "{‚Äú/about/‚Äù:‚Äùhttps://ahy3nz.github.io/‚Äù} .",
          "url": "https://ahy3nz.github.io/fastpayges/redirects.json",
          "relUrl": "/redirects.json",
          "date": ""
      }
      
  

  
  

  

  
  

  
      ,"page13": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://ahy3nz.github.io/fastpayges/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}